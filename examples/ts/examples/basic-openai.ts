/**
 * Basic OpenAI SDK Example
 *
 * This example demonstrates how to use Adaptive as a drop-in replacement
 * for the OpenAI API. Simply change the baseURL and you get intelligent
 * model routing, cost optimization, and provider failover.
 *
 * Features demonstrated:
 * - Drop-in OpenAI SDK compatibility
 * - Both streaming and non-streaming responses
 * - Intelligent model routing (empty model string)
 * - Function calling capabilities
 * - Automatic cost optimization
 * - Provider failover and resilience
 *
 * Key benefits:
 * - 60-80% cost reduction through intelligent routing
 * - No code changes required beyond baseURL
 * - Automatic fallback if providers are unavailable
 * - Real-time model selection based on prompt complexity
 *
 * Set ADAPTIVE_API_KEY environment variable to avoid hardcoding your API key.
 */

import OpenAI, { APIError } from "openai";

// Initialize the OpenAI client with Adaptive's endpoint
// This is the only change needed to use Adaptive instead of OpenAI directly
const client = new OpenAI({
	apiKey: process.env.ADAPTIVE_API_KEY || "your-adaptive-api-key",
	baseURL: "https://www.llmadaptive.uk/api/v1", // Adaptive's endpoint
});

// Non-streaming example - get complete response at once
async function nonStreamingExample() {
	console.log("=== Non-Streaming Example ===");
	console.log("üí¨ Sending message: 'Hello!'");
	console.log("üß† Using intelligent routing (empty model string)...");
	console.log("‚è≥ Waiting for complete response...");

	try {
		const response = await client.chat.completions.create({
			model: "", // Leave empty for intelligent routing - Adaptive chooses the best model
			messages: [{ role: "user", content: "Hello!" }],
			stream: false, // Explicitly disable streaming
		});

		// Type-safe response handling
		if (!response.choices?.[0]?.message?.content) {
			console.log("‚ùå No response content received");
			return;
		}

		// Display the response with provider information
		console.log();
		console.log("‚úÖ Response received:");
		console.log("üìù Content:", response.choices[0].message.content);

		// Adaptive adds provider information to the response
		const adaptiveResponse =
			response as OpenAI.Chat.Completions.ChatCompletion & {
				provider?: string;
				model?: string;
			};

		if (adaptiveResponse.provider) {
			console.log("üè¢ Provider used:", adaptiveResponse.provider);
		}
		if (adaptiveResponse.model) {
			console.log("ü§ñ Model used:", adaptiveResponse.model);
		}

		// Usage information
		if (response.usage) {
			console.log("üìä Usage:");
			console.log(`   ‚Ä¢ Input tokens: ${response.usage.prompt_tokens}`);
			console.log(`   ‚Ä¢ Output tokens: ${response.usage.completion_tokens}`);
			console.log(`   ‚Ä¢ Total tokens: ${response.usage.total_tokens}`);
		}

		console.log();
		console.log("‚ú® Non-streaming example completed successfully!");
	} catch (error) {
		console.error("‚ùå Non-streaming Error:", error);

		if (error instanceof APIError) {
			console.error("üîß API Error Details:");
			console.error("   ‚Ä¢ Status:", error.status);
			console.error("   ‚Ä¢ Message:", error.message);
			if (error.code) {
				console.error("   ‚Ä¢ Code:", error.code);
			}
		}

		throw error;
	}
}

// Streaming example - get response as it's generated
async function streamingExample() {
	console.log("=== Streaming Example ===");
	console.log("üí¨ Sending message: 'Write a short poem about coding'");
	console.log("üß† Using intelligent routing with streaming...");
	console.log("üì° Response will appear in real-time:");
	console.log();

	try {
		const stream = await client.chat.completions.create({
			model: "", // Leave empty for intelligent routing
			messages: [{ role: "user", content: "Write a short poem about coding" }],
			stream: true, // Enable streaming
		});

		let fullContent = "";
		let tokenCount = 0;

		console.log("üîÑ Streaming response:");
		process.stdout.write("üìù ");

		for await (const chunk of stream) {
			const content = chunk.choices[0]?.delta?.content;

			if (content) {
				process.stdout.write(content);
				fullContent += content;
				tokenCount++;
			}

			// Check if streaming is done
			if (chunk.choices[0]?.finish_reason) {
				console.log(); // New line after content
				console.log();
				console.log("‚úÖ Streaming completed!");
				console.log("üèÅ Finish reason:", chunk.choices[0].finish_reason);

				// Note: Streaming responses may not include provider info in every chunk
				// The final chunk might have additional metadata
				const finalChunk = chunk as any;
				if (finalChunk.provider) {
					console.log("üè¢ Provider used:", finalChunk.provider);
				}
				if (finalChunk.model) {
					console.log("ü§ñ Model used:", finalChunk.model);
				}

				console.log("üìä Approximate chunks received:", tokenCount);
				console.log(
					"üìè Total content length:",
					fullContent.length,
					"characters",
				);
				break;
			}
		}

		console.log();
		console.log("‚ú® Streaming example completed successfully!");
	} catch (error) {
		console.error("‚ùå Streaming Error:", error);

		if (error instanceof APIError) {
			console.error("üîß API Error Details:");
			console.error("   ‚Ä¢ Status:", error.status);
			console.error("   ‚Ä¢ Message:", error.message);
			if (error.code) {
				console.error("   ‚Ä¢ Code:", error.code);
			}
		}

		throw error;
	}
}

// Function calling example - demonstrate OpenAI's function calling capabilities
async function functionCallingExample() {
	console.log("=== Function Calling Example ===");
	console.log("üí¨ Testing function calling with weather lookup");
	console.log("üß† Using intelligent routing with function tools...");
	console.log();

	try {
		// Define a function for the model to call
		const weatherFunction = {
			name: "getCurrentWeather",
			description: "Get the current weather for a given location",
			parameters: {
				type: "object",
				properties: {
					location: {
						type: "string",
						description: "The city and state/country, e.g. San Francisco, CA",
					},
					unit: {
						type: "string",
						enum: ["celsius", "fahrenheit"],
						description: "The temperature unit to use",
					},
				},
				required: ["location"],
			},
		};

		const response = await client.chat.completions.create({
			model: "", // Leave empty for intelligent routing - Adaptive will choose a model that supports function calling
			messages: [
				{
					role: "user",
					content:
						"What's the weather like in Tokyo, Japan? Please use Celsius.",
				},
			],
			tools: [
				{
					type: "function",
					function: weatherFunction,
				},
			],
			tool_choice: "auto", // Let the model decide when to call functions
		});

		console.log("üîÑ Function calling response:");

		// Check if the model wants to call a function
		const message = response.choices[0]?.message;
		if (message?.tool_calls && message.tool_calls.length > 0) {
			console.log("üõ†Ô∏è Function call detected:");

			for (const toolCall of message.tool_calls) {
				if (toolCall.type === "function") {
					console.log(`   ‚Ä¢ Function: ${toolCall.function.name}`);
					console.log(`   ‚Ä¢ Arguments:`, toolCall.function.arguments);

					// Parse the arguments
					const args = JSON.parse(toolCall.function.arguments);

					// Simulate function execution (in real implementation, call actual weather API)
					const functionResponse = {
						location: args.location || "Unknown",
						temperature: "22¬∞C",
						condition: "Partly cloudy",
						humidity: "65%",
						wind: "8 km/h NE",
					};

					console.log(
						"üì° Function response:",
						JSON.stringify(functionResponse, null, 2),
					);

					// For a complete function calling flow, you would send the function result back to the model
					const followUpResponse = await client.chat.completions.create({
						model: "", // Continue with intelligent routing
						messages: [
							{
								role: "user",
								content:
									"What's the weather like in Tokyo, Japan? Please use Celsius.",
							},
							message, // Include the assistant's message with function call
							{
								role: "tool",
								content: JSON.stringify(functionResponse),
								tool_call_id: toolCall.id,
							},
						],
						tools: [
							{
								type: "function",
								function: weatherFunction,
							},
						],
					});

					console.log();
					console.log("üìù Follow-up response:");
					console.log(followUpResponse.choices[0]?.message?.content);
				}
			}
		} else {
			// No function call, just regular response
			console.log("üìù Direct response:");
			console.log(message?.content);
		}

		// Usage information
		if (response.usage) {
			console.log();
			console.log("üìä Usage:");
			console.log(`   ‚Ä¢ Input tokens: ${response.usage.prompt_tokens}`);
			console.log(`   ‚Ä¢ Output tokens: ${response.usage.completion_tokens}`);
			console.log(`   ‚Ä¢ Total tokens: ${response.usage.total_tokens}`);
		}

		console.log();
		console.log("‚ú® Function calling example completed successfully!");
	} catch (error) {
		console.error("‚ùå Function Calling Error:", error);

		if (error instanceof APIError) {
			console.error("üîß API Error Details:");
			console.error("   ‚Ä¢ Status:", error.status);
			console.error("   ‚Ä¢ Message:", error.message);
			if (error.code) {
				console.error("   ‚Ä¢ Code:", error.code);
			}
		}

		throw error;
	}
}

async function main() {
	console.log("üöÄ Starting OpenAI SDK example with Adaptive...");
	console.log("üì° Using endpoint:", client.baseURL);
	console.log();

	try {
		// Run non-streaming example first
		await nonStreamingExample();

		console.log();
		console.log("=".repeat(50));
		console.log();

		// Run streaming example second
		await streamingExample();

		console.log();
		console.log("=".repeat(50));
		console.log();

		// Run function calling example third
		await functionCallingExample();

		console.log();
		console.log("=".repeat(50));
		console.log();
		console.log("üéâ All examples completed successfully!");
		console.log(
			"üí∞ You're likely saving 60-80% compared to direct OpenAI usage.",
		);
		console.log(
			"üîÑ Streaming, non-streaming, and function calling all work seamlessly with Adaptive.",
		);
		console.log(
			"üß† Adaptive intelligently routes to the best available model.",
		);
		console.log();
		console.log("üí° Key features demonstrated:");
		console.log("   ‚Ä¢ OpenAI SDK drop-in compatibility");
		console.log("   ‚Ä¢ Intelligent model routing with empty model string");
		console.log("   ‚Ä¢ Native OpenAI response format preservation");
		console.log("   ‚Ä¢ Automatic cost optimization and provider selection");
		console.log("   ‚Ä¢ Function calling capabilities");
		console.log("   ‚Ä¢ Streaming and non-streaming responses");
	} catch (error) {
		console.error("‚ùå Example failed:", error);
		process.exit(1);
	}
}

// Run the example
main();
