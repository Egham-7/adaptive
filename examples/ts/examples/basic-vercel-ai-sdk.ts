/**
 * Basic Vercel AI SDK Example
 *
 * This example demonstrates how to use Adaptive with Vercel AI SDK's
 * modern AI patterns while getting intelligent model routing and cost optimization.
 *
 * Features demonstrated:
 * - Vercel AI SDK compatibility with Adaptive provider
 * - Both streaming and non-streaming text generation
 * - Intelligent model routing and provider selection
 * - Modern AI patterns (generateText, streamText)
 * - Type-safe responses with Vercel AI SDK types
 *
 * Key benefits:
 * - Use familiar Vercel AI SDK patterns
 * - Get automatic cost optimization and intelligent routing
 * - Seamless integration with React apps using ai/react
 * - Built-in streaming support for real-time UIs
 *
 * Set ADAPTIVE_API_KEY environment variable to avoid hardcoding your API key.
 */

import { createAdaptive } from "@adaptive-llm/adaptive-ai-provider";
import { generateText, streamText, tool } from "ai";
import { z } from "zod";

// Initialize the Adaptive provider for Vercel AI SDK
// This gives you intelligent routing while using Vercel AI SDK patterns
const adaptive = createAdaptive({
  apiKey: process.env.ADAPTIVE_API_KEY || "your-adaptive-api-key",
  baseURL: "https://www.llmadaptive.uk/api/v1",
});

// Non-streaming example using generateText
async function nonStreamingExample() {
  console.log("=== Non-Streaming Vercel AI SDK Example ===");
  console.log(
    "ğŸ’¬ Generating text: 'Explain quantum computing in simple terms'",
  );
  console.log("ğŸ§  Using intelligent routing via Adaptive provider...");
  console.log("â³ Waiting for complete response...");

  try {
    const { text, usage, finishReason } = await generateText({
      model: adaptive(), // Adaptive automatically selects the best model
      prompt: "Explain quantum computing in simple terms",
    });

    if (!text) {
      console.log("âŒ No response text received");
      return;
    }

    // Display the response
    console.log();
    console.log("âœ… Response received:");
    console.log("ğŸ“ Content:", text);
    console.log("ğŸ Finish reason:", finishReason);

    // Usage information (if available)
    if (usage) {
      console.log("ğŸ“Š Usage:");
      console.log(`   â€¢ Prompt tokens: ${usage.inputTokens}`);
      console.log(`   â€¢ Completion tokens: ${usage.outputTokens}`);
      console.log(`   â€¢ Total tokens: ${usage.totalTokens}`);
    }

    console.log();
    console.log("âœ¨ Non-streaming example completed successfully!");
  } catch (error) {
    console.error("âŒ Non-streaming Error:", error);
    throw error;
  }
}

// Streaming example using streamText
async function streamingExample() {
  console.log("=== Streaming Vercel AI SDK Example ===");
  console.log(
    "ğŸ’¬ Streaming text: 'Write a creative short story about AI and humanity'",
  );
  console.log("ğŸ§  Using intelligent routing with streaming...");
  console.log("ğŸ“¡ Response will appear in real-time:");
  console.log();

  try {
    const { textStream, usage, finishReason } = streamText({
      model: adaptive(), // Adaptive automatically selects the best model
      prompt: "Write a creative short story about AI and humanity",
    });

    let fullContent = "";
    let chunkCount = 0;

    console.log("ğŸ”„ Streaming response:");
    process.stdout.write("ğŸ“ ");

    // Stream the text chunks
    for await (const textChunk of textStream) {
      process.stdout.write(textChunk);
      fullContent += textChunk;
      chunkCount++;
    }

    // Wait for final metadata
    const finalUsage = await usage;
    const finalFinishReason = await finishReason;

    console.log(); // New line after content
    console.log();
    console.log("âœ… Streaming completed!");
    console.log("ğŸ Finish reason:", finalFinishReason);

    console.log("ğŸ“Š Streaming statistics:");
    console.log(`   â€¢ Text chunks received: ${chunkCount}`);
    console.log(`   â€¢ Total content length: ${fullContent.length} characters`);

    // Usage information
    if (finalUsage) {
      console.log("ğŸ“Š Final usage:");
      console.log(`   â€¢ Prompt tokens: ${finalUsage.inputTokens}`);
      console.log(`   â€¢ Completion tokens: ${finalUsage.outputTokens}`);
      console.log(`   â€¢ Total tokens: ${finalUsage.totalTokens}`);
    }

    console.log();
    console.log("âœ¨ Streaming example completed successfully!");
  } catch (error) {
    console.error("âŒ Streaming Error:", error);
    throw error;
  }
}

// Advanced example with tool calling
async function toolCallingExample() {
  console.log("=== Tool Calling Example ===");
  console.log(
    "ğŸ’¬ Testing tool calling: 'What's the weather like in San Francisco?'",
  );
  console.log("ğŸ”§ Using tools with intelligent model selection...");
  console.log(
    "â³ Adaptive will choose a model that supports function calling...",
  );

  try {
    const { text, toolCalls, finishReason } = await generateText({
      model: adaptive(), // Adaptive prioritizes models with function calling support
      prompt:
        "What's the weather like in San Francisco? Please use the weather tool.",
      tools: {
        getWeather: tool({
          description: "Get the weather in a location",
          inputSchema: z.object({
            location: z
              .string()
              .describe("The location to get the weather for"),
          }),
          execute: async ({ location }: { location: string }) => ({
            location,
            temperature: 72 + Math.floor(Math.random() * 21) - 10,
          }),
        }),
      },
    });

    console.log();
    console.log("âœ… Tool calling completed:");
    console.log("ğŸ“ Final response:", text);
    console.log("ğŸ Finish reason:", finishReason);

    if (toolCalls && toolCalls.length > 0) {
      console.log("ğŸ”§ Tools called:");
      toolCalls.forEach((call, index) => {
        console.log(`   ${index + 1}. ${call.toolName} with args:`, call.input);
      });
    }

    console.log();
    console.log("âœ¨ Tool calling example completed successfully!");
  } catch (error) {
    console.error("âŒ Tool calling Error:", error);
    throw error;
  }
}

async function main() {
  console.log("ğŸš€ Starting Vercel AI SDK example with Adaptive...");
  console.log("ğŸ“¦ Using Adaptive provider with Vercel AI SDK");
  console.log("ğŸ”§ Modern AI patterns with intelligent routing");
  console.log();

  try {
    // Run non-streaming example first
    await nonStreamingExample();

    console.log();
    console.log("=".repeat(50));
    console.log();

    // Run streaming example second
    await streamingExample();

    console.log();
    console.log("=".repeat(50));
    console.log();

    // Run tool calling example
    await toolCallingExample();

    console.log();
    console.log("=".repeat(50));
    console.log();
    console.log("ğŸ‰ All Vercel AI SDK examples completed successfully!");
    console.log("ğŸ’° You're getting intelligent routing and cost optimization.");
    console.log("ğŸ”„ Perfect for React apps with ai/react hooks.");
    console.log("ğŸ§  Adaptive works seamlessly with Vercel AI SDK patterns.");
    console.log();
    console.log("ğŸ’¡ Next steps:");
    console.log("â€¢ Use these patterns in your React components");
    console.log("â€¢ Try the useChat, useCompletion hooks with Adaptive");
    console.log("â€¢ Leverage streaming for better user experience");
  } catch (error) {
    console.error("âŒ Example failed:", error);
    process.exit(1);
  }
}

// Run the example
main();
