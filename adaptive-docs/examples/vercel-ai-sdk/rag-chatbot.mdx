---
title: 'Vercel AI SDK RAG Chatbot'
description: 'Build a streaming RAG chatbot with Vercel AI SDK and Adaptive routing'
---

## Overview

This example demonstrates how to build a modern RAG (Retrieval-Augmented Generation) chatbot using the Vercel AI SDK with Adaptive's intelligent routing. The implementation features real-time streaming, React components, and Next.js integration.

## Prerequisites

- Next.js 14+
- Node.js 18+
- Adaptive API key
- OpenAI API key (for embeddings)

## Project Setup

### Installation

<CodeGroup>

```bash npm
npm install ai @ai-sdk/openai @langchain/openai @langchain/community faiss-node pdf-parse
npm install -D @types/node
```

```bash yarn
yarn add ai @ai-sdk/openai @langchain/openai @langchain/community faiss-node pdf-parse
yarn add -D @types/node
```

</CodeGroup>

### Environment Variables

```bash .env.local
ADAPTIVE_API_KEY=your-adaptive-api-key
OPENAI_API_KEY=your-openai-api-key
```

## Backend Implementation

### Vector Store Service

```typescript lib/vectorstore.ts
import { OpenAIEmbeddings } from "@langchain/openai";
import { FaissStore } from "@langchain/community/vectorstores/faiss";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { Document } from "@langchain/core/documents";
import { PDFLoader } from "langchain/document_loaders/fs/pdf";
import { TextLoader } from "langchain/document_loaders/fs/text";
import * as fs from "fs";
import * as path from "path";

export class VectorStoreService {
  private embeddings: OpenAIEmbeddings;
  private vectorStore: FaissStore | null = null;
  private documentsPath: string;

  constructor(documentsPath: string = "./documents") {
    this.embeddings = new OpenAIEmbeddings({
      apiKey: process.env.OPENAI_API_KEY,
    });
    this.documentsPath = documentsPath;
  }

  async loadDocuments(): Promise<Document[]> {
    const documents: Document[] = [];

    if (!fs.existsSync(this.documentsPath)) {
      throw new Error(`Documents directory ${this.documentsPath} not found`);
    }

    const files = fs.readdirSync(this.documentsPath, { recursive: true });

    for (const file of files) {
      const filePath = path.join(this.documentsPath, file as string);
      
      if (!fs.statSync(filePath).isFile()) continue;

      try {
        let loader;
        const ext = path.extname(filePath).toLowerCase();

        if (ext === ".pdf") {
          loader = new PDFLoader(filePath);
        } else if ([".txt", ".md"].includes(ext)) {
          loader = new TextLoader(filePath);
        } else {
          continue;
        }

        const docs = await loader.load();
        docs.forEach(doc => {
          doc.metadata.source = path.basename(filePath);
        });
        
        documents.push(...docs);
        console.log(`Loaded ${docs.length} chunks from ${path.basename(filePath)}`);

      } catch (error) {
        console.error(`Error loading ${filePath}:`, error);
      }
    }

    return documents;
  }

  async createVectorStore(): Promise<void> {
    console.log("Creating vector store...");

    const documents = await this.loadDocuments();

    const textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    });

    const texts = await textSplitter.splitDocuments(documents);
    
    this.vectorStore = await FaissStore.fromDocuments(texts, this.embeddings);
    await this.vectorStore.save("./vector_index");
    
    console.log(`Vector store created with ${texts.length} chunks`);
  }

  async loadVectorStore(): Promise<void> {
    try {
      this.vectorStore = await FaissStore.load("./vector_index", this.embeddings);
      console.log("Loaded existing vector store");
    } catch (error) {
      console.log("Creating new vector store...");
      await this.createVectorStore();
    }
  }

  async similaritySearch(query: string, k: number = 4): Promise<Document[]> {
    if (!this.vectorStore) {
      await this.loadVectorStore();
    }

    return this.vectorStore!.similaritySearch(query, k);
  }

  async getRelevantContext(query: string): Promise<string> {
    const docs = await this.similaritySearch(query);
    
    return docs
      .map((doc, index) => 
        `[Source ${index + 1}: ${doc.metadata.source}]\n${doc.pageContent}`
      )
      .join("\n\n");
  }
}

// Singleton instance
let vectorStoreInstance: VectorStoreService | null = null;

export function getVectorStore(): VectorStoreService {
  if (!vectorStoreInstance) {
    vectorStoreInstance = new VectorStoreService();
  }
  return vectorStoreInstance;
}
```

### Chat API Route

```typescript app/api/chat/route.ts
import { openai } from '@ai-sdk/openai';
import { streamText, convertToCoreMessages } from 'ai';
import { getVectorStore } from '@/lib/vectorstore';

// Configure Adaptive OpenAI
const adaptiveOpenAI = openai({
  apiKey: process.env.ADAPTIVE_API_KEY!,
  baseURL: 'https://llmadaptive.uk/api/v1',
});

export async function POST(req: Request) {
  try {
    const { messages } = await req.json();
    
    // Get the latest user message for context retrieval
    const lastMessage = messages[messages.length - 1];
    if (lastMessage.role !== 'user') {
      return new Response('Invalid message format', { status: 400 });
    }

    // Retrieve relevant context from vector store
    const vectorStore = getVectorStore();
    const context = await vectorStore.getRelevantContext(lastMessage.content);

    // Create system message with context
    const systemMessage = {
      role: 'system' as const,
      content: `You are a helpful AI assistant that answers questions based on the provided context from documents. 

CONTEXT FROM DOCUMENTS:
${context}

Instructions:
1. Answer based primarily on the provided context
2. If the information is not in the context, clearly state this
3. Be specific and cite sources when possible
4. Maintain a helpful and professional tone
5. If asked about previous conversation, use the chat history

If no relevant context is found, politely explain that you don't have information about that topic in the current document collection.`
    };

    // Combine system message with conversation
    const messagesWithContext = [
      systemMessage,
      ...convertToCoreMessages(messages)
    ];

    // Stream response with Adaptive routing
    const result = await streamText({
      model: adaptiveOpenAI(''), // Empty string for intelligent routing
      messages: messagesWithContext,
      temperature: 0.3, // Lower temperature for factual responses
      maxTokens: 1000,
    });

    return result.toAIStreamResponse();

  } catch (error) {
    console.error('Chat API error:', error);
    return new Response(
      JSON.stringify({ error: 'Internal server error' }),
      { 
        status: 500, 
        headers: { 'Content-Type': 'application/json' } 
      }
    );
  }
}
```

### Vector Store Initialization API

```typescript app/api/setup/route.ts
import { getVectorStore } from '@/lib/vectorstore';

export async function POST() {
  try {
    const vectorStore = getVectorStore();
    await vectorStore.loadVectorStore();

    return Response.json({ 
      success: true, 
      message: 'Vector store initialized successfully' 
    });
  } catch (error) {
    console.error('Setup error:', error);
    return Response.json(
      { error: 'Failed to initialize vector store' },
      { status: 500 }
    );
  }
}

export async function GET() {
  return Response.json({ 
    message: 'Vector store setup endpoint. Use POST to initialize.' 
  });
}
```

## Frontend Implementation

### Main Chat Component

```tsx components/RAGChatbot.tsx
'use client';

import { useChat } from 'ai/react';
import { useState, useEffect, useRef } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Loader2, Send, FileText, Bot, User } from 'lucide-react';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
}

export default function RAGChatbot() {
  const [isSetup, setIsSetup] = useState(false);
  const [isSettingUp, setIsSettingUp] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: '/api/chat',
    onError: (error) => {
      console.error('Chat error:', error);
    },
  });

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const setupVectorStore = async () => {
    setIsSettingUp(true);
    try {
      const response = await fetch('/api/setup', {
        method: 'POST',
      });

      if (response.ok) {
        setIsSetup(true);
      } else {
        throw new Error('Setup failed');
      }
    } catch (error) {
      console.error('Setup error:', error);
      alert('Failed to setup vector store. Please try again.');
    } finally {
      setIsSettingUp(false);
    }
  };

  if (!isSetup) {
    return (
      <Card className="w-full max-w-2xl mx-auto">
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            <FileText className="h-6 w-6" />
            RAG Document Chatbot
          </CardTitle>
        </CardHeader>
        <CardContent className="space-y-4">
          <p className="text-gray-600">
            Initialize the document knowledge base to start chatting about your documents.
          </p>
          <Button 
            onClick={setupVectorStore} 
            disabled={isSettingUp}
            className="w-full"
          >
            {isSettingUp ? (
              <>
                <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                Setting up knowledge base...
              </>
            ) : (
              'Initialize Knowledge Base'
            )}
          </Button>
        </CardContent>
      </Card>
    );
  }

  return (
    <Card className="w-full max-w-4xl mx-auto h-[80vh] flex flex-col">
      <CardHeader className="border-b">
        <CardTitle className="flex items-center gap-2">
          <Bot className="h-6 w-6 text-blue-600" />
          Document Q&A Assistant
          <span className="ml-auto text-sm font-normal text-gray-500">
            Powered by Adaptive AI
          </span>
        </CardTitle>
      </CardHeader>

      <CardContent className="flex-1 flex flex-col p-0">
        <ScrollArea className="flex-1 p-6">
          <div className="space-y-4">
            {messages.length === 0 && (
              <div className="text-center text-gray-500 py-8">
                <Bot className="h-12 w-12 mx-auto mb-4 text-gray-300" />
                <p className="text-lg font-medium">Ask me anything about your documents!</p>
                <p className="text-sm">I'll search through your knowledge base to provide accurate answers.</p>
              </div>
            )}

            {messages.map((message) => (
              <div
                key={message.id}
                className={`flex items-start gap-3 ${
                  message.role === 'user' ? 'justify-end' : 'justify-start'
                }`}
              >
                {message.role === 'assistant' && (
                  <div className="flex-shrink-0 w-8 h-8 bg-blue-100 rounded-full flex items-center justify-center">
                    <Bot className="h-4 w-4 text-blue-600" />
                  </div>
                )}
                
                <div
                  className={`max-w-[80%] rounded-lg px-4 py-2 ${
                    message.role === 'user'
                      ? 'bg-blue-600 text-white'
                      : 'bg-gray-100 text-gray-900'
                  }`}
                >
                  <div className="whitespace-pre-wrap">{message.content}</div>
                </div>

                {message.role === 'user' && (
                  <div className="flex-shrink-0 w-8 h-8 bg-gray-200 rounded-full flex items-center justify-center">
                    <User className="h-4 w-4 text-gray-600" />
                  </div>
                )}
              </div>
            ))}

            {isLoading && (
              <div className="flex items-start gap-3">
                <div className="flex-shrink-0 w-8 h-8 bg-blue-100 rounded-full flex items-center justify-center">
                  <Bot className="h-4 w-4 text-blue-600" />
                </div>
                <div className="bg-gray-100 rounded-lg px-4 py-2">
                  <div className="flex items-center gap-2">
                    <Loader2 className="h-4 w-4 animate-spin" />
                    <span>Searching documents and generating response...</span>
                  </div>
                </div>
              </div>
            )}

            <div ref={messagesEndRef} />
          </div>
        </ScrollArea>

        <div className="border-t p-4">
          <form onSubmit={handleSubmit} className="flex gap-2">
            <Input
              value={input}
              onChange={handleInputChange}
              placeholder="Ask me about your documents..."
              disabled={isLoading}
              className="flex-1"
            />
            <Button type="submit" disabled={isLoading || !input.trim()}>
              <Send className="h-4 w-4" />
            </Button>
          </form>
        </div>
      </CardContent>
    </Card>
  );
}
```

### Enhanced Chat with Source Citations

```tsx components/EnhancedRAGChat.tsx
'use client';

import { useChat } from 'ai/react';
import { useState, useEffect } from 'react';
import { Card } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { ExternalLink, FileText } from 'lucide-react';

interface EnhancedMessage extends Message {
  sources?: string[];
  provider?: string;
}

export default function EnhancedRAGChat() {
  const [sources, setSources] = useState<Record<string, string[]>>({});

  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({
    api: '/api/enhanced-chat',
    onFinish: (message) => {
      // Extract sources from the response if provided
      // This would be handled by your API route
    },
  });

  return (
    <div className="w-full max-w-4xl mx-auto space-y-4">
      {messages.map((message) => (
        <Card key={message.id} className="p-4">
          <div className="flex items-start gap-3">
            <div className="flex-1">
              <div className="whitespace-pre-wrap">{message.content}</div>
              
              {/* Show sources for assistant messages */}
              {message.role === 'assistant' && sources[message.id] && (
                <div className="mt-3 pt-3 border-t">
                  <div className="text-sm font-medium text-gray-700 mb-2">
                    Sources:
                  </div>
                  <div className="flex flex-wrap gap-2">
                    {sources[message.id].map((source, index) => (
                      <Badge 
                        key={index} 
                        variant="secondary" 
                        className="flex items-center gap-1"
                      >
                        <FileText className="h-3 w-3" />
                        {source}
                      </Badge>
                    ))}
                  </div>
                </div>
              )}
            </div>
          </div>
        </Card>
      ))}

      {/* Chat input form */}
      <Card className="p-4">
        <form onSubmit={handleSubmit} className="flex gap-2">
          <input
            value={input}
            onChange={handleInputChange}
            placeholder="Ask about your documents..."
            disabled={isLoading}
            className="flex-1 px-3 py-2 border rounded-md"
          />
          <button
            type="submit"
            disabled={isLoading || !input.trim()}
            className="px-4 py-2 bg-blue-600 text-white rounded-md disabled:opacity-50"
          >
            Send
          </button>
        </form>
      </Card>
    </div>
  );
}
```

## Advanced Features

### Multi-Modal RAG with Image Support

```typescript app/api/multimodal-chat/route.ts
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { getVectorStore } from '@/lib/vectorstore';

export async function POST(req: Request) {
  try {
    const { messages, imageData } = await req.json();
    
    const lastMessage = messages[messages.length - 1];
    const vectorStore = getVectorStore();
    
    // Get text context
    const textContext = await vectorStore.getRelevantContext(lastMessage.content);
    
    // Prepare message content with optional image
    const messageContent = imageData 
      ? [
          { type: 'text', text: lastMessage.content },
          { 
            type: 'image_url', 
            image_url: { url: imageData } 
          }
        ]
      : lastMessage.content;

    const systemMessage = {
      role: 'system' as const,
      content: `You are a helpful assistant that can analyze both text documents and images.

DOCUMENT CONTEXT:
${textContext}

Instructions:
1. If an image is provided, analyze it in conjunction with the document context
2. Answer questions based on both visual information and document content
3. Be specific about what you can see in images and how it relates to the documents
4. Cite sources from documents when relevant`
    };

    const result = await streamText({
      model: adaptiveOpenAI(''), // Will route to vision-capable model when needed
      messages: [
        systemMessage,
        { role: 'user', content: messageContent }
      ],
      temperature: 0.3,
    });

    return result.toAIStreamResponse();

  } catch (error) {
    console.error('Multimodal chat error:', error);
    return new Response('Error processing request', { status: 500 });
  }
}
```

### Conversation Memory with Summarization

```typescript lib/conversationMemory.ts
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

export class ConversationMemory {
  private adaptiveOpenAI;
  private conversationHistory: string[] = [];
  private maxHistoryLength = 10;

  constructor() {
    this.adaptiveOpenAI = openai({
      apiKey: process.env.ADAPTIVE_API_KEY!,
      baseURL: 'https://llmadaptive.uk/api/v1',
    });
  }

  addMessage(role: 'user' | 'assistant', content: string) {
    this.conversationHistory.push(`${role}: ${content}`);
    
    if (this.conversationHistory.length > this.maxHistoryLength) {
      this.summarizeHistory();
    }
  }

  private async summarizeHistory() {
    const historyText = this.conversationHistory.slice(0, -2).join('\n');
    
    const { text } = await generateText({
      model: this.adaptiveOpenAI(''),
      prompt: `Summarize this conversation history in 2-3 sentences, focusing on key topics and context:

${historyText}

Summary:`,
      temperature: 0.3,
    });

    // Keep summary + recent messages
    this.conversationHistory = [
      `Summary: ${text}`,
      ...this.conversationHistory.slice(-2)
    ];
  }

  getContextString(): string {
    return this.conversationHistory.join('\n');
  }

  clear() {
    this.conversationHistory = [];
  }
}
```

## Production Optimizations

### Caching and Performance

```typescript lib/cache.ts
import { Redis } from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

export class RAGCache {
  private static instance: RAGCache;
  
  static getInstance(): RAGCache {
    if (!RAGCache.instance) {
      RAGCache.instance = new RAGCache();
    }
    return RAGCache.instance;
  }

  async getCachedResponse(query: string): Promise<string | null> {
    const cacheKey = `rag:${Buffer.from(query).toString('base64')}`;
    return await redis.get(cacheKey);
  }

  async cacheResponse(query: string, response: string, ttl = 3600): Promise<void> {
    const cacheKey = `rag:${Buffer.from(query).toString('base64')}`;
    await redis.setex(cacheKey, ttl, response);
  }

  async getCachedContext(query: string): Promise<string | null> {
    const cacheKey = `context:${Buffer.from(query).toString('base64')}`;
    return await redis.get(cacheKey);
  }

  async cacheContext(query: string, context: string, ttl = 1800): Promise<void> {
    const cacheKey = `context:${Buffer.from(query).toString('base64')}`;
    await redis.setex(cacheKey, ttl, context);
  }
}
```

### Rate Limiting and Error Handling

```typescript lib/rateLimiter.ts
import { Ratelimit } from '@upstash/ratelimit';
import { Redis } from '@upstash/redis';

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});

export const rateLimiter = new Ratelimit({
  redis: redis,
  limiter: Ratelimit.slidingWindow(10, '1 m'), // 10 requests per minute
  analytics: true,
});

export async function withRateLimit(req: Request, handler: () => Promise<Response>) {
  const ip = req.headers.get('x-forwarded-for') ?? 'anonymous';
  const { success, limit, reset, remaining } = await rateLimiter.limit(ip);

  if (!success) {
    return new Response('Rate limit exceeded', { 
      status: 429,
      headers: {
        'X-RateLimit-Limit': limit.toString(),
        'X-RateLimit-Remaining': remaining.toString(),
        'X-RateLimit-Reset': new Date(reset).toISOString(),
      }
    });
  }

  return handler();
}
```

## Usage Examples

### Page Integration

```tsx app/page.tsx
import RAGChatbot from '@/components/RAGChatbot';

export default function HomePage() {
  return (
    <div className="container mx-auto py-8">
      <div className="text-center mb-8">
        <h1 className="text-3xl font-bold">Document Q&A Assistant</h1>
        <p className="text-gray-600 mt-2">
          Ask questions about your documents and get intelligent answers
        </p>
      </div>
      
      <RAGChatbot />
    </div>
  );
}
```

### API Testing

```typescript scripts/test-rag.ts
async function testRAG() {
  const response = await fetch('http://localhost:3000/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      messages: [
        { role: 'user', content: 'What are the main themes in the documents?' }
      ]
    })
  });

  const reader = response.body?.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader!.read();
    if (done) break;
    
    const chunk = decoder.decode(value);
    process.stdout.write(chunk);
  }
}

testRAG().catch(console.error);
```

## Best Practices

<CardGroup cols={2}>
  <Card title="Streaming UX" icon="zap">
    Use Vercel AI SDK's streaming capabilities to show responses as they generate for better user experience.
  </Card>
  <Card title="Context Management" icon="layers">
    Balance context relevance with token limits. Use semantic search and reranking for better retrieval.
  </Card>
  <Card title="Error Recovery" icon="shield">
    Implement graceful fallbacks when vector search fails or when no relevant context is found.
  </Card>
  <Card title="Performance" icon="gauge">
    Cache frequent queries and implement request debouncing to reduce API calls and improve response times.
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Advanced Agents" icon="robot" href="/examples/vercel-agents">
    Build multi-agent systems with specialized RAG capabilities
  </Card>
  <Card title="Real-time Collaboration" icon="users" href="/examples/collaborative-rag">
    Implement real-time collaborative document analysis
  </Card>
  <Card title="Production Scaling" icon="trending-up" href="/examples/production-rag">
    Scale RAG systems for enterprise workloads
  </Card>
  <Card title="Custom Embeddings" icon="brain" href="/examples/custom-embeddings">
    Implement domain-specific embedding models
  </Card>
</CardGroup>