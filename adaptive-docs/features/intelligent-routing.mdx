---
title: "Intelligent Routing"
description: "How Adaptive automatically selects the best AI model for each request"
---

## Overview

Adaptive's intelligent routing system analyzes every request and automatically selects the optimal AI model and provider based on the content, complexity, cost, and performance requirements. This happens transparently - you just send requests and get back the best possible responses at the lowest cost.

## How It Works

<Steps>
  <Step title="Prompt Classification">
    Our neural classifier (based on NVIDIA's DeBERTa v3) analyzes your prompt to
    determine task type, complexity score, reasoning requirements, creativity
    scope, and domain knowledge needs
  </Step>
  <Step title="Task-Specific Routing">
    The system matches your classified task type to curated model rankings
    optimized for that specific use case (e.g., Code Generation → DeepSeek,
    Creative Writing → Grok)
  </Step>
  <Step title="Capability Filtering">
    Filters candidate models based on context length requirements, function
    calling needs, and provider constraints
  </Step>
  <Step title="Cost-Performance Selection">
    Applies your cost_bias preference to select the optimal model from ranked
    candidates, balancing cost efficiency with quality
  </Step>
</Steps>

## Routing Factors

### Neural Classification System

Our classification system analyzes multiple dimensions of your request:

<CardGroup cols={2}>
  <Card title="Task Type Classification" icon="brain">
    Identifies 11 task types: Open QA, Code Generation, Summarization, Text
    Generation, Chatbot, Closed QA, Classification, Rewrite, Brainstorming,
    Extraction, and Other
  </Card>
  <Card title="Complexity Scoring" icon="calculator">
    Calculates weighted complexity score based on creativity scope, reasoning
    requirements, constraint complexity, domain knowledge, and contextual
    knowledge needs
  </Card>
  <Card title="Reasoning Analysis" icon="lightbulb">
    Measures logical reasoning, mathematical thinking, and analytical depth
    requirements from 0.0 to 1.0
  </Card>
  <Card title="Domain Expertise" icon="graduation-cap">
    Evaluates specialized knowledge requirements and contextual understanding
    needs
  </Card>
</CardGroup>

### Active Provider Capabilities

Currently active providers and their specialized strengths:

| Provider     | Available Models                     | Strengths                                      | Best For                                                    |
| ------------ | ------------------------------------ | ---------------------------------------------- | ----------------------------------------------------------- |
| **OpenAI**   | gpt-4o, gpt-4o-mini, gpt-4.1, o3, o1 | General purpose, function calling, reliability | Balanced tasks, tool use, production workloads              |
| **DeepSeek** | deepseek-chat, deepseek-reasoner     | Code generation, mathematical reasoning        | Programming, technical analysis, cost-effective solutions   |
| **Grok**     | grok-3, grok-3-mini                  | Ultra-fast inference, creative tasks           | Real-time applications, creative writing, conversational AI |
| **Groq**     | Fast inference models (HuggingFace)  | Extremely low latency                          | High-throughput applications, specialized tasks             |

<Note>
  Provider availability is dynamically managed. Currently active providers:
  OpenAI, DeepSeek, Grok, and Groq (for specialized models).
</Note>

## Task-Specific Routing Examples

### Open QA (Factual Questions)

**Input:** "What's the capital of France?"

**Classification:** Task Type: Open QA, Complexity: 0.1, Reasoning: 0.2
**Routing Decision:** → **gpt-4o-mini**

- **Why:** Simple factual query, prioritizes cost efficiency
- **Cost:** $0.15 per 1M input tokens vs $2.50 for gpt-4o
- **Ranking:** gpt-4o → deepseek-chat → gpt-4.1 → grok-3 → gpt-4o-mini → grok-3-mini

### Code Generation

**Input:** "Write a Python function to implement binary search with error handling"

**Classification:** Task Type: Code Generation, Complexity: 0.6, Reasoning: 0.7
**Routing Decision:** → **deepseek-chat**

- **Why:** DeepSeek ranks #1 for code generation tasks
- **Cost:** $0.14 per 1M input tokens, optimized for programming
- **Ranking:** deepseek-chat → gpt-4o → deepseek-reasoner → gpt-4.1 → grok-3 → o3

### Text Generation (Creative)

**Input:** "Write a compelling marketing story about sustainable technology"

**Classification:** Task Type: Text Generation, Complexity: 0.7, Creativity: 0.8
**Routing Decision:** → **grok-3**

- **Why:** Grok excels at creative content generation
- **Trade-off:** Higher cost ($3.00/1M) but superior creative output
- **Ranking:** gpt-4o → grok-3 → deepseek-chat → gpt-4.1 → o1

### Brainstorming

**Input:** "Generate innovative ideas for reducing urban traffic congestion"

**Classification:** Task Type: Brainstorming, Complexity: 0.8, Creativity: 0.9
**Routing Decision:** → **o1** (high cost_bias) or **grok-3** (balanced)

- **Why:** Requires high creativity and innovative thinking
- **Complexity weights:** Creativity: 50%, Reasoning: 20%, others: 10% each
- **Ranking:** gpt-4o → grok-3 → o1 → deepseek-reasoner

## Controlling Routing Behavior

### Cost Bias Parameter

Control the cost vs performance trade-off:

```javascript
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Explain machine learning" }],
  cost_bias: 0.2, // 0 = cheapest, 1 = best performance
});
```

<CardGroup cols={3}>
  <Card title="cost_bias: 0.0-0.3" icon="dollar-sign">
    **Maximum Savings** Routes to cheapest compatible models. Great for simple
    tasks, high volume.
  </Card>
  <Card title="cost_bias: 0.4-0.6" icon="balance-scale">
    **Balanced (Default)** Optimal balance of cost and quality for most use
    cases.
  </Card>
  <Card title="cost_bias: 0.7-1.0" icon="star">
    **Maximum Quality** Routes to best performing models regardless of cost. For
    critical tasks.
  </Card>
</CardGroup>

### Provider Constraints

Limit which providers can be selected:

```javascript
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Write code to sort an array" }],
  provider_constraints: ["deepseek", "anthropic"], // Only these providers
});
```

**Use cases for provider constraints:**

- **Compliance:** Limit to specific providers for regulatory requirements
- **Performance:** Force selection from fastest providers (e.g., Groq)
- **Quality:** Restrict to premium providers for important tasks
- **Cost:** Limit to most economical providers

## Real-World Performance

### Routing Accuracy

Our routing system achieves:

- **94% accuracy** in selecting optimal models for cost savings
- **97% user satisfaction** with routing decisions
- 2ms latency for routing decisions
- **99.9% uptime** across all providers

### Cost Savings by Category

<CardGroup cols={2}>
  <Card title="Simple Q&A" icon="message-circle">
    **Average savings: 92%** "What is X?", "How do I Y?" → Gemini 2.0 Flash
  </Card>
  <Card title="Code Tasks" icon="code">
    **Average savings: 78%** Programming, debugging → DeepSeek, Llama models
  </Card>
  <Card title="Creative Writing" icon="pen-tool">
    **Average savings: 65%** Stories, marketing copy → Various optimized models
  </Card>
  <Card title="Analysis Tasks" icon="search">
    **Average savings: 45%** Research, reasoning → Premium models when needed
  </Card>
</CardGroup>

## Routing Transparency

### Provider Field

Every response includes which provider was selected:

```json
{
  "id": "chatcmpl-123",
  "model": "gemini-2.0-flash",
  "provider": "gemini",  // ← Shows routing decision
  "choices": [...],
  "usage": {...}
}
```

### Dashboard Analytics

Monitor routing decisions in your dashboard:

- **Provider distribution** - See which providers are used most
- **Cost savings** - Track savings vs. using premium models only
- **Request patterns** - Understand your usage patterns
- **Model performance** - Compare quality across different routing decisions

## Advanced Routing Features

### Context-Aware Routing

The system considers conversation context:

```javascript
// Initial message routes to efficient model
{ role: 'user', content: 'Hello' }
// → Gemini 2.0 Flash

// Follow-up with complexity routes to better model
{ role: 'user', content: 'Now analyze the geopolitical implications...' }
// → Claude 3.5 Sonnet
```

### Load Balancing

Automatic load balancing across providers:

- **Provider availability** - Routes around outages or slow responses
- **Rate limits** - Distributes load to avoid hitting limits
- **Regional optimization** - Routes to geographically optimal providers

### Quality Monitoring

Continuous improvement of routing decisions:

- **Response quality tracking** - Monitors output quality across providers
- **User feedback integration** - Learns from usage patterns
- **Model performance updates** - Adapts to new model releases

## Best Practices

<CardGroup cols={2}>
  <Card title="Trust the System" icon="check-circle">
    Let intelligent routing work automatically. Manual model selection often
    performs worse and costs more.
  </Card>
  <Card title="Use Cost Bias" icon="sliders">
    Adjust `cost_bias` based on your use case rather than constraining
    providers.
  </Card>
  <Card title="Monitor Performance" icon="bar-chart">
    Use dashboard analytics to understand routing patterns and optimize your
    usage.
  </Card>
  <Card title="Test Different Settings" icon="test-tube">
    Experiment with different `cost_bias` values to find your optimal balance.
  </Card>
</CardGroup>

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="How fast is the routing decision?">
    Routing decisions are made in under 50ms on average. The total request latency is primarily determined by the selected model's inference time, not the routing overhead.
  </Accordion>

<Accordion title="Can I see which model was selected?">
  Yes! Every response includes a `provider` field showing which provider was
  selected. You can also see detailed analytics in your dashboard.
</Accordion>

<Accordion title="What if my preferred provider is down?">
  Intelligent routing automatically handles provider outages by selecting
  alternative providers that can handle your request. This provides better
  reliability than using a single provider.
</Accordion>

<Accordion title="How does routing work with streaming?">
  Routing decisions are made before streaming begins. The entire response
  streams from the selected provider, maintaining the same low-latency
  experience.
</Accordion>

  <Accordion title="Can I force a specific model?">
    While you can use provider constraints to limit options, we recommend letting intelligent routing optimize for you. Manual model selection typically results in higher costs and lower performance.
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Cost Optimization"
    icon="dollar-sign"
    href="/features/cost-optimization"
  >
    Learn more about cost savings strategies
  </Card>
  <Card
    title="Provider Constraints"
    icon="filter"
    href="/features/provider-constraints"
  >
    Control which providers can be selected
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/chat-completions"
  >
    See all routing parameters in the API docs
  </Card>
  <Card title="Examples" icon="code" href="/examples/basic-chat">
    Working examples of intelligent routing
  </Card>
</CardGroup>

