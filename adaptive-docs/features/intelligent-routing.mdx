---
title: "Intelligent Routing"
description: "Automatic model selection for optimal cost and performance"
icon: "route"
---

Adaptive analyzes your requests and automatically routes to the best model. Set `model: ""` to enable intelligent routing.

## Quick Examples

```javascript
// Automatic routing (recommended)
const completion = await openai.chat.completions.create({
  model: "", // Let Adaptive choose
  messages: [{ role: "user", content: "Hello!" }]
});
```

## Routing Results

- **Simple Q&A** → gpt-4o-mini (92% cost savings)
- **Code Generation** → deepseek-chat (78% cost savings)  
- **Creative Writing** → grok-3 (65% cost savings)

## Configuration

### Cost vs Performance

<CodeGroup>

```javascript JavaScript
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Hello" }],
  model_router: {
    cost_bias: 0.2 // 0 = cheapest, 1 = best performance
  }
});
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Hello"}],
    model_router={
        "cost_bias": 0.2  # 0 = cheapest, 1 = best performance
    }
)
```

```bash cURL
curl https://llmadaptive.uk/api/v1/chat/completions \
  -H "X-Stainless-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "Hello"}],
    "model_router": {
      "cost_bias": 0.2
    }
  }'
```

</CodeGroup>

### Limit Providers

<CodeGroup>

```javascript JavaScript
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Hello" }],
  model_router: {
    models: [
      { provider: "openai" },
      { provider: "anthropic", model_name: "claude-3-sonnet" }
    ]
  }
});
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Hello"}],
    model_router={
        "models": [
            {"provider": "openai"},
            {"provider": "anthropic", "model_name": "claude-3-sonnet"}
        ]
    }
)
```

```bash cURL
curl https://llmadaptive.uk/api/v1/chat/completions \
  -H "X-Stainless-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "Hello"}],
    "model_router": {
      "models": [
        {"provider": "openai"},
        {"provider": "anthropic", "model_name": "claude-3-sonnet"}
      ]
    }
  }'
```

</CodeGroup>

## Performance

- 94% accuracy in model selection
- Under 1ms model selection
- 99.9% uptime

## Response Format

```json
{
  "model": "",
  "provider": "openai", // Shows selected provider
  "choices": [...],
  "usage": {...}
}
```

## Preview Routing Decisions

Want to see which model Adaptive would choose before running the completion? Use the [Select Model endpoint](/api-reference/select-model):

<CodeGroup>

```javascript JavaScript
// Preview what model would be selected
const selection = await fetch('https://llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 
    'Content-Type': 'application/json',
    'X-Stainless-API-Key': 'your-key' 
  },
  body: JSON.stringify({
    models: [
      { provider: 'openai' },
      { provider: 'anthropic' },
      { provider: 'deepseek' }
    ],
    prompt: 'Complex analysis task',
    cost_bias: 0.8
  })
});

const result = await selection.json();
console.log(`Would select: ${result.provider}/${result.model}`);
```

```python Python
import requests

# Preview what model would be selected
response = requests.post(
    'https://llmadaptive.uk/api/v1/select-model',
    headers={'X-Stainless-API-Key': 'your-key'},
    json={
        'models': [
            {'provider': 'openai'},
            {'provider': 'anthropic'},
            {'provider': 'deepseek'}
        ],
        'prompt': 'Complex analysis task',
        'cost_bias': 0.8
    }
)

result = response.json()
print(f"Would select: {result['provider']}/{result['model']}")
```

```bash cURL
curl https://llmadaptive.uk/api/v1/select-model \
  -H "X-Stainless-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "models": [
      {"provider": "openai"},
      {"provider": "anthropic"},
      {"provider": "deepseek"}
    ],
    "prompt": "Complex analysis task",
    "cost_bias": 0.8
  }'
```

</CodeGroup>

Perfect for:
- **Testing routing decisions** before implementing
- **Using your own provider accounts** with intelligent selection
- **On-premise inference** with cloud-quality routing decisions
- **Enterprise contracts** optimization
- **Data privacy** scenarios where you need local inference

