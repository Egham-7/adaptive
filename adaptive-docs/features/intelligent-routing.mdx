---
title: "Intelligent Routing"
description: "Automatic model selection for optimal cost and performance"
icon: "route"
---

Adaptive analyzes your requests and automatically routes to the best model. Set `model: ""` to enable intelligent routing.

## Quick Examples

```javascript
// Automatic routing (recommended)
const completion = await openai.chat.completions.create({
  model: "", // Let Adaptive choose
  messages: [{ role: "user", content: "Hello!" }]
});
```

## Routing Results

- **Simple Q&A** → gpt-4o-mini (92% cost savings)
- **Code Generation** → deepseek-chat (78% cost savings)  
- **Creative Writing** → grok-3 (65% cost savings)

## Configuration

### Cost vs Performance

<CodeGroup>

```javascript JavaScript
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Hello" }],
  protocol_manager: {
    cost_bias: 0.2 // 0 = cheapest, 1 = best performance
  }
});
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Hello"}],
    protocol_manager={
        "cost_bias": 0.2  # 0 = cheapest, 1 = best performance
    }
)
```

```bash cURL
curl https://llmadaptive.uk/api/v1/chat/completions \
  -H "X-Stainless-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "Hello"}],
    "protocol_manager": {
      "cost_bias": 0.2
    }
  }'
```

</CodeGroup>

### Limit Providers

<CodeGroup>

```javascript JavaScript
const completion = await openai.chat.completions.create({
  model: "",
  messages: [{ role: "user", content: "Hello" }],
  protocol_manager: {
    models: [
      { provider: "openai" },
      { provider: "anthropic", model_name: "claude-3-sonnet" }
    ]
  }
});
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[{"role": "user", "content": "Hello"}],
    protocol_manager={
        "models": [
            {"provider": "openai"},
            {"provider": "anthropic", "model_name": "claude-3-sonnet"}
        ]
    }
)
```

```bash cURL
curl https://llmadaptive.uk/api/v1/chat/completions \
  -H "X-Stainless-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "Hello"}],
    "protocol_manager": {
      "models": [
        {"provider": "openai"},
        {"provider": "anthropic", "model_name": "claude-3-sonnet"}
      ]
    }
  }'
```

</CodeGroup>

## Performance

- 94% accuracy in model selection
- Under 1ms model selection
- 99.9% uptime

## Routing Protocols

Adaptive uses two intelligent protocols to optimize model selection:

### Standard Protocol
For complex, general-purpose tasks requiring powerful models:

- **Triggers**: Complex prompts (>0.4 complexity), long prompts (>60K tokens), or user-specified models
- **Task Types**: Code generation, text generation, brainstorming, analysis
- **Models**: Multi-tier approach from budget (deepseek-chat, gpt-4o-mini) to premium (claude-3.5-sonnet, gpt-4o)
- **Providers**: OpenAI, Anthropic, DeepSeek, Groq, Grok

### Minion Protocol  
For domain-specific, simpler tasks using specialized models:

- **Triggers**: Simple prompts in specific domains (business, health, science, finance)
- **Focus**: Cost optimization with domain-specialized models
- **Models**: Primarily open-source HuggingFace models (Llama-3.1-8B, Mistral-7B, DeepSeek-R1)
- **Benefits**: 80%+ cost savings for routine tasks

### Selection Logic

```
1. Analyze prompt → complexity + domain + tokens
2. IF complex OR long OR custom models:
   → Standard Protocol
3. ELSE IF domain-specific:
   → Minion Protocol
4. ELSE:
   → Standard Protocol (fallback)
```

## Response Format

```json
{
  "model": "",
  "provider": "openai", // Shows selected provider
  "choices": [...],
  "usage": {...}
}
```

## Preview Routing Decisions

Want to see which model Adaptive would choose before running the completion? Use the [Select Model endpoint](/api-reference/select-model):

<CodeGroup>

```javascript JavaScript
// Preview what model would be selected
const selection = await fetch('https://llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': 'your-key' },
  body: JSON.stringify({
    model: '',
    messages: [{ role: 'user', content: 'Complex analysis task' }],
    protocol_manager: { cost_bias: 0.8 }
  })
});

const result = await selection.json();
console.log(`Would select: ${result.metadata.provider}/${result.metadata.model}`);
```

```python Python
import requests

# Preview what model would be selected
response = requests.post(
    'https://llmadaptive.uk/api/v1/select-model',
    headers={'X-Stainless-API-Key': 'your-key'},
    json={
        'model': '',
        'messages': [{'role': 'user', 'content': 'Complex analysis task'}],
        'protocol_manager': {'cost_bias': 0.8}
    }
)

result = response.json()
print(f"Would select: {result['metadata']['provider']}/{result['metadata']['model']}")
```

```bash cURL
curl https://llmadaptive.uk/api/v1/select-model \
  -H "X-Stainless-API-Key: your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "",
    "messages": [{"role": "user", "content": "Complex analysis task"}],
    "protocol_manager": {"cost_bias": 0.8}
  }'
```

</CodeGroup>

Perfect for:
- **Using your own provider accounts** with intelligent selection
- **On-premise inference** with cloud-quality routing decisions
- **Enterprise contracts** optimization
- **Data privacy** scenarios where you need local inference

