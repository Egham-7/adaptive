---
title: 'Chat Completions'
api: 'POST https://llmadaptive.uk/api/v1/chat/completions'
description: 'Create a chat completion with intelligent model routing'
icon: "message-square"
---

OpenAI-compatible chat completions with intelligent routing. Set `model: ""` for automatic provider selection.

## Authentication

Use any OpenAI-compatible authorization header:
- `X-Stainless-API-Key: your-adaptive-api-key` (recommended)
- `Authorization: Bearer your-adaptive-api-key`

## Request

<ParamField body="model" type="string" required>
  Model to use for completion. Use `""` (empty string) for intelligent routing.
</ParamField>

<ParamField body="messages" type="array" required>
  Array of message objects representing the conversation.
  
  <Expandable title="Message Object">
    <ParamField body="role" type="string" required>
      Role of the message author. One of: `system`, `user`, `assistant`, `tool`
    </ParamField>
    
    <ParamField body="content" type="string | array" required>
      Content of the message. Can be text or array for multimodal inputs.
    </ParamField>
    
    <ParamField body="name" type="string">
      Optional name for the message author
    </ParamField>
  </Expandable>
</ParamField>

### Core Parameters

<ParamField body="temperature" type="number">
  Sampling temperature between 0 and 2. Higher values make output more random.
  Default: `1`
</ParamField>

<ParamField body="max_tokens" type="integer">
  **Deprecated** - Maximum number of tokens to generate. Use `max_completion_tokens` instead.
</ParamField>

<ParamField body="max_completion_tokens" type="integer">
  Maximum number of tokens that can be generated for completion, including reasoning tokens.
</ParamField>

<ParamField body="stream" type="boolean">
  Whether to stream the response. Default: `false`
</ParamField>

<ParamField body="top_p" type="number">
  Nucleus sampling parameter between 0 and 1. Default: `1`
</ParamField>

<ParamField body="frequency_penalty" type="number">
  Penalty for token frequency. Range: -2.0 to 2.0. Default: `0`
</ParamField>

<ParamField body="presence_penalty" type="number">
  Penalty for token presence. Range: -2.0 to 2.0. Default: `0`
</ParamField>

<ParamField body="n" type="integer">
  Number of chat completion choices to generate. Default: `1`
</ParamField>

<ParamField body="seed" type="integer">
  Seed for deterministic sampling. Helps ensure reproducible results.
</ParamField>

<ParamField body="stop" type="string | array">
  Up to 4 sequences where the API will stop generating tokens.
</ParamField>

<ParamField body="user" type="string">
  Unique identifier for end-user to help detect abuse and improve caching.
</ParamField>

### Advanced Parameters

<ParamField body="logprobs" type="boolean">
  Whether to return log probabilities of output tokens. Default: `false`
</ParamField>

<ParamField body="top_logprobs" type="integer">
  Number of most likely tokens to return at each position (0-20). Requires `logprobs: true`.
</ParamField>

<ParamField body="logit_bias" type="object">
  Modify likelihood of specified tokens. Maps token IDs to bias values (-100 to 100).
</ParamField>

<ParamField body="response_format" type="object">
  Format for model output. Supports JSON schema for structured outputs.
  
  <Expandable title="Response Format Options">
    <ParamField body="type" type="string">
      Either `json_object` or `json_schema`
    </ParamField>
    
    <ParamField body="json_schema" type="object">
      JSON schema definition when using `json_schema` type
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="service_tier" type="string">
  Latency tier for processing. Options: `auto`, `default`, `flex`
</ParamField>

<ParamField body="store" type="boolean">
  Whether to store output for model distillation or evals. Default: `false`
</ParamField>

<ParamField body="metadata" type="object">
  Set of 16 key-value pairs for storing additional information about the request.
</ParamField>

### Audio and Multimodal

<ParamField body="modalities" type="array">
  Output types to generate. Options: `["text"]`, `["audio"]`, or `["text", "audio"]`
</ParamField>

<ParamField body="audio" type="object">
  Parameters for audio output when `modalities` includes `"audio"`.
</ParamField>

### Reasoning Models (o-series)

<ParamField body="reasoning_effort" type="string">
  **o-series models only** - Effort level for reasoning: `low`, `medium`, or `high`
</ParamField>

### Function Calling

<ParamField body="tools" type="array">
  Array of tool definitions for function calling. Maximum 128 functions.
  
  <Expandable title="Tool Definition">
    <ParamField body="type" type="string">
      Tool type, currently only `function` supported
    </ParamField>
    
    <ParamField body="function" type="object">
      Function definition with name, description, and parameters schema
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="tool_choice" type="string | object">
  Controls tool usage: `none`, `auto`, `required`, or specific tool selection
</ParamField>

<ParamField body="parallel_tool_calls" type="boolean">
  Whether to enable parallel function calling. Default: `true`
</ParamField>

<ParamField body="function_call" type="string | object">
  **Deprecated** - Use `tool_choice` instead. Controls function calling behavior.
</ParamField>

### Web Search

<ParamField body="web_search_options" type="object">
  Options for web search tool functionality.
</ParamField>

### Streaming Options

<ParamField body="stream_options" type="object">
  Additional options for streaming responses.
  
  <Expandable title="Stream Options">
    <ParamField body="include_usage" type="boolean">
      Whether to include usage statistics in streaming response
    </ParamField>
  </Expandable>
</ParamField>

### Prediction and Caching

<ParamField body="prediction" type="object">
  Static predicted output content for regeneration scenarios.
</ParamField>

### Adaptive-Specific Parameters

<ParamField body="protocol_manager" type="object">
  Configuration for intelligent routing and provider selection.
  
  <Expandable title="Protocol Manager Config">
    <ParamField body="models" type="ModelCapability[]">
      Array of model capabilities to consider for routing. Can be simplified or detailed:
      
      **Simple formats:**
      - `{ "provider": "openai" }` - Use all OpenAI models
      - `{ "provider": "anthropic", "model_name": "claude-3-sonnet-20240229" }` - Use specific model
      
      **Custom models require all parameters:**
      
      <Expandable title="Model Capability Object">
        <ParamField body="provider" type="string" required>
          Provider name: `"openai"`, `"anthropic"`, `"google"`, `"groq"`, `"deepseek"`, `"mistral"`, `"grok"`, `"huggingface"`
        </ParamField>
        
        <ParamField body="model_name" type="string">
          Specific model identifier (e.g., `"gpt-4"`, `"claude-3-sonnet"`). **Required for custom models or specific model selection**
        </ParamField>
        
        <ParamField body="cost_per_1m_input_tokens" type="number">
          Cost per 1 million input tokens in USD. **Required for custom models**
        </ParamField>
        
        <ParamField body="cost_per_1m_output_tokens" type="number">
          Cost per 1 million output tokens in USD. **Required for custom models**
        </ParamField>
        
        <ParamField body="max_context_tokens" type="integer">
          Maximum context window size in tokens. **Required for custom models**
        </ParamField>
        
        <ParamField body="max_output_tokens" type="integer">
          Maximum output tokens the model can generate. **Required for custom models**
        </ParamField>
        
        <ParamField body="supports_function_calling" type="boolean">
          Whether the model supports function/tool calling. **Required for custom models**
        </ParamField>
        
        <ParamField body="description" type="string">
          Human-readable description of the model
        </ParamField>
        
        <ParamField body="languages_supported" type="string[]">
          Array of supported language codes
        </ParamField>
        
        <ParamField body="model_size_params" type="string">
          Model size information (e.g., `"7B"`, `"70B"`)
        </ParamField>
        
        <ParamField body="latency_tier" type="string">
          Expected latency: `"low"`, `"medium"`, `"high"`
        </ParamField>
        
        <ParamField body="task_type" type="string">
          Optimal task type: `"Open QA"`, `"Closed QA"`, `"Summarization"`, `"Text Generation"`, `"Code Generation"`, `"Chatbot"`, `"Classification"`, `"Rewrite"`, `"Brainstorming"`, `"Extraction"`, `"Other"`
        </ParamField>
        
        <ParamField body="complexity" type="string">
          Model complexity tier: `"low"`, `"medium"`, `"high"`
        </ParamField>
      </Expandable>
    </ParamField>
    
    <ParamField body="cost_bias" type="number">
      Bias towards cost optimization. Range: 0.0-1.0 where 0.0 = cheapest, 1.0 = best performance
    </ParamField>
    
    <ParamField body="complexity_threshold" type="number">
      Threshold for task complexity routing decisions. Range: 0.0-1.0
    </ParamField>
    
    <ParamField body="token_threshold" type="integer">
      Token count threshold for model selection. Positive integer.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="semantic_cache" type="object">
  Configuration for semantic caching to improve response times and reduce costs.
  
  <Expandable title="Cache Config">
    <ParamField body="enabled" type="boolean" required>
      Whether semantic caching is enabled
    </ParamField>
    
    <ParamField body="semantic_threshold" type="number" required>
      Similarity threshold for cache hits. Range: 0.0-1.0, higher values require more similarity
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="prompt_cache" type="object">
  Configuration for prompt-response caching (disabled by default).
  
  <Expandable title="Prompt Cache Config">
    <ParamField body="enabled" type="boolean" required>
      Whether to enable prompt-response caching for this request
    </ParamField>
    
    <ParamField body="ttl" type="integer">
      Cache duration in seconds. Default: 3600 (1 hour)
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="fallback" type="object">
  Configuration for provider fallback behavior.
  
  <Expandable title="Fallback Config">
    <ParamField body="enabled" type="boolean">
      Whether fallback is enabled. Default: `true`
    </ParamField>
    
    <ParamField body="mode" type="string">
      Fallback strategy: `"sequential"` or `"race"`. Default: `"race"`
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="provider_configs" type="object">
  Configuration for custom providers. Required when using custom providers in your model list.
  
  <Expandable title="Provider Config Object">
    <ParamField body="base_url" type="string" required>
      API base URL for the custom provider (e.g., `"https://api.custom.com/v1"`)
    </ParamField>
    
    <ParamField body="api_key" type="string" required>
      Full API key for authentication with the custom provider
    </ParamField>
    
    <ParamField body="auth_type" type="string">
      Authentication type: `"bearer"`, `"api_key"`, `"basic"`, or `"custom"`. Default: `"bearer"`
    </ParamField>
    
    <ParamField body="auth_header_name" type="string">
      Custom authentication header name. Default: `"Authorization"`
    </ParamField>
    
    <ParamField body="headers" type="object">
      Additional headers to send with requests to the custom provider
    </ParamField>
    
    <ParamField body="timeout_ms" type="integer">
      Request timeout in milliseconds. Range: 1000-120000. Default: 30000
    </ParamField>
    
    <ParamField body="rate_limit_rpm" type="integer">
      Rate limit in requests per minute. Range: 1-100000
    </ParamField>
    
    <ParamField body="health_endpoint" type="string">
      Health check endpoint for monitoring provider availability
    </ParamField>
    
    <ParamField body="retry_config" type="object">
      Custom retry configuration for failed requests
    </ParamField>
  </Expandable>
</ParamField>

## Response

<ResponseField name="id" type="string">
  Unique identifier for the completion
</ResponseField>

<ResponseField name="object" type="string">
  Object type, always `chat.completion`
</ResponseField>

<ResponseField name="created" type="integer">
  Unix timestamp of creation
</ResponseField>

<ResponseField name="model" type="string">
  Model used for the completion
</ResponseField>

<ResponseField name="provider" type="string">
  **Adaptive addition:** Which provider was selected (e.g., "openai", "anthropic")
</ResponseField>

<ResponseField name="choices" type="array">
  Array of completion choices
  
  <Expandable title="Choice Object">
    <ResponseField name="index" type="integer">
      Index of the choice
    </ResponseField>
    
    <ResponseField name="message" type="object">
      The generated message
      
      <Expandable title="Message Object">
        <ResponseField name="role" type="string">
          Role of the message, always "assistant"
        </ResponseField>
        
        <ResponseField name="content" type="string">
          The content of the message
        </ResponseField>
        
        <ResponseField name="tool_calls" type="array">
          Tool calls made by the model (if any)
        </ResponseField>
      </Expandable>
    </ResponseField>
    
    <ResponseField name="finish_reason" type="string">
      Reason completion finished: `stop`, `length`, `tool_calls`, or `content_filter`
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="usage" type="object">
  Token usage statistics
  
  <Expandable title="Usage Object">
    <ResponseField name="prompt_tokens" type="integer">
      Number of tokens in the prompt
    </ResponseField>
    
    <ResponseField name="completion_tokens" type="integer">
      Number of tokens in the completion
    </ResponseField>
    
    <ResponseField name="total_tokens" type="integer">
      Total tokens used
    </ResponseField>
    
    <ResponseField name="cache_tier" type="string">
      **Adaptive addition:** Cache tier used for this response. Possible values:
      - `"semantic_exact"` - Exact semantic cache match
      - `"semantic_similar"` - Similar semantic cache match  
      - `"prompt_response"` - Prompt response cache hit
      - Omitted when no cache is used
    </ResponseField>
  </Expandable>
</ResponseField>

## Examples

### Basic Chat

<CodeGroup>

```javascript JavaScript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Explain quantum computing simply' }
  ],
});

console.log(completion.choices[0].message.content);
```

```python Python
completion = client.chat.completions.create(
    model="",
    messages=[
        {"role": "user", "content": "Explain quantum computing simply"}
    ]
)

print(completion.choices[0].message.content)
```

```bash cURL
curl https://llmadaptive.uk/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-Stainless-API-Key: $ADAPTIVE_API_KEY" \
  -d '{
    "model": "",
    "messages": [
      {"role": "user", "content": "Explain quantum computing simply"}
    ]
  }'
```

</CodeGroup>

### With Intelligent Routing Configuration

```javascript
// Simple provider selection
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Write a Python function to sort a list' }
  ],
  protocol_manager: {
    models: [
      { provider: "anthropic" }, // All Anthropic models
      { provider: "openai", model_name: "gpt-4" } // Specific OpenAI model
    ],
    cost_bias: 0.2, // Prefer cost savings
    complexity_threshold: 0.3,
    token_threshold: 1000
  },
  fallback: {
    enabled: true,
    mode: 'sequential'
  }
});
```

### Using Custom Providers

```javascript
// Custom provider example
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Explain machine learning concepts' }
  ],
  protocol_manager: {
    models: [
      { provider: "openai" }, // Standard provider
      { 
        provider: "my-custom-llm", // Custom provider
        model_name: "custom-model-v1",
        cost_per_1m_input_tokens: 2.0,
        cost_per_1m_output_tokens: 6.0,
        max_context_tokens: 16000,
        max_output_tokens: 4000,
        supports_function_calling: true,
        task_type: "Text Generation",
        complexity: "medium"
      }
    ],
    cost_bias: 0.5
  },
  
  // Configure each custom provider
  provider_configs: {
    "my-custom-llm": {
      base_url: "https://api.mycustom.com/v1",
      api_key: "sk-custom-api-key-here",
      auth_type: "bearer",
      headers: {
        "X-Custom-Header": "value"
      },
      timeout_ms: 45000
    }
  }
});
```

### Customizing Standard Providers

You can also customize standard providers (OpenAI, Anthropic, etc.) with custom base URLs, API keys, and settings:

```javascript
// Override standard provider configuration
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Hello from custom OpenAI endpoint!' }
  ],
  protocol_manager: {
    models: [
      { provider: "openai" }, // Will use custom config below
      { provider: "anthropic" } // Will also use custom config
    ]
  },
  
  // Custom configurations for standard providers
  provider_configs: {
    "openai": {
      base_url: "https://my-custom-openai-proxy.com/v1",
      api_key: "sk-my-custom-openai-key",
      timeout_ms: 60000,
      headers: {
        "X-Proxy-Key": "proxy-auth-123"
      }
    },
    "anthropic": {
      base_url: "https://my-anthropic-proxy.com/v1",
      api_key: "sk-ant-custom-key",
      timeout_ms: 45000
    }
  }
});
```

### Streaming Response

```javascript
const stream = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'Tell me a story about space exploration' }
  ],
  stream: true
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

### Function Calling

```javascript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'user', content: 'What\'s the weather like in San Francisco?' }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'get_weather',
        description: 'Get current weather for a location',
        parameters: {
          type: 'object',
          properties: {
            location: {
              type: 'string',
              description: 'City and state, e.g. San Francisco, CA'
            }
          },
          required: ['location']
        }
      }
    }
  ]
});
```

### Vision (Multimodal)

```javascript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'What\'s in this image?' },
        {
          type: 'image_url',
          image_url: {
            url: 'https://example.com/image.jpg'
          }
        }
      ]
    }
  ],
  modalities: ['text'] // Can also include 'audio' for supported models
});
```

### Advanced Configuration with All Parameters

```javascript
const completion = await openai.chat.completions.create({
  model: '',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Explain machine learning concepts' }
  ],
  
  // Core parameters
  temperature: 0.7,
  max_completion_tokens: 1000,
  top_p: 0.9,
  frequency_penalty: 0.1,
  presence_penalty: 0.1,
  n: 1,
  seed: 12345,
  stop: ['\n\n'],
  user: 'user-123',
  
  // Advanced parameters
  logprobs: true,
  top_logprobs: 5,
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'explanation',
      schema: {
        type: 'object',
        properties: {
          concept: { type: 'string' },
          explanation: { type: 'string' }
        }
      }
    }
  },
  service_tier: 'auto',
  store: false,
  metadata: {
    session_id: 'abc123',
    user_type: 'premium'
  },
  
  // Reasoning models (o-series)
  reasoning_effort: 'medium',
  
  // Function calling
  tools: [
    {
      type: 'function',
      function: {
        name: 'search_knowledge',
        description: 'Search knowledge base for information',
        parameters: {
          type: 'object',
          properties: {
            query: { type: 'string' }
          }
        }
      }
    }
  ],
  tool_choice: 'auto',
  parallel_tool_calls: true,
  
  // Streaming
  stream: false,
  stream_options: {
    include_usage: true
  },
  
  // Adaptive-specific
  protocol_manager: {
    models: [
      { provider: "openai" }, // Use all OpenAI models
      { provider: "anthropic", model_name: "claude-3-sonnet-20240229" }, // Specific model
      // Custom model example (all params required):
      {
        provider: "my-custom-provider",
        model_name: "custom-model-v1",
        cost_per_1m_input_tokens: 5.0,
        cost_per_1m_output_tokens: 10.0,
        max_context_tokens: 32000,
        max_output_tokens: 2048,
        supports_function_calling: false,
        task_type: "Text Generation",
        complexity: "medium"
      }
    ],
    cost_bias: 0.3,
    complexity_threshold: 0.5,
    token_threshold: 2000
  },
  
  provider_configs: {
    "my-custom-provider": {
      base_url: "https://api.custom.com/v1",
      api_key: "sk-custom-key-123",
      auth_type: "bearer",
      headers: {
        "Custom-Header": "custom-value"
      },
      timeout_ms: 30000,
      rate_limit_rpm: 1000
    }
  },
  
  semantic_cache: {
    enabled: true,
    semantic_threshold: 0.85
  },
  fallback: {
    enabled: true,
    mode: 'sequential'
  }
});
```

## Response Examples

### Cache Tier Tracking

The `usage.cache_tier` field shows which cache served your response:

```json
// Semantic cache hit
{
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 8, 
    "total_tokens": 18,
    "cache_tier": "semantic_exact"
  }
}

// Prompt cache hit  
{
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 4,
    "total_tokens": 9,
    "cache_tier": "prompt_response"
  }
}

// No cache used
{
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 10,
    "total_tokens": 18
    // cache_tier omitted
  }
}
```

## Error Responses

<ResponseField name="error" type="object">
  Error information
  
  <Expandable title="Error Object">
    <ResponseField name="message" type="string">
      Human-readable error message
    </ResponseField>
    
    <ResponseField name="type" type="string">
      Error type: `invalid_request_error`, `authentication_error`, `permission_error`, `rate_limit_error`, or `server_error`
    </ResponseField>
    
    <ResponseField name="code" type="string">
      Error code for programmatic handling
    </ResponseField>
  </Expandable>
</ResponseField>

### Common Errors

| Status Code | Error Type | Description |
|-------------|------------|-------------|
| 400 | `invalid_request_error` | Invalid request format or parameters |
| 401 | `authentication_error` | Invalid or missing API key |
| 403 | `permission_error` | API key doesn't have required permissions |
| 429 | `rate_limit_error` | Rate limit exceeded |
| 500 | `server_error` | Internal server error |

## Rate Limits

| Plan | Requests per Minute | Tokens per Minute |
|------|-------------------|-------------------|
| Free | 100 | 10,000 |
| Pro | 1,000 | 100,000 |
| Enterprise | Custom | Custom |

Rate limits are applied per API key and reset every minute.

## Best Practices

<CardGroup cols={2}>
  <Card title="Model Selection" icon="brain">
    Use empty string `""` for model to enable intelligent routing and cost savings
  </Card>
  <Card title="Cost Control" icon="dollar-sign">
    Use `cost_bias` parameter to balance cost vs performance for your use case
  </Card>
  <Card title="Custom Providers" icon="plug">
    When using custom providers, always include their configuration in `provider_configs`
  </Card>
  <Card title="Error Handling" icon="shield-check">
    Always implement proper error handling for network and API failures
  </Card>
</CardGroup>