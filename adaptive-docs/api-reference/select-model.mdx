---
title: 'Select Model'
api: 'POST https://llmadaptive.uk/api/v1/select-model'
description: 'Get intelligent model selection for any provider or infrastructure'
icon: "brain"
---

Get Adaptive's intelligent model selection without using our inference. **Provider-agnostic design** - works with any models, any providers, any infrastructure.

## Why Use This?

**Use Adaptive's intelligence, run inference wherever you want:**

- **"I have my own OpenAI/Anthropic accounts"** - Get optimal model selection, pay your providers directly
- **"I run models on-premise"** - Get routing decisions for your local infrastructure  
- **"I have enterprise contracts"** - Use your existing provider relationships with intelligent routing
- **"I need data privacy"** - Keep inference local while getting smart model selection

## Request

**Provider-agnostic format** - send your available models and prompt, get intelligent selection back.

<ParamField body="models" type="array" required>
  Array of available models. **For known models (GPT-4, Claude, Gemini, etc.), just specify `provider` and `model_name` - Adaptive knows the rest. Only provide full specs for custom/unknown models.**
  
  <Expandable title="Model Specification Options">
    **Provider + Model (for known models):**
    ```json
    {"provider": "openai", "model_name": "gpt-4o-mini"}
    ```
    
    **Provider-only (let Adaptive choose best model):**
    ```json
    {"provider": "anthropic"}
    ```
    
    **Model-only (if provider is obvious):**
    ```json
    {"model_name": "gpt-4o-mini"}
    ```
    
    **Full specification (for custom models):**
    <ParamField body="provider" type="string" required>
      Provider name (e.g., "openai", "anthropic", "local", "custom")
    </ParamField>
    
    <ParamField body="model_name" type="string">
      Model identifier (required unless provider-only)
    </ParamField>
    
    <ParamField body="cost_per_1m_input_tokens" type="number">
      Cost per 1M input tokens (auto-filled for known models)
    </ParamField>
    
    <ParamField body="cost_per_1m_output_tokens" type="number">
      Cost per 1M output tokens (auto-filled for known models)
    </ParamField>
    
    <ParamField body="max_context_tokens" type="number">
      Maximum context window size (auto-filled for known models)
    </ParamField>
    
    <ParamField body="supports_function_calling" type="boolean">
      Whether the model supports function/tool calling (auto-filled for known models)
    </ParamField>
    
    <ParamField body="max_output_tokens" type="number">
      Maximum output tokens (optional)
    </ParamField>
    
    <ParamField body="complexity" type="string">
      Model complexity tier: "low", "medium", "high" (optional)
    </ParamField>
    
    <ParamField body="task_type" type="string">
      Optimized task type (optional)
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="prompt" type="string" required>
  The prompt text to analyze for optimal model selection
</ParamField>

<ParamField body="user" type="string">
  Optional user identifier for caching optimization (enables user-specific cache hits)
</ParamField>

<ParamField body="cost_bias" type="number">
  Cost optimization preference (0.0 = cheapest, 1.0 = best performance)
  
  Default: Uses server configuration. Override to prioritize cost savings or performance for this specific selection.
</ParamField>

<ParamField body="semantic_cache" type="object">
  Semantic cache configuration for this request
  
  <Expandable title="Semantic Cache Configuration">
    <ParamField body="enabled" type="boolean">
      Whether to use semantic caching for this request
    </ParamField>
    
    <ParamField body="semantic_threshold" type="number">
      Similarity threshold for cache hits (0.0-1.0, higher = more strict matching)
    </ParamField>
  </Expandable>
</ParamField>

## Response

<ResponseField name="provider" type="string">
  **Selected provider name**
  
  The provider that was chosen for this prompt (e.g., "openai", "anthropic", "local")
</ResponseField>

<ResponseField name="model" type="string">
  **Selected model identifier**
  
  The specific model that was chosen (e.g., "gpt-4", "claude-3-5-sonnet", "llama-3-8b")
</ResponseField>

<ResponseField name="alternatives" type="array">
  **Alternative provider/model combinations** (optional)
  
  Fallback options if the primary selection is unavailable
  
  <Expandable title="Alternative Object">
    <ResponseField name="provider" type="string">
      Alternative provider name
    </ResponseField>
    
    <ResponseField name="model" type="string">
      Alternative model identifier
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="metadata" type="object">
  **Additional selection information** (for logging/debugging)
  
  <Expandable title="Selection Metadata">
    <ResponseField name="reasoning" type="string">
      Why this model was selected (optional)
    </ResponseField>
    
    <ResponseField name="cost_per_1m_tokens" type="number">
      Cost per 1M input tokens for the selected model (optional)
    </ResponseField>
    
    <ResponseField name="complexity" type="string">
      Complexity classification of the prompt (optional)
    </ResponseField>
    
    <ResponseField name="cache_source" type="string">
      Cache hit information if applicable (optional)
    </ResponseField>
  </Expandable>
</ResponseField>

## Quick Examples

### "Known models - just specify what you have"

```bash
# Mix and match specification styles
response=$(curl -s -w "\n%{http_code}" https://llmadaptive.uk/api/v1/select-model \
  -H "X-Stainless-API-Key: $API_KEY" \
  -d '{
    "models": [
      {"provider": "openai", "model_name": "gpt-4o-mini"},
      {"model_name": "claude-3-5-sonnet"},
      {"provider": "google"}
    ],
    "prompt": "Hello, how are you?"
  }')

http_code=$(echo "$response" | tail -n1)
response_body=$(echo "$response" | head -n -1)

if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
  echo "Success: $response_body"
else
  echo "Error $http_code: $response_body" >&2
  exit 1
fi

# Success response:
{
  "provider": "openai", 
  "model": "gpt-4o-mini",
  "metadata": {
    "cost_per_1m_tokens": 0.15,
    "complexity": "low"
  }
}
```

### "Just specify providers - let Adaptive choose"

```bash
# Even simpler - just say what providers you have access to
response=$(curl -s -w "\n%{http_code}" https://llmadaptive.uk/api/v1/select-model \
  -H "X-Stainless-API-Key: $API_KEY" \
  -d '{
    "models": [
      {"provider": "openai"},
      {"provider": "anthropic"}
    ],
    "prompt": "Write a complex analysis of market trends"
  }')

http_code=$(echo "$response" | tail -n1)
response_body=$(echo "$response" | head -n -1)

if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
  echo "Success: $response_body"
else
  echo "Error $http_code: $response_body" >&2
  exit 1
fi

# Success response:
{
  "provider": "anthropic",
  "model": "claude-3-5-sonnet-20241022",
  "alternatives": [
    {"provider": "openai", "model": "gpt-4o"}
  ],
  "metadata": {
    "complexity": "high"
  }
}
```

### "Custom models - specify full details"

```bash
# Only specify details for custom/unknown models
response=$(curl -s -w "\n%{http_code}" https://llmadaptive.uk/api/v1/select-model \
  -H "X-Stainless-API-Key: $API_KEY" \
  -d '{
    "models": [
      {"provider": "openai", "model_name": "gpt-4o-mini"},
      {
        "provider": "local",
        "model_name": "my-custom-llama-fine-tune",
        "cost_per_1m_input_tokens": 0.0,
        "cost_per_1m_output_tokens": 0.0,
        "max_context_tokens": 4096,
        "supports_function_calling": false,
        "complexity": "medium"
      }
    ],
    "prompt": "Hello, how are you?"
  }')

http_code=$(echo "$response" | tail -n1)
response_body=$(echo "$response" | head -n -1)

if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
  echo "Success: $response_body"
else
  echo "Error $http_code: $response_body" >&2
  exit 1
fi

# Known models use Adaptive's specs, custom models use yours
```

### "Test cost optimization"

```javascript
// Will cost_bias actually pick cheaper models?
const response = await fetch('/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': apiKey },
  body: JSON.stringify({
    models: [
      {
        provider: "openai",
        model_name: "gpt-4o-mini",
        cost_per_1m_input_tokens: 0.15,
        cost_per_1m_output_tokens: 0.6,
        max_context_tokens: 128000,
        supports_function_calling: true
      },
      {
        provider: "openai",
        model_name: "gpt-4o", 
        cost_per_1m_input_tokens: 2.5,
        cost_per_1m_output_tokens: 10.0,
        max_context_tokens: 128000,
        supports_function_calling: true
      }
    ],
    prompt: "Analyze this complex dataset and provide insights...",
    cost_bias: 0.1  // Maximize cost savings
  })
});

if (!response.ok) {
  const errorBody = await response.text();
  throw new Error(`HTTP ${response.status}: ${errorBody}`);
}

const result = await response.json();
console.log(result);
// Check if it picked the cheaper model despite complexity
```

### "Compare different configurations"

```python
import requests
import os

# Configuration
BASE_URL = "https://api.yourdomain.com"  # Replace with your actual domain
API_TOKEN = os.getenv("ADAPTIVE_API_TOKEN", "your-api-token-here")  # Set via environment variable
TIMEOUT = 30  # Request timeout in seconds

# Define available models
models = [
    {
        "provider": "openai",
        "model_name": "gpt-4o-mini",
        "cost_per_1m_input_tokens": 0.15,
        "cost_per_1m_output_tokens": 0.6,
        "max_context_tokens": 128000,
        "supports_function_calling": True,
        "complexity": "low"
    },
    {
        "provider": "openai", 
        "model_name": "gpt-4o",
        "cost_per_1m_input_tokens": 2.5,
        "cost_per_1m_output_tokens": 10.0,
        "max_context_tokens": 128000,
        "supports_function_calling": True,
        "complexity": "high"
    }
]

base_request = {
    "models": models,
    "prompt": "Write Python code to analyze customer data"
}

# Headers for authentication
headers = {
    "Authorization": f"Bearer {API_TOKEN}",
    "Content-Type": "application/json"
}

# Test cost-focused vs performance-focused
configs = [
    {"cost_bias": 0.1, "name": "cost-optimized"},
    {"cost_bias": 0.9, "name": "performance-focused"}
]

for config in configs:
    try:
        response = requests.post(
            f"{BASE_URL}/api/v1/select-model",
            json={
                **base_request,
                "cost_bias": config["cost_bias"]
            },
            headers=headers,
            timeout=TIMEOUT
        )
        
        # Check if request was successful
        if response.ok:
            result = response.json()
            print(f"{config['name']}: {result['provider']}/{result['model']}")
        else:
            print(f"Error for {config['name']}: HTTP {response.status_code} - {response.text}")
            
    except requests.exceptions.Timeout:
        print(f"Timeout error for {config['name']}: Request took longer than {TIMEOUT} seconds")
    except requests.exceptions.ConnectionError:
        print(f"Connection error for {config['name']}: Unable to connect to {BASE_URL}")
    except requests.exceptions.RequestException as e:
        print(f"Request error for {config['name']}: {e}")
    except Exception as e:
        print(f"Unexpected error for {config['name']}: {e}")
```

## Real-World Integration Patterns

### 1. Use Your Own Provider Accounts
```javascript
// Define your available models with your own pricing
const availableModels = [
  {
    provider: "openai",
    model_name: "gpt-4o-mini", 
    cost_per_1m_input_tokens: 0.15,
    cost_per_1m_output_tokens: 0.6,
    max_context_tokens: 128000,
    supports_function_calling: true
  },
  {
    provider: "anthropic",
    model_name: "claude-3-5-sonnet-20241022",
    cost_per_1m_input_tokens: 3.0,
    cost_per_1m_output_tokens: 15.0,
    max_context_tokens: 200000,
    supports_function_calling: true
  }
];

// Get intelligent selection
const selection = await fetch('/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': adaptiveKey },
  body: JSON.stringify({
    models: availableModels,
    prompt: userMessage
  })
});

const result = await selection.json();

// Route to your own provider accounts
if (result.provider === "openai") {
  const completion = await yourOpenAI.chat.completions.create({
    model: result.model,
    messages: [{ role: "user", content: userMessage }]
  });
} else if (result.provider === "anthropic") {
  const completion = await yourAnthropic.messages.create({
    model: result.model,
    messages: [{ role: "user", content: userMessage }],
    max_tokens: 4096
  });
}
```

### 2. On-Premise Model Routing
```javascript
// Tell Adaptive about your local models (plus a cloud fallback)
const res = await fetch('https://llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': adaptiveKey, 'Content-Type': 'application/json' },
  body: JSON.stringify({
    models: [
      { provider: "local", model_name: "llama-3-8b" },
      { provider: "local", model_name: "llama-3-70b" },
      { provider: "openai", model_name: "gpt-4" } // Cloud fallback
    ],
    prompt: userMessage
  })
});
const selection = await res.json();

// Route to the right infrastructure using provider/model
if (selection.provider === "local" && selection.model === "llama-3-8b") {
  await yourLocalServer.infer({ model: selection.model, messages: [{ role: "user", content: userMessage }] });
} else if (selection.provider === "openai") {
  await yourOpenAI.chat.completions.create({ model: selection.model, messages: [{ role: "user", content: userMessage }] });
}
```

### 3. Enterprise Contract Optimization
```javascript
// Maximize usage of your enterprise contracts
const res = await fetch('https://llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': adaptiveKey, 'Content-Type': 'application/json' },
  body: JSON.stringify({
    models: [
      { provider: "anthropic" }, // Your enterprise contract
      { provider: "openai" },    // Your enterprise contract  
      { provider: "google" }     // Pay-per-use fallback
    ],
    prompt: userMessage,
    cost_bias: 0.8
  })
});
const selection = await res.json();

// Always use your own accounts
const client = yourProviderClients[selection.provider];
const completion = await client.create({
  model: selection.model,
  messages: [{ role: "user", content: userMessage }]
});
```

### 4. Data Privacy & Compliance
```javascript
// Keep sensitive data local while getting smart routing
const selection = await selectModel({
  model: '',
  messages: [{ role: "user", content: "NON_SENSITIVE_TASK_DESCRIPTION" }],
  // Don't send actual sensitive data to Adaptive
});

// Run inference on your secure infrastructure
if (selection.metadata.complexity === "high") {
  // Use your high-end local model
  const result = await yourLocalGPU.infer(actualSensitiveData);
} else {
  // Use your efficient local model
  const result = await yourLocalCPU.infer(actualSensitiveData);
}
```

## Understanding the Response

### What You Get Back

```json
{
  "provider": "anthropic",
  "model": "claude-3-5-sonnet-20241022",
  "alternatives": [
    { "provider": "openai", "model": "gpt-4o" }
  ],
  "metadata": {
    "cost_per_1m_tokens": 3.0,
    "complexity": "high",
    "cache_source": "semantic_cache"
  }
}
```

### Key Insights

- **`request.model`** - This is what gets sent to the actual provider
- **`metadata.provider`** - Which API service will be called
- **`metadata.cost_per_1m_tokens`** - Calculate your costs upfront
- **`metadata.complexity`** - How Adaptive classified your task

## Common Patterns

### Before/After Comparison
```javascript
// See what changes with different parameters
const baseline = await selectModel(request);
const withConstraints = await selectModel({
  ...request,
  cost_bias: 0.1
});

console.log(`Baseline: ${baseline.model}`);
console.log(`Cost-optimized: ${withConstraints.model}`);
```

### Validate Your Setup
```javascript
// Make sure your routing rules work
const shouldUseCheap = await fetch('https://llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': adaptiveKey, 'Content-Type': 'application/json' },
  body: JSON.stringify({
    models: [{ provider: "openai", model_name: "gpt-4o-mini" }, { provider: "openai", model_name: "gpt-4o" }],
    prompt: "Hi"
  })
}).then(r => r.json());

const shouldUseExpensive = await fetch('https://llmadaptive.uk/api/v1/select-model', {
  method: 'POST',
  headers: { 'X-Stainless-API-Key': adaptiveKey, 'Content-Type': 'application/json' },
  body: JSON.stringify({
    models: [{ provider: "openai", model_name: "gpt-4o-mini" }, { provider: "openai", model_name: "gpt-4o" }],
    prompt: "Analyze this complex dataset..."
  })
}).then(r => r.json());

// Verify different complexity tasks get different models
```

## Authentication

Same as chat completions:
```bash
# Any of these work
-H "X-Stainless-API-Key: your-key"
-H "Authorization: Bearer your-key"
```

## No Inference = Fast & Cheap

This endpoint:
- ✅ **Fast** - No LLM inference, just routing logic
- ✅ **Cheap** - Doesn't count against token usage
- ✅ **Accurate** - Uses exact same selection logic as real completions

Perfect for testing, debugging, and cost planning without burning through your budget.