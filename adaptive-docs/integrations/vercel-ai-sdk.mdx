---
title: "Vercel AI SDK Integration"
description: "Use Adaptive with the Vercel AI SDK for streamlined AI applications"
sidebarTitle: "Vercel AI SDK"
icon: "triangle"
---

## Overview

The Vercel AI SDK works seamlessly with Adaptive through two methods:

- **Adaptive Provider (Recommended):** Use the native `@adaptive-llm/adaptive` provider for built-in support.
- **OpenAI Provider:** Use Adaptive via `@ai-sdk/openai` with a custom base URL.

---

## Method 1: OpenAI Provider

### Installation

```bash
npm install ai @ai-sdk/openai
```

### Configuration

```ts
import { openai } from "@ai-sdk/openai";

const adaptiveOpenAI = openai({
  apiKey: process.env.ADAPTIVE_API_KEY,
  baseURL: "https://llmadaptive.uk/api/v1",
});
```

### Usage

```ts
import { generateText } from "ai";

const { text } = await generateText({
  model: adaptiveOpenAI(""), // Empty string enables intelligent routing
  prompt: "Explain quantum computing simply",
});
```

---

## Method 2: Adaptive Provider

### Installation

```bash
npm install @adaptive-llm/adaptive
```

### Basic Setup

```ts
import { adaptive } from "@adaptive-llm/adaptive";

// Or customize:
import { createAdaptive } from "@adaptive-llm/adaptive";

const customAdaptive = createAdaptive({
  baseURL: "https://llmadaptive.uk/api/v1",
  apiKey: process.env.ADAPTIVE_API_KEY,
  headers: {
    "User-Agent": "MyApp/1.0",
  },
});
```

---

## Generating Text

```ts
import { generateText } from "ai";

const { text } = await generateText({
  model: adaptive(),
  prompt: "Write a vegetarian lasagna recipe for 4 people.",
});
```

---

## Streaming Example

```ts
import { streamText } from "ai";

const { textStream } = await streamText({
  model: adaptive(),
  prompt: "Explain machine learning.",
});

for await (const delta of textStream) {
  process.stdout.write(delta);
}
```

---

## Configuration Parameters

Control routing behavior with detailed configuration options:

```ts
await generateText({
  model: adaptive(),
  prompt: "Summarize this article",
  providerOptions: {
    adaptive: {
      // Intelligent routing configuration
      protocol_manager: {
        models: [
          { provider: "anthropic" }, // All Anthropic models
          { provider: "openai", model_name: "gpt-4" } // Specific OpenAI model
        ],
        cost_bias: 0.3, // 0 = cheapest, 1 = best performance
        complexity_threshold: 0.5, // Override complexity detection
        token_threshold: 1000 // Override token threshold
      },
      
      // Fallback configuration
      fallback: {
        enabled: true, // Enable/disable fallback (default: true)
        mode: "parallel" // 'sequential' or 'parallel'
      },
      
      // Semantic caching
      semantic_cache: {
        enabled: true,
        semantic_threshold: 0.85 // Similarity threshold for cache hits
      },
      
      // Prompt response cache for identical requests
      prompt_cache: {
        enabled: true,
        ttl: 3600 // Cache duration in seconds (default: 1 hour)
      }
    }
  }
});
```

### Parameter Details

#### `protocol_manager`
Controls intelligent model selection:

- **`models`**: Array of allowed providers/models
  - `{ provider: "openai" }` - All models from provider
  - `{ provider: "anthropic", model_name: "claude-3-sonnet" }` - Specific model
- **`cost_bias`**: Balance cost vs performance (0-1)
  - `0` = Always choose cheapest option
  - `0.5` = Balanced cost and performance  
  - `1` = Always choose best performance
- **`complexity_threshold`**: Override automatic complexity detection (0-1)
- **`token_threshold`**: Override automatic token counting threshold

#### `fallback`
Controls provider fallback behavior:

- **`enabled`**: Enable/disable fallback (default: true)
- **`mode`**: Fallback strategy
  - `"sequential"` = Try providers one by one (lower cost)
  - `"parallel"` = Try multiple providers simultaneously (faster)

#### `semantic_cache`
Improves performance by caching similar requests:

- **`enabled`**: Enable semantic caching
- **`semantic_threshold`**: Similarity threshold (0-1) for cache hits
  - Higher values = more strict matching
  - Lower values = more cache hits but less accuracy

#### `prompt_cache`
Ultra-fast caching for identical requests (disabled by default):

- **`enabled`**: Enable prompt response caching for this request
- **`ttl`**: Cache duration in seconds (default: 3600, i.e., 1 hour)
  - Provides sub-millisecond response times for repeated requests
  - Only successful responses are cached


---

## Tool Use Example

```ts
import { generateText, tool } from "ai";
import { z } from "zod";

const { text } = await generateText({
  model: adaptive(),
  prompt: "What's the weather in New York?",
  tools: {
    getWeather: tool({
      description: "Get weather for a location",
      parameters: z.object({
        location: z.string(),
      }),
      execute: async ({ location }) => {
        return `Weather in ${location} is sunny and 72Â°F`;
      },
    }),
  },
});
```

---

## Environment Setup

```bash
# .env.local
ADAPTIVE_API_KEY=your-adaptive-api-key
```
