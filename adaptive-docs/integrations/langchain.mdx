---
title: "Langchain Integration"
description: "Connect LangChain (Python & JS) with Adaptive for intelligent routing."
sidebarTitle: "LangChain"
icon: "link"
---

## Overview

Use Adaptive as your OpenAI-compatible base URL in LangChain to enable intelligent routing, streaming, and cost optimizations.

## Installation

<Tabs>
<Tab title="Python">
```bash
pip install langchain langchain-openai
````

</Tab>
<Tab title="JavaScript">
```bash
npm install langchain @langchain/openai
```
</Tab>
</Tabs>

## Quick Start

### Python

```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(api_key="ADAPTIVE_KEY", base_url="https://llmadaptive.uk/api/v1", model="")
response = llm.invoke("Explain ML simply")
print(response.content)
```

### JavaScript

```js
import { ChatOpenAI } from "@langchain/openai";
const llm = new ChatOpenAI({
  apiKey: "ADAPTIVE_KEY",
  baseURL: "https://llmadaptive.uk/api/v1",
  model: "",
});
const res = await llm.invoke("Explain quantum computing");
console.log(res.content);
```

## Advanced Usage

### Streaming

```python
for chunk in llm.stream("Tell me a story"):
    print(chunk.content, end="")
```

## Configuration Parameters

Configure Adaptive's intelligent routing through `model_kwargs`:

<Tabs>
<Tab title="Python">
```python
llm = ChatOpenAI(
    api_key="ADAPTIVE_KEY",
    base_url="https://llmadaptive.uk/api/v1",
    model="",
    model_kwargs={
        # Intelligent routing configuration
        "model_router": {
            "models": [
                {"provider": "openai"},  # All OpenAI models
                {"provider": "anthropic", "model_name": "claude-3-sonnet"}  # Specific model
            ],
            "cost_bias": 0.2,  # 0 = cheapest, 1 = best performance
            "complexity_threshold": 0.5,  # Override complexity detection
            "token_threshold": 2000  # Override token threshold
        },
        
        # Fallback configuration
        "fallback": {
            "enabled": True,  # Enable/disable fallback (default: True)
            "mode": "race"  # 'sequential' or 'race'
        },
        
        # Semantic caching
        "prompt_response_cache": {
            "enabled": True,
            "semantic_threshold": 0.85  # Similarity threshold for cache hits
        },
        
        # Prompt response cache for identical requests
        "prompt_cache": {
            "enabled": True,
            "ttl": 3600  # Cache duration in seconds (default: 1 hour)
        }
    }
)
```
</Tab>
<Tab title="JavaScript">
```js
const llm = new ChatOpenAI({
  apiKey: "ADAPTIVE_KEY",
  baseURL: "https://llmadaptive.uk/api/v1",
  model: "",
  modelKwargs: {
    // Intelligent routing configuration
    model_router: {
      models: [
        { provider: "openai" },  // All OpenAI models
        { provider: "anthropic", model_name: "claude-3-sonnet" }  // Specific model
      ],
      cost_bias: 0.2,  // 0 = cheapest, 1 = best performance
      complexity_threshold: 0.5,  // Override complexity detection
      token_threshold: 2000  // Override token threshold
    },
    
    // Fallback configuration
    fallback: {
      enabled: true,  // Enable/disable fallback (default: true)
      mode: "race"  // 'sequential' or 'race'
    },
    
    // Semantic caching
    prompt_response_cache: {
      enabled: true,
      semantic_threshold: 0.85  // Similarity threshold for cache hits
    },
    
    // Prompt response cache for identical requests
    prompt_cache: {
      enabled: true,
      ttl: 3600  // Cache duration in seconds (default: 1 hour)
    }
  }
});
```
</Tab>
</Tabs>

### Parameter Details

#### `model_router`
Controls intelligent model selection:

- **`models`**: Array of allowed providers/models
  - `{"provider": "openai"}` - All models from provider
  - `{"provider": "anthropic", "model_name": "claude-3-sonnet"}` - Specific model
- **`cost_bias`**: Balance cost vs performance (0-1)
  - `0` = Always choose cheapest option
  - `0.5` = Balanced cost and performance  
  - `1` = Always choose best performance
- **`complexity_threshold`**: Override automatic complexity detection (0-1)  
- **`token_threshold`**: Override automatic token counting threshold

#### `fallback`
Controls provider fallback behavior:

- **`enabled`**: Enable/disable fallback (default: true)
- **`mode`**: Fallback strategy
  - `"sequential"` = Try providers one by one (lower cost)
  - `"race"` = Try multiple providers simultaneously (faster)

#### `prompt_response_cache`
Improves performance by caching similar requests:

- **`enabled`**: Enable semantic caching
- **`semantic_threshold`**: Similarity threshold (0-1) for cache hits
  - Higher values = more strict matching
  - Lower values = more cache hits but less accuracy

#### `prompt_cache`
Ultra-fast caching for identical requests (disabled by default):

- **`enabled`**: Enable prompt response caching for this request
- **`ttl`**: Cache duration in seconds (default: 3600, i.e., 1 hour)
  - Provides sub-millisecond response times for repeated requests
  - Only successful responses are cached

## Custom Providers

You can configure custom providers alongside standard ones:

<Tabs>
<Tab title="Python">
```python
llm = ChatOpenAI(
    api_key="ADAPTIVE_KEY",
    base_url="https://llmadaptive.uk/api/v1",
    model="",
    model_kwargs={
        # Include custom provider in model list
        "model_router": {
            "models": [
                {"provider": "openai"},  # Standard provider
                {
                    "provider": "my-custom-llm",  # Custom provider
                    "model_name": "custom-model-v1",
                    "cost_per_1m_input_tokens": 2.0,
                    "cost_per_1m_output_tokens": 6.0,
                    "max_context_tokens": 16000,
                    "max_output_tokens": 4000,
                    "supports_function_calling": True,
                    "task_type": "Text Generation",
                    "complexity": "medium"
                }
            ],
            "cost_bias": 0.5
        },
        
        # Configure each custom provider
        "provider_configs": {
            "my-custom-llm": {
                "base_url": "https://api.mycustom.com/v1",
                "api_key": "sk-custom-api-key-here",
                "auth_type": "bearer",
                "headers": {
                    "X-Custom-Header": "value"
                },
                "timeout_ms": 45000
            }
    }
)
```
</Tab>
<Tab title="JavaScript">
```js
const llm = new ChatOpenAI({
  apiKey: "ADAPTIVE_KEY",
  baseURL: "https://llmadaptive.uk/api/v1",
  model: "",
  modelKwargs: {
    // Include custom provider in model list
    model_router: {
      models: [
        { provider: "openai" },  // Standard provider
        {
          provider: "my-custom-llm",  // Custom provider
          model_name: "custom-model-v1",
          cost_per_1m_input_tokens: 2.0,
          cost_per_1m_output_tokens: 6.0,
          max_context_tokens: 16000,
          max_output_tokens: 4000,
          supports_function_calling: true,
          task_type: "Text Generation",
          complexity: "medium"
        }
      ],
      cost_bias: 0.5
    },
    
    // Required: Configuration for each custom provider (top-level)
    provider_configs: {
      "my-custom-llm": {
        base_url: "https://api.mycustom.com/v1",
        api_key: "sk-custom-api-key-here",
        auth_type: "bearer",
        headers: {
          "X-Custom-Header": "value"
        },
        timeout_ms: 45000
      }
  }
});
```
</Tab>
</Tabs>

## Cache Tier Tracking

LangChain responses include cache information:

```python
response = llm.invoke("Hello world")
print(response.usage_metadata.get('cache_tier'))
# "semantic_exact" | "semantic_similar" | "prompt_response" | None
```

## Extras

- **Function calling**, **tools**, **chains**, **memory** support â€” works out of the box.
- For embeddings, use OpenAI's API (embeddings coming soon!).

## Migration

Simply switch:

```diff
- model=""
+ model=""           # for Adaptive
```

## Best Practices

| Tip                            | Description                           |
| ------------------------------ | ------------------------------------- |
| **Leave `model=""`**           | Enables intelligent routing           |
| **Use `model_router`** | Controls routing behavior and providers |
| **Handle errors**              | Wrap calls in try/catch or retries    |

