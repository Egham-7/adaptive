{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Backend API Testing\n",
    "\n",
    "This notebook allows direct testing of the Azure backend API endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU Testing Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU Complete Dataset Benchmarking with Adaptive Backend\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "\n",
    "\n",
    "def run_complete_mmlu_benchmark():\n",
    "    \"\"\"\n",
    "    Run MMLU benchmark against the adaptive backend on the COMPLETE dataset\n",
    "    Tests ALL 59 subjects with ALL questions (approximately 14,000+ questions)\n",
    "    \n",
    "    WARNING: This will take several hours to complete!\n",
    "    \"\"\"\n",
    "    print(\"=== COMPLETE MMLU DATASET BENCHMARKING ===\")\n",
    "    print(\"üö® WARNING: This will test ~14,000 questions across 59 subjects!\")\n",
    "    print(\"‚è∞ Estimated time: 3-6 hours depending on response times\")\n",
    "    print(\"üí∞ Cost: Significant API usage costs\\n\")\n",
    "    \n",
    "    # Confirmation\n",
    "    print(\"‚ö° Starting in 10 seconds... (stop the cell if you want to cancel)\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Load MMLU dataset\n",
    "    print(\"üìö Loading complete MMLU dataset...\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "        test_data = dataset[\"test\"]\n",
    "        print(f\"‚úÖ Loaded {len(test_data)} total test questions\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get all subjects\n",
    "    all_subjects = list(set(test_data[\"subject\"]))\n",
    "    all_subjects.sort()  # Sort for consistent order\n",
    "    print(f\"üìñ Testing ALL {len(all_subjects)} subjects\")\n",
    "    print(f\"üìã Subjects: {', '.join(all_subjects[:10])}{'...' if len(all_subjects) > 10 else ''}\")\n",
    "    \n",
    "    results = []\n",
    "    total_questions = 0\n",
    "    correct_answers = 0\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if API_KEY != \"your-api-key\":\n",
    "        headers[\"X-Stainless-API-Key\"] = API_KEY\n",
    "    \n",
    "    # Process each subject\n",
    "    for subject_idx, subject in enumerate(all_subjects, 1):\n",
    "        print(f\"\\nüî¨ [{subject_idx}/{len(all_subjects)}] Testing subject: {subject}\")\n",
    "        \n",
    "        # Filter questions for this subject\n",
    "        subject_questions = [q for q in test_data if q[\"subject\"] == subject]\n",
    "        subject_total = len(subject_questions)\n",
    "        subject_correct = 0\n",
    "        \n",
    "        print(f\"   üìä {subject_total} questions in this subject\")\n",
    "        \n",
    "        for i, question in enumerate(subject_questions):\n",
    "            if (i + 1) % 10 == 0 or i == 0:  # Progress indicator\n",
    "                print(f\"   Question {i+1}/{subject_total}...\", end=\" \")\n",
    "            \n",
    "            # Format question for the API\n",
    "            choices = [question[\"choices\"][j] for j in range(len(question[\"choices\"]))]\n",
    "            question_text = f\"\"\"Question: {question['question']}\n",
    "            \n",
    "A) {choices[0]}\n",
    "B) {choices[1]}\n",
    "C) {choices[2]}\n",
    "D) {choices[3]}\n",
    "\n",
    "Please answer with only the letter (A, B, C, or D).\"\"\"\n",
    "            \n",
    "            # Prepare API request with varied parameters for better testing\n",
    "            chat_data = {\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": question_text}],\n",
    "                \"max_tokens\": 10,\n",
    "                \"temperature\": 0.1,  # Low temperature for consistent answers\n",
    "                \"provider_constraint\": [\"openai\", \"deepseek\", \"anthropic\"],  # Multiple providers\n",
    "                \"cost_bias\": random.uniform(0.2, 0.8)  # Vary cost bias to test routing\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                request_start_time = time.time()\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/v1/chat/completions\",\n",
    "                    json=chat_data,\n",
    "                    headers=headers,\n",
    "                    timeout=60  # Longer timeout for complete test\n",
    "                )\n",
    "                response_time = time.time() - request_start_time\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    ai_answer = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                    \n",
    "                    # Extract adaptive backend selection info\n",
    "                    selected_model = result.get(\"model\", \"unknown\")\n",
    "                    selected_provider = result.get(\"provider\", \"unknown\")\n",
    "                    \n",
    "                    # Extract letter from AI response\n",
    "                    ai_letter = None\n",
    "                    for char in ai_answer.upper():\n",
    "                        if char in ['A', 'B', 'C', 'D']:\n",
    "                            ai_letter = char\n",
    "                            break\n",
    "                    \n",
    "                    # Convert correct answer index to letter\n",
    "                    correct_letter = ['A', 'B', 'C', 'D'][question[\"answer\"]]\n",
    "                    \n",
    "                    is_correct = ai_letter == correct_letter\n",
    "                    if is_correct:\n",
    "                        subject_correct += 1\n",
    "                        correct_answers += 1\n",
    "                    \n",
    "                    # Store result with detailed backend selection info\n",
    "                    results.append({\n",
    "                        \"subject\": subject,\n",
    "                        \"question_id\": f\"{subject}_{i}\",\n",
    "                        \"question\": question[\"question\"][:200] + \"...\",  # Longer preview\n",
    "                        \"correct_answer\": correct_letter,\n",
    "                        \"ai_answer\": ai_letter,\n",
    "                        \"ai_response_full\": ai_answer,\n",
    "                        \"is_correct\": is_correct,\n",
    "                        \"response_time\": response_time,\n",
    "                        \"selected_model\": selected_model,\n",
    "                        \"selected_provider\": selected_provider,\n",
    "                        \"model_provider_combo\": f\"{selected_provider}/{selected_model}\",\n",
    "                        \"completion_tokens\": result.get(\"usage\", {}).get(\"completion_tokens\", 0),\n",
    "                        \"prompt_tokens\": result.get(\"usage\", {}).get(\"prompt_tokens\", 0),\n",
    "                        \"total_tokens\": result.get(\"usage\", {}).get(\"total_tokens\", 0),\n",
    "                        \"cost_bias_used\": chat_data[\"cost_bias\"],\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    })\n",
    "                    \n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        accuracy_so_far = (subject_correct / (i + 1)) * 100\n",
    "                        print(f\"‚úÖ Accuracy: {accuracy_so_far:.1f}%\")\n",
    "                        \n",
    "                else:\n",
    "                    print(f\"‚ùå API Error: {response.status_code}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Request failed: {str(e)}\")\n",
    "                \n",
    "            total_questions += 1\n",
    "            \n",
    "            # Save progress every 100 questions\n",
    "            if total_questions % 100 == 0:\n",
    "                temp_df = pd.DataFrame(results)\n",
    "                temp_filename = f\"mmlu_progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                temp_df.to_csv(temp_filename, index=False)\n",
    "                \n",
    "                elapsed = datetime.now() - start_time\n",
    "                rate = total_questions / elapsed.total_seconds() * 60  # questions per minute\n",
    "                remaining = (len(test_data) - total_questions) / rate if rate > 0 else 0\n",
    "                \n",
    "                print(f\"\\nüíæ Progress saved: {total_questions}/{len(test_data)} questions\")\n",
    "                print(f\"‚è±Ô∏è  Rate: {rate:.1f} questions/min | ETA: {remaining:.0f} minutes\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.05)  # Small delay to avoid overwhelming the API\n",
    "        \n",
    "        subject_accuracy = (subject_correct / subject_total) * 100\n",
    "        print(f\"   üìä {subject}: {subject_correct}/{subject_total} ({subject_accuracy:.1f}%)\")\n",
    "        \n",
    "        # Save intermediate results after each subject\n",
    "        if results:\n",
    "            intermediate_df = pd.DataFrame(results)\n",
    "            intermediate_filename = f\"mmlu_intermediate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            intermediate_df.to_csv(intermediate_filename, index=False)\n",
    "    \n",
    "    # Calculate final results\n",
    "    overall_accuracy = (correct_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "    total_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL COMPLETE DATASET RESULTS:\")\n",
    "    print(f\"üìä Overall Accuracy: {correct_answers}/{total_questions} ({overall_accuracy:.1f}%)\")\n",
    "    print(f\"‚è±Ô∏è  Total Time: {total_time}\")\n",
    "    print(f\"‚ö° Average Response Time: {sum(r['response_time'] for r in results) / len(results):.2f}s\")\n",
    "    \n",
    "    # Create comprehensive results DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        print(f\"\\nüìà COMPREHENSIVE PERFORMANCE ANALYSIS:\")\n",
    "        \n",
    "        # Subject-wise performance\n",
    "        subject_stats = df.groupby('subject').agg({\n",
    "            'is_correct': ['count', 'sum', 'mean'],\n",
    "            'response_time': 'mean',\n",
    "            'selected_model': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'selected_provider': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'total_tokens': 'sum'\n",
    "        }).round(3)\n",
    "        \n",
    "        subject_stats.columns = ['Questions', 'Correct', 'Accuracy', 'Avg_Time', 'Most_Used_Model', 'Most_Used_Provider', 'Total_Tokens']\n",
    "        \n",
    "        # Show top and bottom performing subjects\n",
    "        subject_stats_sorted = subject_stats.sort_values('Accuracy', ascending=False)\n",
    "        print(f\"\\nüèÜ TOP 10 PERFORMING SUBJECTS:\")\n",
    "        print(subject_stats_sorted.head(10)[['Questions', 'Correct', 'Accuracy', 'Most_Used_Model']])\n",
    "        \n",
    "        print(f\"\\nüìâ BOTTOM 10 PERFORMING SUBJECTS:\")\n",
    "        print(subject_stats_sorted.tail(10)[['Questions', 'Correct', 'Accuracy', 'Most_Used_Model']])\n",
    "        \n",
    "        print(f\"\\nü§ñ COMPLETE ADAPTIVE BACKEND MODEL SELECTION ANALYSIS:\")\n",
    "        model_provider_usage = df['model_provider_combo'].value_counts()\n",
    "        for combo, count in model_provider_usage.items():\n",
    "            accuracy = df[df['model_provider_combo'] == combo]['is_correct'].mean() * 100\n",
    "            avg_time = df[df['model_provider_combo'] == combo]['response_time'].mean()\n",
    "            print(f\"  {combo}: {count:,} questions ({count/len(df)*100:.1f}%) - Accuracy: {accuracy:.1f}% - Avg Time: {avg_time:.2f}s\")\n",
    "        \n",
    "        print(f\"\\nüí∞ COMPLETE TOKEN USAGE ANALYSIS:\")\n",
    "        total_tokens_used = df['total_tokens'].sum()\n",
    "        avg_tokens_per_question = df['total_tokens'].mean()\n",
    "        print(f\"  Total tokens consumed: {total_tokens_used:,}\")\n",
    "        print(f\"  Average tokens per question: {avg_tokens_per_question:.1f}\")\n",
    "        print(f\"  Estimated API cost (rough): ${total_tokens_used * 0.000002:.2f}\")  # Rough estimate\n",
    "        \n",
    "        provider_token_usage = df.groupby('selected_provider')['total_tokens'].agg(['sum', 'mean', 'count']).round(1)\n",
    "        print(f\"\\nüìä TOKEN USAGE BY PROVIDER:\")\n",
    "        for provider in provider_token_usage.index:\n",
    "            total = provider_token_usage.loc[provider, 'sum']\n",
    "            avg = provider_token_usage.loc[provider, 'mean']\n",
    "            count = provider_token_usage.loc[provider, 'count']\n",
    "            print(f\"  {provider}: {total:,} total tokens ({avg:.1f} avg) across {count:,} questions\")\n",
    "        \n",
    "        # Cost bias analysis\n",
    "        print(f\"\\nüí∏ COST BIAS EFFECTIVENESS ANALYSIS:\")\n",
    "        df['cost_bias_bin'] = pd.cut(df['cost_bias_used'], bins=[0, 0.3, 0.7, 1.0], labels=['Low (0-0.3)', 'Med (0.3-0.7)', 'High (0.7-1.0)'])\n",
    "        cost_bias_analysis = df.groupby('cost_bias_bin').agg({\n",
    "            'selected_model': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'selected_provider': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'is_correct': 'mean',\n",
    "            'total_tokens': 'mean'\n",
    "        }).round(3)\n",
    "        print(cost_bias_analysis)\n",
    "        \n",
    "        # Save final comprehensive results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"mmlu_complete_benchmark_results_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary_filename = f\"mmlu_complete_summary_{timestamp}.csv\"\n",
    "        subject_stats.to_csv(summary_filename)\n",
    "        \n",
    "        print(f\"\\nüíæ COMPLETE RESULTS SAVED:\")\n",
    "        print(f\"  üìã Detailed results: {filename}\")\n",
    "        print(f\"  üìä Subject summary: {summary_filename}\")\n",
    "        \n",
    "        print(f\"\\nüîÑ ADAPTIVE ROUTING EFFECTIVENESS (COMPLETE ANALYSIS):\")\n",
    "        print(f\"  Total questions tested: {len(df):,}\")\n",
    "        print(f\"  Subjects covered: {len(df['subject'].unique())}\")\n",
    "        print(f\"  Unique model/provider combinations used: {len(model_provider_usage)}\")\n",
    "        print(f\"  Most frequently selected: {model_provider_usage.index[0] if len(model_provider_usage) > 0 else 'N/A'}\")\n",
    "        print(f\"  Routing diversity: {len(model_provider_usage)} different combinations\")\n",
    "        \n",
    "        # Performance by subject category analysis\n",
    "        print(f\"\\nüìö SUBJECT CATEGORY ANALYSIS:\")\n",
    "        # Group subjects by domain (rough categorization)\n",
    "        science_subjects = [s for s in df['subject'].unique() if any(term in s.lower() for term in ['physics', 'chemistry', 'biology', 'astronomy', 'anatomy'])]\n",
    "        math_subjects = [s for s in df['subject'].unique() if any(term in s.lower() for term in ['math', 'statistics', 'calculus', 'algebra', 'geometry'])]\n",
    "        humanities_subjects = [s for s in df['subject'].unique() if any(term in s.lower() for term in ['history', 'philosophy', 'literature', 'religion'])]\n",
    "        \n",
    "        for category, subjects in [('Science', science_subjects), ('Mathematics', math_subjects), ('Humanities', humanities_subjects)]:\n",
    "            if subjects:\n",
    "                category_df = df[df['subject'].isin(subjects)]\n",
    "                if len(category_df) > 0:\n",
    "                    category_accuracy = category_df['is_correct'].mean() * 100\n",
    "                    print(f\"  {category}: {category_accuracy:.1f}% accuracy across {len(subjects)} subjects\")\n",
    "    \n",
    "    print(f\"\\n‚ú® COMPLETE MMLU BENCHMARK FINISHED!\")\n",
    "    print(f\"üèÅ Total time: {total_time}\")\n",
    "    print(f\"üìä Overall performance: {overall_accuracy:.1f}% accuracy\")\n",
    "    return df\n",
    "\n",
    "# Run the complete benchmark\n",
    "print(\"üöÄ Starting COMPLETE MMLU dataset benchmark...\")\n",
    "print(\"üìã This will test your adaptive backend against ALL ~14,000 academic questions\")\n",
    "print(\"üîç Comprehensive analysis of model selection across all 59 subjects\")\n",
    "print(\"‚ö†Ô∏è  WARNING: This will take several hours and consume significant API credits!\\n\")\n",
    "\n",
    "# Run the complete benchmark\n",
    "complete_benchmark_results = run_complete_mmlu_benchmark()\n",
    "\n",
    "print(\"\\nüéâ COMPLETE BENCHMARK ANALYSIS FINISHED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADAPTIVE BACKEND COMPATIBILITY TESTER ===\n",
      "\n",
      "1. Go Backend Health Check:\n",
      "üîç Testing: GET https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/health\n",
      "‚úÖ Status: 200 | Time: 0.33s\n",
      "üìÑ Response: {\n",
      "  \"go_version\": \"go1.24.5\",\n",
      "  \"goroutines\": 14,\n",
      "  \"status\": \"healthy\",\n",
      "  \"timestamp\": \"2025-07-12T12:16:07.700473908Z\",\n",
      "  \"uptime\": \"40ns\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "3. Python AI Service Protocol Selection:\n",
      "üîç Testing: POST https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict\n",
      "‚ùå Error: HTTPSConnectionPool(host='prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "============================================================\n",
      "\n",
      "4. Go Backend Chat Completions (Basic):\n",
      "üîç Testing: POST https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/v1/chat/completions\n",
      "‚úÖ Status: 200 | Time: 1.46s\n",
      "üìÑ Response: {\n",
      "  \"id\": \"chatcmpl-BsTRKsIdepbqer4xsjQlQEPPW5hAm\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello! Your message came through successfully. How can I assist you today?\",\n",
      "        \"refusal\": \"\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": [],\n",
      "        \"audio\": {\n",
      "          \"id\": \"\",\n",
      "          \"data\": \"\",\n",
      "          \"expires_at\": 0,\n",
      "          \"transcript\": \"\"\n",
      "        },\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"\",\n",
      "          \"name\": \"\"\n",
      "        },\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1752322578,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 15,\n",
      "    \"prompt_tokens\": 13,\n",
      "    \"total_tokens\": 28,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"provider\": \"openai\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "\n",
      "5. Go Backend Chat Completions (Adaptive Features):\n",
      "üîç Testing: POST https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/v1/chat/completions\n",
      "‚úÖ Status: 200 | Time: 5.98s\n",
      "üìÑ Response: {\n",
      "  \"id\": \"chatcmpl-BsTRMfNlwXarOXI0DCE1Jnbd98G9D\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"message\": {\n",
      "        \"content\": \"Sure! At its core, quantum computing is a new way of processing information that takes advantage of the strange and fascinating principles of quantum mechanics, which govern the behavior of very tiny particles like atoms and photons.\\n\\nHere are some key concepts to understand quantum computing in simple terms:\\n\\n1. **Bits vs. Qubits**: In traditional computers, the basic unit of information is called a \\\"bit,\\\" which can be either a 0 or a 1. Quantum computers use \\\"qubits\\\" (quantum bits), which can be both 0 and 1 at the same time, thanks to a property called superposition. This means a quantum computer can handle a vast amount of information simultaneously.\\n\\n2. **Superposition**: Imagine a spinning coin. While it\\u2019s spinning, it\\u2019s not just heads or tails; it\\u2019s in a state that represents both until you stop it and look. Similarly, qubits can exist in multiple states at once, allowing quantum computers to perform many calculations at the same time.\\n\\n3. **Entanglement**: This is another key feature of quantum mechanics. When qubits become entangled, the state of one qubit is directly related to the state of another, no matter how far apart they are. This connection can be used to perform complex calculations much faster than classical computers.\\n\\n4. **Quantum Speedup**: Because of superposition and entanglement, quantum computers can solve certain problems much more quickly than traditional computers. For example, they can factor large numbers or search through databases in ways that would take classical computers an impractically long time.\\n\\n5. **Applications**: Quantum computing has the potential to revolutionize fields like cryptography, drug discovery, optimization problems, and complex simulations, among others. However, it\\u2019s still a developing technology, and practical, large-scale quantum computers are still in the works.\\n\\nIn summary, quantum computing harnesses the unique properties of quantum mechanics to process information in ways that classical computers cannot, potentially leading to breakthroughs in various fields.\",\n",
      "        \"refusal\": \"\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": [],\n",
      "        \"audio\": {\n",
      "          \"id\": \"\",\n",
      "          \"data\": \"\",\n",
      "          \"expires_at\": 0,\n",
      "          \"transcript\": \"\"\n",
      "        },\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"\",\n",
      "          \"name\": \"\"\n",
      "        },\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1752322580,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 407,\n",
      "    \"prompt_tokens\": 14,\n",
      "    \"total_tokens\": 421,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"provider\": \"openai\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "\n",
      "6. Cost Bias Variation Test:\n",
      "\n",
      "   6.1 Low cost bias (favor premium models):\n",
      "üîç Testing: POST https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/v1/chat/completions\n",
      "‚úÖ Status: 200 | Time: 1.51s\n",
      "üìÑ Response: {\n",
      "  \"id\": \"chatcmpl-BsTRSsBmyvlY8Q4AxSOk7pUOQjB4u\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"message\": {\n",
      "        \"content\": \"2 + 2 equals 4.\",\n",
      "        \"refusal\": \"\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": [],\n",
      "        \"audio\": {\n",
      "          \"id\": \"\",\n",
      "          \"data\": \"\",\n",
      "          \"expires_at\": 0,\n",
      "          \"transcript\": \"\"\n",
      "        },\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"\",\n",
      "          \"name\": \"\"\n",
      "        },\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1752322586,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"system_fingerprint\": \"fp_62a23a81ef\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 8,\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"total_tokens\": 25,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"provider\": \"openai\"\n",
      "}\n",
      "   ‚Üí Selected: openai/gpt-4o-mini-2024-07-18\n",
      "\n",
      "   6.2 High cost bias (favor cheaper models):\n",
      "üîç Testing: POST https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/v1/chat/completions\n",
      "‚úÖ Status: 200 | Time: 1.07s\n",
      "üìÑ Response: {\n",
      "  \"id\": \"chatcmpl-BsTRTy6BlpyLtGtu0Y8xkdBcOoyxr\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"message\": {\n",
      "        \"content\": \"The capital of France is Paris.\",\n",
      "        \"refusal\": \"\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": [],\n",
      "        \"audio\": {\n",
      "          \"id\": \"\",\n",
      "          \"data\": \"\",\n",
      "          \"expires_at\": 0,\n",
      "          \"transcript\": \"\"\n",
      "        },\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"\",\n",
      "          \"name\": \"\"\n",
      "        },\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1752322587,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": \"default\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 7,\n",
      "    \"prompt_tokens\": 16,\n",
      "    \"total_tokens\": 23,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"provider\": \"openai\"\n",
      "}\n",
      "   ‚Üí Selected: openai/gpt-4o-mini-2024-07-18\n",
      "\n",
      "============================================================\n",
      "\n",
      "7. Backend Integration Analysis:\n",
      "   ‚ùå Python AI Service: Not responding properly\n",
      "   ‚úÖ Go Backend Adaptive Features: Working (openai/gpt-4o-mini-2024-07-18)\n",
      "\n",
      "   üîó Integration Status:\n",
      "   ‚ö†Ô∏è  PARTIAL INTEGRATION: Go backend working, check Python service connection\n",
      "\n",
      "============================================================\n",
      "\n",
      "8. Configuration Recommendations:\n",
      "   üìã Required Environment Variables for Go Backend:\n",
      "   ‚Ä¢ OPENAI_API_KEY: For semantic cache initialization\n",
      "   ‚Ä¢ ADAPTIVE_AI_BASE_URL: Points to Python service\n",
      "   ‚Ä¢ ADDR: Server port (default :8080)\n",
      "   ‚Ä¢ ALLOWED_ORIGINS: CORS configuration\n",
      "\n",
      "   üîß Expected Request Format for Chat Completions:\n",
      "   {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Your message\"\n",
      "    }\n",
      "  ],\n",
      "  \"provider_constraint\": [\n",
      "    \"openai\",\n",
      "    \"deepseek\"\n",
      "  ],\n",
      "  \"cost_bias\": 0.5,\n",
      "  \"stream\": false\n",
      "}\n",
      "\n",
      "   üåê Service Endpoints:\n",
      "   ‚Ä¢ Go Backend: https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\n",
      "   ‚Ä¢ Python AI Service: https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\n",
      "   ‚Ä¢ Go Health: GET /health\n",
      "   ‚Ä¢ Go Chat: POST /v1/chat/completions\n",
      "   ‚Ä¢ Python Health: GET /health\n",
      "   ‚Ä¢ Python Predict: POST /predict\n",
      "\n",
      "‚ú® Compatibility test complete!\n",
      "üí° Use this information to verify your adaptive backend setup.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Backend URL Tester - Compatible with Adaptive Backend\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration - UPDATE THESE VALUES\n",
    "BASE_URL = \"https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "PYTHON_SERVICE_URL = \"https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "\n",
    "def quick_test(url, endpoint=\"/health\", method=\"GET\", data=None):\n",
    "    \"\"\"Quick test function for any backend endpoint\"\"\"\n",
    "    full_url = f\"{url.rstrip('/')}/{endpoint.lstrip('/')}\"\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    print(f\"üîç Testing: {method} {full_url}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.request(method, full_url, headers=headers, json=data, timeout=10)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Status: {response.status_code} | Time: {response_time:.2f}s\")\n",
    "        \n",
    "        try:\n",
    "            result = response.json()\n",
    "            print(f\"üìÑ Response: {json.dumps(result, indent=2)}\")\n",
    "        except:\n",
    "            print(f\"üìÑ Response: {response.text}\")\n",
    "            \n",
    "        return {\"success\": True, \"status\": response.status_code, \"data\": result if 'result' in locals() else response.text}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "# Test Examples - Compatible with Adaptive Backend Architecture\n",
    "print(\"=== ADAPTIVE BACKEND COMPATIBILITY TESTER ===\\n\")\n",
    "\n",
    "# 1. Go Backend Health Check\n",
    "print(\"1. Go Backend Health Check:\")\n",
    "quick_test(BASE_URL, \"/health\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Python AI Service Protocol Selection Test\n",
    "print(\"3. Python AI Service Protocol Selection:\")\n",
    "predict_data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Test protocol selection with a complex mathematical question about quantum mechanics.\"}],\n",
    "    \"user_id\": \"test-user\",\n",
    "    \"provider_constraint\": [\"openai\", \"deepseek\", \"anthropic\"],\n",
    "    \"cost_bias\": 0.7\n",
    "}\n",
    "python_result = quick_test(PYTHON_SERVICE_URL, \"/predict\", \"POST\", predict_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 4. Go Backend Chat Completions - Basic Test\n",
    "print(\"4. Go Backend Chat Completions (Basic):\")\n",
    "basic_chat_data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! Test basic message.\"}],\n",
    "\n",
    "}\n",
    "quick_test(BASE_URL, \"/v1/chat/completions\", \"POST\", basic_chat_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 5. Go Backend Chat Completions - Adaptive Features Test\n",
    "print(\"5. Go Backend Chat Completions (Adaptive Features):\")\n",
    "adaptive_chat_data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}],\n",
    "    \"provider_constraint\": [\"openai\", \"deepseek\"],  # Multiple providers\n",
    "    \"cost_bias\": 0.8  # Favor cheaper options\n",
    "}\n",
    "adaptive_result = quick_test(BASE_URL, \"/v1/chat/completions\", \"POST\", adaptive_chat_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 6. Go Backend Chat Completions - Cost Bias Variation Test\n",
    "print(\"6. Cost Bias Variation Test:\")\n",
    "test_prompts = [\n",
    "    {\"content\": \"Simple math: What is 2+2?\", \"cost_bias\": 0.2, \"description\": \"Low cost bias (favor premium models)\"},\n",
    "    {\"content\": \"Quick question: What's the capital of France?\", \"cost_bias\": 0.9, \"description\": \"High cost bias (favor cheaper models)\"}\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n   6.{i} {test['description']}:\")\n",
    "    cost_test_data = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": test[\"content\"]}],\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"max_tokens\": 80,\n",
    "        \"provider_constraint\": [\"openai\", \"deepseek\"],\n",
    "        \"cost_bias\": test[\"cost_bias\"]\n",
    "    }\n",
    "    \n",
    "    result = quick_test(BASE_URL, \"/v1/chat/completions\", \"POST\", cost_test_data)\n",
    "    \n",
    "    # Extract model selection info if available\n",
    "    if result.get(\"success\") and \"data\" in result:\n",
    "        try:\n",
    "            data = result[\"data\"] if isinstance(result[\"data\"], dict) else json.loads(result[\"data\"])\n",
    "            selected_model = data.get(\"model\", \"unknown\")\n",
    "            selected_provider = data.get(\"provider\", \"unknown\")\n",
    "            print(f\"   ‚Üí Selected: {selected_provider}/{selected_model}\")\n",
    "        except:\n",
    "            print(\"   ‚Üí Model selection info not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 7. Backend Integration Analysis\n",
    "print(\"7. Backend Integration Analysis:\")\n",
    "\n",
    "# Check if Python service is working\n",
    "python_working = False\n",
    "if \"data\" in str(python_result):\n",
    "    try:\n",
    "        if isinstance(python_result.get(\"data\"), dict):\n",
    "            protocol = python_result[\"data\"].get(\"protocol\")\n",
    "            if protocol:\n",
    "                python_working = True\n",
    "                print(f\"   ‚úÖ Python AI Service: Working (Protocol: {protocol})\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Python AI Service: Responding but format unclear\")\n",
    "    except:\n",
    "        print(\"   ‚ùå Python AI Service: Response parsing failed\")\n",
    "else:\n",
    "    print(\"   ‚ùå Python AI Service: Not responding properly\")\n",
    "\n",
    "# Check if adaptive features are working in Go backend\n",
    "adaptive_working = False\n",
    "if \"data\" in str(adaptive_result):\n",
    "    try:\n",
    "        if isinstance(adaptive_result.get(\"data\"), dict):\n",
    "            model = adaptive_result[\"data\"].get(\"model\")\n",
    "            provider = adaptive_result[\"data\"].get(\"provider\")\n",
    "            if model and provider:\n",
    "                adaptive_working = True\n",
    "                print(f\"   ‚úÖ Go Backend Adaptive Features: Working ({provider}/{model})\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Go Backend: Basic functionality working\")\n",
    "    except:\n",
    "        print(\"   ‚ùå Go Backend: Response parsing failed\")\n",
    "\n",
    "# Integration status\n",
    "print(f\"\\n   üîó Integration Status:\")\n",
    "if python_working and adaptive_working:\n",
    "    print(\"   ‚úÖ FULL INTEGRATION: Both services working and communicating\")\n",
    "elif adaptive_working:\n",
    "    print(\"   ‚ö†Ô∏è  PARTIAL INTEGRATION: Go backend working, check Python service connection\")\n",
    "elif python_working:\n",
    "    print(\"   ‚ö†Ô∏è  SERVICES ISOLATED: Python service working but Go backend not using it\")\n",
    "else:\n",
    "    print(\"   ‚ùå INTEGRATION ISSUES: Check both service configurations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 8. Configuration Recommendations\n",
    "print(\"8. Configuration Recommendations:\")\n",
    "print(\"   üìã Required Environment Variables for Go Backend:\")\n",
    "print(\"   ‚Ä¢ OPENAI_API_KEY: For semantic cache initialization\")\n",
    "print(\"   ‚Ä¢ ADAPTIVE_AI_BASE_URL: Points to Python service\")\n",
    "print(\"   ‚Ä¢ ADDR: Server port (default :8080)\")\n",
    "print(\"   ‚Ä¢ ALLOWED_ORIGINS: CORS configuration\")\n",
    "\n",
    "print(\"\\n   üîß Expected Request Format for Chat Completions:\")\n",
    "expected_format = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Your message\"}],\n",
    "    \"provider_constraint\": [\"openai\", \"deepseek\"],  # Adaptive feature\n",
    "    \"cost_bias\": 0.5,  # Adaptive feature (0.0-1.0)\n",
    "    \"stream\": False  # Optional \n",
    "}\n",
    "print(f\"   {json.dumps(expected_format, indent=2)}\")\n",
    "\n",
    "print(\"\\n   üåê Service Endpoints:\")\n",
    "print(f\"   ‚Ä¢ Go Backend: {BASE_URL}\")\n",
    "print(f\"   ‚Ä¢ Python AI Service: {PYTHON_SERVICE_URL}\")\n",
    "print(\"   ‚Ä¢ Go Health: GET /health\")\n",
    "print(\"   ‚Ä¢ Go Chat: POST /v1/chat/completions\")\n",
    "print(\"   ‚Ä¢ Python Health: GET /health\")\n",
    "print(\"   ‚Ä¢ Python Predict: POST /predict\")\n",
    "\n",
    "print(\"\\n‚ú® Compatibility test complete!\")\n",
    "print(\"üí° Use this information to verify your adaptive backend setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 500\n",
      "Response:\n",
      "{\n",
      "  \"code\": 500,\n",
      "  \"error\": \"runtime error: invalid memory address or nil pointer dereference\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE VERSION:\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your backend URL and endpoint\n",
    "BASE_URL = \"https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "ENDPOINT = \"/v1/chat/completions\"\n",
    "\n",
    "# Example request data\n",
    "data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! Simple test message.\"}],\n",
    "}\n",
    "\n",
    "def test_backend():\n",
    "    url = f\"{BASE_URL.rstrip('/')}/{ENDPOINT.lstrip('/')}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=10)\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        print(\"Response:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "test_backend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
