---
# Adaptive Backend Configuration
# Environment variables can be referenced using ${VAR} or ${VAR:-default} syntax (defaults apply when a variable is unset or empty)

server:
  port: "${PORT:-8080}"
  allowed_origins: "${ALLOWED_ORIGINS:-http://localhost:3000}"
  environment: "${ENV:-development}"
  log_level: "${LOG_LEVEL:-info}"
  jwt_secret: "${JWT_SECRET}"

# Endpoint-specific provider configurations
endpoints:
  chat_completions:
    providers:
      openai:
        api_key: "${OPENAI_API_KEY}"
        enabled: true

      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        enabled: true
        base_url: "https://api.anthropic.com/v1"

      gemini:
        api_key: "${GEMINI_API_KEY}"
        enabled: true
        base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"

      deepseek:
        api_key: "${DEEPSEEK_API_KEY}"
        enabled: true
        base_url: "https://api.deepseek.com"

  messages:
    providers:
      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        enabled: true
        base_url: "https://api.anthropic.com"

  select_model:
    providers:
      openai:
        api_key: "${OPENAI_API_KEY}"
        enabled: true

      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        enabled: true
        base_url: "https://api.anthropic.com/v1"

      gemini:
        api_key: "${GEMINI_API_KEY}"
        enabled: true
        base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"

      deepseek:
        api_key: "${DEEPSEEK_API_KEY}"
        enabled: true
        base_url: "https://api.deepseek.com"

      huggingface:
        api_key: "${HUGGINGFACE_API_KEY}"
        enabled: true
        base_url: "https://router.huggingface.co/v1"

  generate:
    providers:
      gemini:
        api_key: "${GEMINI_API_KEY}"
        enabled: true
        base_url: "https://generativelanguage.googleapis.com/v1beta"

# Services configuration
services:
  model_router:
    cost_bias: 0.9 # 0.0 = cheapest, 1.0 = best performance
    semantic_cache:
      enabled: true
      semantic_threshold: 0.95
      openai_api_key: "${OPENAI_API_KEY}" # For embeddings
    client:
      base_url: "${MODEL_ROUTER_BASE_URL:-http://localhost:8000}"
      timeout_ms: 3000
      circuit_breaker:
        failure_threshold: 3
        success_threshold: 2
        timeout_ms: 5000
        reset_after_ms: 30000

  redis:
    url: "${REDIS_URL:-redis://localhost:6379}"

# Fallback configuration
fallback:
  mode: "race" # "race" or "sequential"
  timeout_ms: 30000 # Keep longer for streaming LLM responses
  max_retries: 3
  circuit_breaker:
    failure_threshold: 5
    success_threshold: 3
    timeout_ms: 15000
    reset_after_ms: 60000

# Prompt cache configuration
prompt_cache:
  enabled: false
