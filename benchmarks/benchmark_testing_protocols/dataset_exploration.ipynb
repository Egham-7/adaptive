{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration and Endpoint Benchmarking\n",
    "\n",
    "This notebook provides comprehensive exploration and benchmarking tools for instruction-following datasets against AI model endpoints.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook enables you to:\n",
    "- **Load and analyze datasets**: Load instruction datasets from HuggingFace Hub with sampling options\n",
    "- **Endpoint testing**: Test AI model selection endpoints with real dataset samples\n",
    "- **Cost analysis**: Calculate and track API usage costs across different models\n",
    "- **Export results**: Save detailed results and summaries to CSV files\n",
    "\n",
    "### Supported Datasets:\n",
    "- **Databricks Dolly 15k**: High-quality instruction dataset (default)\n",
    "- **Open-Orca datasets**: Large-scale instruction datasets\n",
    "- **Any HuggingFace instruction dataset**: Customizable dataset loading\n",
    "\n",
    "### Features:\n",
    "- ✅ **Model cost tracking** with accurate pricing data\n",
    "- ✅ **Progress monitoring** with real-time updates  \n",
    "- ✅ **Error handling** for robust API testing\n",
    "- ✅ **Flexible JSON payload** supporting multiple AI providers\n",
    "- ✅ **Detailed analytics** with model selection distributions\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Install Dependencies\n",
    "```bash\n",
    "# Using pip\n",
    "pip install datasets pandas requests numpy matplotlib seaborn huggingface_hub\n",
    "\n",
    "# Using uv (recommended)\n",
    "uv add datasets pandas requests numpy matplotlib seaborn huggingface_hub\n",
    "```\n",
    "\n",
    "### 2. Configure HuggingFace Authentication\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "### 3. Update Endpoint Configuration\n",
    "Before running, update the `ENDPOINT_URL` variable in the \"Configuration\" section with your actual endpoint URL.\n",
    "\n",
    "### 4. Customize Model Costs (Optional)\n",
    "Update the `model_costs` dictionary in the `process_dataset_with_endpoint` function to match your actual model pricing.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run all cells sequentially\n",
    "2. The notebook will load 1000 samples from Databricks Dolly 15k dataset\n",
    "3. Test your endpoint connectivity\n",
    "4. Process samples and generate cost analysis\n",
    "5. Results are saved as CSV files with timestamps\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- `dolly_endpoint_results_YYYYMMDD_HHMMSS.csv`: Detailed results for each sample\n",
    "- `dolly_summary_YYYYMMDD_HHMMSS.csv`: Summary statistics and costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**✏️ Update these settings before running the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n",
      "📊 Dataset: databricks/databricks-dolly-15k\n",
      "📊 Sample size: 1000\n",
      "🌐 Endpoint: https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict\n",
      "⚠️  Remember to update ENDPOINT_URL before running!\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION SECTION =====\n",
    "# Update these variables before running the notebook\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_NAME = \"databricks/databricks-dolly-15k\"  # HuggingFace dataset identifier\n",
    "SAMPLE_SIZE = 1000  # Number of samples to process (None for all)\n",
    "DATASET_SPLIT = \"train\"  # Dataset split to use\n",
    "\n",
    "# Endpoint Configuration\n",
    "ENDPOINT_URL = \"https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict\"  # ⚠️ UPDATE THIS\n",
    "\n",
    "# Processing Configuration\n",
    "MAX_SAMPLES_TO_PROCESS = 1000  # Maximum samples to send to endpoint\n",
    "REQUEST_TIMEOUT = 30  # Timeout for each API request (seconds)\n",
    "DELAY_BETWEEN_REQUESTS = 0.05  # Delay between requests (seconds)\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_CSV_RESULTS = True  # Whether to save results to CSV\n",
    "SAVE_SUMMARY = True  # Whether to save summary statistics\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"📊 Dataset: {DATASET_NAME}\")\n",
    "print(f\"📊 Sample size: {SAMPLE_SIZE}\")\n",
    "print(f\"🌐 Endpoint: {ENDPOINT_URL}\")\n",
    "print(\"⚠️  Remember to update ENDPOINT_URL before running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import time\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Requests available for API testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Authenticated as: AImen44\n",
      "User type: user\n"
     ]
    }
   ],
   "source": [
    "# Check HuggingFace authentication\n",
    "from huggingface_hub import whoami\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"✓ Authenticated as: {user_info['name']}\")\n",
    "    print(f\"User type: {user_info.get('type', 'Unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Authentication failed: {e}\")\n",
    "    print(\"Please run: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure(dataset_info: dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and display dataset structure and statistics.\n",
    "    \"\"\"\n",
    "    if 'error' in dataset_info:\n",
    "        print(f\"Cannot analyze {dataset_info['dataset_name']} due to error: {dataset_info['error']}\")\n",
    "        return\n",
    "\n",
    "    samples = dataset_info['samples']\n",
    "    if not samples:\n",
    "        print(\"No samples to analyze\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATASET ANALYSIS: {dataset_info['dataset_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"Total samples: {dataset_info['num_samples']:,}\")\n",
    "    print(f\"Load time: {dataset_info['load_time']:.2f}s\")\n",
    "    print(f\"Streaming mode: {dataset_info['streaming']}\")\n",
    "\n",
    "    # Schema analysis\n",
    "    first_sample = samples[0]\n",
    "    print(f\"\\nSchema ({len(first_sample)} fields):\")\n",
    "    for field, value in first_sample.items():\n",
    "        value_type = type(value).__name__\n",
    "        if isinstance(value, str):\n",
    "            print(f\"  {field}: {value_type} (avg length: {len(value)} chars)\")\n",
    "        elif isinstance(value, list | dict):\n",
    "            print(f\"  {field}: {value_type} (length: {len(value)})\")\n",
    "        else:\n",
    "            print(f\"  {field}: {value_type} = {value}\")\n",
    "\n",
    "    # Text statistics for string fields\n",
    "    print(f\"\\nText Statistics (based on {len(samples)} samples):\")\n",
    "    for field in first_sample.keys():\n",
    "        if isinstance(first_sample[field], str):\n",
    "            lengths = [len(str(sample[field])) for sample in samples]\n",
    "            print(f\"  {field}:\")\n",
    "            print(f\"    Min: {min(lengths)} chars\")\n",
    "            print(f\"    Max: {max(lengths)} chars\")\n",
    "            print(f\"    Avg: {np.mean(lengths):.1f} chars\")\n",
    "            print(f\"    Median: {np.median(lengths):.1f} chars\")\n",
    "\n",
    "def display_sample_data(dataset_info: dict[str, Any], num_samples: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Display sample data from the dataset.\n",
    "    \"\"\"\n",
    "    if 'error' in dataset_info:\n",
    "        return\n",
    "\n",
    "    samples = dataset_info['samples']\n",
    "    if not samples:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAMPLE DATA: {dataset_info['dataset_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, sample in enumerate(samples[:num_samples]):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        for field, value in sample.items():\n",
    "            if isinstance(value, str):\n",
    "                if len(value) > 200:\n",
    "                    print(f\"{field}: {value[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"{field}: {value}\")\n",
    "            else:\n",
    "                print(f\"{field}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "def load_and_sample_dataset(dataset_name: str, sample_size: int | None = None,\n",
    "                          dataset_split: str = \"train\", streaming: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Load a dataset from HuggingFace Hub with optional sampling.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: HuggingFace dataset identifier\n",
    "        sample_size: Number of samples to load (None for all)\n",
    "        dataset_split: Dataset split to use\n",
    "        streaming: Whether to use streaming mode\n",
    "\n",
    "    Returns:\n",
    "        Dict with dataset info and samples\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load dataset\n",
    "        if streaming:\n",
    "            dataset = load_dataset(dataset_name, split=dataset_split, streaming=True)\n",
    "            if sample_size:\n",
    "                dataset = dataset.take(sample_size)\n",
    "            samples = list(dataset)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "            if sample_size:\n",
    "                # Get a sample\n",
    "                if sample_size >= len(dataset):\n",
    "                    samples = list(dataset)\n",
    "                else:\n",
    "                    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "                    samples = [dataset[int(i)] for i in indices]\n",
    "            else:\n",
    "                samples = list(dataset)\n",
    "\n",
    "        load_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'num_samples': len(samples),\n",
    "            'samples': samples,\n",
    "            'load_time': load_time,\n",
    "            'streaming': streaming,\n",
    "            'split': dataset_split\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'error': str(e),\n",
    "            'samples': []\n",
    "        }\n",
    "\n",
    "print(\"✓ Analysis functions and dataset loader defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Testing and Benchmarking\n",
    "\n",
    "Now let's test and benchmark different instruction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_endpoint(dataset_info: dict, endpoint_url: str,\n",
    "                                max_samples: int | None = None, save_csv: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process dataset instructions through the endpoint and collect results with cost calculation.\n",
    "\n",
    "    Args:\n",
    "        dataset_info: Dataset information from load_and_sample_dataset\n",
    "        endpoint_url: Endpoint URL\n",
    "        max_samples: Maximum number of samples to process (None for all)\n",
    "        save_csv: Whether to save results to CSV file\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all requests and responses\n",
    "    \"\"\"\n",
    "    if 'error' in dataset_info or not dataset_info['samples']:\n",
    "        print(\"❌ No valid dataset to process\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Model costs (cost per 1M tokens) - update these based on your model catalog\n",
    "    model_costs = {\n",
    "        \"gemini-2.5-flash-lite-preview-06-17\": {\"input\": 0.075, \"output\": 0.30},\n",
    "        \"gemini-2.5-flash\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gemini-2.5-pro\": {\"input\": 1.25, \"output\": 10.00},\n",
    "        \"mistral-small-latest\": {\"input\": 0.10, \"output\": 0.30},\n",
    "        \"gpt-4.1-nano\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4.1-mini\": {\"input\": 0.40, \"output\": 1.60},\n",
    "        \"gpt-4.1\": {\"input\": 2.00, \"output\": 8.00},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"o3-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "        \"o4-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "        \"o3\": {\"input\": 10.00, \"output\": 40.00},\n",
    "        \"gpt-4.5\": {\"input\": 75.00, \"output\": 150.00},\n",
    "        \"o1\": {\"input\": 15.00, \"output\": 60.00},\n",
    "        \"o1-pro\": {\"input\": 150.00, \"output\": 600.00},\n",
    "        \"deepseek-chat\": {\"input\": 0.14, \"output\": 0.28},\n",
    "        \"deepseek-reasoner\": {\"input\": 0.55, \"output\": 2.19},\n",
    "        \"grok-3-mini\": {\"input\": 0.30, \"output\": 0.50},\n",
    "        \"grok-3\": {\"input\": 3.00, \"output\": 15.00},\n",
    "        \"claude-sonnet-4-20250514\": {\"input\": 3.00, \"output\": 15.00},\n",
    "        \"claude-opus-4-20250514\": {\"input\": 15.00, \"output\": 75.00},\n",
    "        \"Qwen/Qwen2.5-14B-Instruct\": {\"input\": 0.12, \"output\": 0.12},\n",
    "        \"meta-llama/Llama-3.1-8B-Instruct\": {\"input\": 0.10, \"output\": 0.10},\n",
    "        \"codellama/CodeLlama-13b-Instruct-hf\": {\"input\": 0.11, \"output\": 0.11},\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\": {\"input\": 0.08, \"output\": 0.08},\n",
    "        \"google/flan-t5-xl\": {\"input\": 0.06, \"output\": 0.06},\n",
    "        \"microsoft/deberta-v3-large\": {\"input\": 0.04, \"output\": 0.04},\n",
    "    }\n",
    "\n",
    "    def estimate_tokens(text: str) -> int:\n",
    "        \"\"\"Rough token estimation (1 token ≈ 4 characters)\"\"\"\n",
    "        return len(text) // 4\n",
    "\n",
    "    def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate cost based on model and token counts\"\"\"\n",
    "        if model_name not in model_costs:\n",
    "            return 0.0\n",
    "\n",
    "        costs = model_costs[model_name]\n",
    "        input_cost = (input_tokens / 1_000_000) * costs[\"input\"]\n",
    "        output_cost = (output_tokens / 1_000_000) * costs[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "    samples = dataset_info['samples']\n",
    "    if max_samples:\n",
    "        samples = samples[:max_samples]\n",
    "\n",
    "    print(f\"🚀 Processing {len(samples)} samples through endpoint...\")\n",
    "    print(f\"📊 Endpoint: {endpoint_url}\")\n",
    "\n",
    "    results = []\n",
    "    successful_calls = 0\n",
    "    failed_calls = 0\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Progress: {i}/{len(samples)} ({i/len(samples)*100:.1f}%) - Cost so far: ${total_cost:.6f}\")\n",
    "\n",
    "        # Extract fields from sample\n",
    "        instruction = sample.get('instruction', '')\n",
    "        context = sample.get('context', '')\n",
    "        response = sample.get('response', '')  # Use for output token estimation only\n",
    "\n",
    "        # Create full prompt\n",
    "        full_prompt = f\"{instruction}\\n\\nContext: {context}\" if context.strip() else instruction\n",
    "\n",
    "        # Call endpoint\n",
    "        start_time = time.time()\n",
    "        api_result = call_endpoint(endpoint_url, instruction, context)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Estimate token counts\n",
    "        input_tokens = estimate_tokens(full_prompt)\n",
    "        output_tokens = estimate_tokens(response) if response else 100  # Use actual response for estimation\n",
    "\n",
    "        # Prepare result row\n",
    "        result_row = {\n",
    "            'sample_id': i,\n",
    "            'instruction': instruction,\n",
    "            'context': context,\n",
    "            'full_prompt': full_prompt,\n",
    "            'input_token_estimate': input_tokens,\n",
    "            'output_token_estimate': output_tokens,\n",
    "            'api_success': api_result['success'],\n",
    "            'api_status_code': api_result['status_code'],\n",
    "            'api_error': api_result['error'],\n",
    "            'response_time_seconds': end_time - start_time,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Add API response fields if successful\n",
    "        if api_result['success'] and api_result['response']:\n",
    "            api_response = api_result['response']\n",
    "\n",
    "            # Extract protocol and model information\n",
    "            protocol = api_response.get('protocol', '')\n",
    "            selected_model = ''\n",
    "            selected_provider = ''\n",
    "            estimated_cost = 0.0\n",
    "\n",
    "            # Parse based on protocol type\n",
    "            if protocol == 'standard' and 'standard' in api_response:\n",
    "                standard_info = api_response['standard']\n",
    "                selected_model = standard_info.get('model', '')\n",
    "                selected_provider = standard_info.get('provider', '')\n",
    "\n",
    "                # Calculate cost for selected model\n",
    "                estimated_cost = calculate_cost(selected_model, input_tokens, output_tokens)\n",
    "\n",
    "            elif protocol == 'minion' and 'minion' in api_response:\n",
    "                minion_info = api_response['minion']\n",
    "                selected_model = minion_info.get('model', '')\n",
    "                selected_provider = 'huggingface'  # Minions are HuggingFace models\n",
    "\n",
    "                # Calculate cost for selected model\n",
    "                estimated_cost = calculate_cost(selected_model, input_tokens, output_tokens)\n",
    "\n",
    "            # Update result with API response details\n",
    "            result_row.update({\n",
    "                'api_protocol': protocol,\n",
    "                'api_selected_model': selected_model,\n",
    "                'api_selected_provider': selected_provider,\n",
    "                'api_estimated_cost_usd': estimated_cost,\n",
    "                'api_full_response': json.dumps(api_response, indent=2)\n",
    "            })\n",
    "\n",
    "            total_cost += estimated_cost\n",
    "            successful_calls += 1\n",
    "        else:\n",
    "            # Add empty fields for failed calls\n",
    "            result_row.update({\n",
    "                'api_protocol': '',\n",
    "                'api_selected_model': '',\n",
    "                'api_selected_provider': '',\n",
    "                'api_estimated_cost_usd': 0.0,\n",
    "                'api_full_response': ''\n",
    "            })\n",
    "            failed_calls += 1\n",
    "\n",
    "        results.append(result_row)\n",
    "\n",
    "        # Small delay to avoid overwhelming the endpoint\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n✅ Processing complete!\")\n",
    "    print(f\"📊 Results: {successful_calls} successful, {failed_calls} failed\")\n",
    "    print(f\"📈 Success rate: {successful_calls/(successful_calls+failed_calls)*100:.1f}%\")\n",
    "    print(f\"💰 Total estimated cost: ${total_cost:.6f} USD\")\n",
    "    print(f\"💰 Average cost per request: ${total_cost/len(samples):.6f} USD\")\n",
    "\n",
    "    if save_csv:\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"dolly_endpoint_results_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"💾 Results saved to: {filename}\")\n",
    "\n",
    "        # Also save a summary\n",
    "        summary_filename = f\"dolly_summary_{timestamp}.csv\"\n",
    "\n",
    "        summary_df = pd.DataFrame([{\n",
    "            'total_samples': len(samples),\n",
    "            'successful_calls': successful_calls,\n",
    "            'failed_calls': failed_calls,\n",
    "            'success_rate_percent': successful_calls/(successful_calls+failed_calls)*100,\n",
    "            'total_cost_usd': total_cost,\n",
    "            'avg_cost_per_request_usd': total_cost/len(samples),\n",
    "            'endpoint_url': endpoint_url,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }])\n",
    "        summary_df.to_csv(summary_filename, index=False)\n",
    "        print(f\"📋 Summary saved to: {summary_filename}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"✅ Dataset processing function with cost calculation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint connection...\n",
      "✅ Endpoint https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict is accessible (405 expected for GET on POST endpoint)\n",
      "Endpoint accessible: True\n"
     ]
    }
   ],
   "source": [
    "# Endpoint configuration and functions\n",
    "def test_endpoint_connection(url: str) -> bool:\n",
    "    \"\"\"Test if the endpoint is accessible.\"\"\"\n",
    "    try:\n",
    "        # For LitServe endpoints, try the predict endpoint directly\n",
    "        response = requests.get(f\"{url.rstrip('/')}\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ Endpoint {url} is accessible\")\n",
    "            return True\n",
    "        elif response.status_code == 405:  # Method not allowed (GET on POST endpoint)\n",
    "            print(f\"✅ Endpoint {url} is accessible (405 expected for GET on POST endpoint)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ Endpoint returned status code: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Failed to connect to endpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_model_selection_request(prompt: str, context: str = \"\") -> dict:\n",
    "    \"\"\"Create a request payload matching ModelSelectionRequest structure.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n\\nContext: {context}\" if context.strip() else prompt\n",
    "\n",
    "    # Active providers: OpenAI, GROQ (fast inference), GROK (X.AI models), and DeepSeek\n",
    "    active_providers = [\n",
    "        \"openai\",      # ProviderType.OPENAI\n",
    "        \"groq\",        # ProviderType.GROQ (fast inference provider)\n",
    "        \"grok\",        # ProviderType.GROK (X.AI's grok-3 models)\n",
    "        \"deepseek\",    # ProviderType.DEEPSEEK\n",
    "    ]\n",
    "\n",
    "    # Correct JSON format matching ModelSelectionRequest from llm_core_models.py\n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"user_id\": None,\n",
    "        \"provider_constraint\": active_providers,  # Only include active providers\n",
    "        \"cost_bias\": None\n",
    "    }\n",
    "\n",
    "def call_endpoint(url: str, prompt: str, context: str = \"\", timeout: int | None = None) -> dict:\n",
    "    \"\"\"Call the endpoint with a single prompt.\"\"\"\n",
    "    if timeout is None:\n",
    "        timeout = REQUEST_TIMEOUT\n",
    "\n",
    "    try:\n",
    "        payload = create_model_selection_request(prompt, context)\n",
    "\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response.json(),\n",
    "                \"status_code\": 200,\n",
    "                \"error\": None\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"response\": None,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"error\": f\"HTTP {response.status_code}: {response.text[:200]}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"status_code\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Test endpoint connection\n",
    "print(\"Testing endpoint connection...\")\n",
    "endpoint_accessible = test_endpoint_connection(ENDPOINT_URL)\n",
    "print(f\"Endpoint accessible: {endpoint_accessible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing endpoint with sample data...\n",
      "Test instruction: Identify the bird from the list: Not, Knot, Nought\n",
      "Test context: \n",
      "\n",
      "JSON payload being sent:\n",
      "{\n",
      "  \"prompt\": \"Identify the bird from the list: Not, Knot, Nought\",\n",
      "  \"user_id\": null,\n",
      "  \"provider_constraint\": [\n",
      "    \"openai\",\n",
      "    \"groq\",\n",
      "    \"grok\",\n",
      "    \"deepseek\"\n",
      "  ],\n",
      "  \"cost_bias\": null\n",
      "}\n",
      "\n",
      "✅ Endpoint test successful!\n",
      "Status code: 200\n",
      "API Response preview:\n",
      "{\n",
      "  \"protocol\": \"minion\",\n",
      "  \"standard\": null,\n",
      "  \"minion\": {\n",
      "    \"model\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
      "    \"base_url\": \"https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-14B-Instruct/v1\",\n",
      "    \"parameters\": {\n",
      "      \"temperature\": 0.7,\n",
      "      \"top_p\": 0.9,\n",
      "      \"max_tokens\": 2048,\n",
      "      \"n\": 1,\n",
      "      \"stop\": null,\n",
      "      \"frequency_penalty\": 0.0,\n",
      "      \"presence_penalty\": 0.0\n",
      "    },\n",
      "    \"alternatives\": [\n",
      "      {\n",
      "        \"model\": \"codellama/CodeLlama-13b-Instruct-hf\",\n",
      "        \"base_url\":...\n"
     ]
    }
   ],
   "source": [
    "# Test endpoint with sample data\n",
    "if endpoint_accessible:\n",
    "    print(\"🧪 Testing endpoint with sample data...\")\n",
    "\n",
    "    # Load dataset first if not already loaded\n",
    "    if 'dolly_info' not in globals():\n",
    "        print(\"🔄 Loading dataset for testing...\")\n",
    "        dolly_info = load_and_sample_dataset(\n",
    "            DATASET_NAME,\n",
    "            sample_size=10,  # Just load a few samples for testing\n",
    "            dataset_split=DATASET_SPLIT\n",
    "        )\n",
    "\n",
    "    # Check if dolly_info is available and valid\n",
    "    if 'dolly_info' in globals() and 'error' not in dolly_info:\n",
    "        # Test with first sample from Dolly dataset\n",
    "        test_sample = dolly_info['samples'][0]\n",
    "        test_instruction = test_sample['instruction']\n",
    "        test_context = test_sample['context']\n",
    "\n",
    "        print(f\"Test instruction: {test_instruction}\")\n",
    "        print(f\"Test context: {test_context[:100]}...\" if len(test_context) > 100 else f\"Test context: {test_context}\")\n",
    "\n",
    "        # Create and display the JSON payload\n",
    "        test_payload = create_model_selection_request(test_instruction, test_context)\n",
    "        print(\"\\nJSON payload being sent:\")\n",
    "        print(json.dumps(test_payload, indent=2))\n",
    "\n",
    "        # Test the endpoint\n",
    "        result = call_endpoint(ENDPOINT_URL, test_instruction, test_context)\n",
    "\n",
    "        if result['success']:\n",
    "            print(\"\\n✅ Endpoint test successful!\")\n",
    "            print(f\"Status code: {result['status_code']}\")\n",
    "            print(\"API Response preview:\")\n",
    "            response_preview = json.dumps(result['response'], indent=2)[:500]\n",
    "            print(f\"{response_preview}...\")\n",
    "        else:\n",
    "            print(\"\\n❌ Endpoint test failed:\")\n",
    "            print(f\"Status code: {result['status_code']}\")\n",
    "            print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"⚠️ Failed to load dataset for testing.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Endpoint not accessible - skipping test\")\n",
    "    print(\"💡 Make sure to update ENDPOINT_URL in the configuration section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the Dolly dataset\n",
    "print(\"✅ Dataset loading function already defined\")\n",
    "\n",
    "# Load the Dolly dataset\n",
    "print(f\"🔄 Loading {DATASET_NAME} dataset...\")\n",
    "dolly_info = load_and_sample_dataset(\n",
    "    DATASET_NAME,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    dataset_split=DATASET_SPLIT\n",
    ")\n",
    "\n",
    "if 'error' not in dolly_info:\n",
    "    print(\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"📊 Dataset: {dolly_info['dataset_name']}\")\n",
    "    print(f\"📊 Samples loaded: {dolly_info['num_samples']:,}\")\n",
    "    print(f\"⏱️  Load time: {dolly_info['load_time']:.2f}s\")\n",
    "\n",
    "    # Analyze and display dataset structure\n",
    "    analyze_dataset_structure(dolly_info)\n",
    "\n",
    "    # Display sample data\n",
    "    display_sample_data(dolly_info, num_samples=2)\n",
    "else:\n",
    "    print(f\"❌ Failed to load dataset: {dolly_info['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing 1000 samples through endpoint...\n",
      "🚀 Processing 1000 samples through endpoint...\n",
      "📊 Endpoint: https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict\n",
      "Progress: 0/1000 (0.0%) - Cost so far: $0.000000\n",
      "Progress: 50/1000 (5.0%) - Cost so far: $0.001138\n",
      "Progress: 100/1000 (10.0%) - Cost so far: $0.002184\n",
      "Progress: 150/1000 (15.0%) - Cost so far: $0.003079\n",
      "Progress: 200/1000 (20.0%) - Cost so far: $0.004296\n",
      "Progress: 250/1000 (25.0%) - Cost so far: $0.005082\n",
      "Progress: 300/1000 (30.0%) - Cost so far: $0.006039\n",
      "Progress: 350/1000 (35.0%) - Cost so far: $0.007520\n",
      "Progress: 400/1000 (40.0%) - Cost so far: $0.008530\n",
      "Progress: 450/1000 (45.0%) - Cost so far: $0.009337\n",
      "Progress: 500/1000 (50.0%) - Cost so far: $0.010382\n",
      "Progress: 550/1000 (55.0%) - Cost so far: $0.011354\n",
      "Progress: 600/1000 (60.0%) - Cost so far: $0.012514\n",
      "Progress: 650/1000 (65.0%) - Cost so far: $0.013480\n",
      "Progress: 700/1000 (70.0%) - Cost so far: $0.014593\n",
      "Progress: 750/1000 (75.0%) - Cost so far: $0.015431\n",
      "Progress: 800/1000 (80.0%) - Cost so far: $0.016543\n",
      "Progress: 850/1000 (85.0%) - Cost so far: $0.017363\n",
      "Progress: 900/1000 (90.0%) - Cost so far: $0.018317\n",
      "Progress: 950/1000 (95.0%) - Cost so far: $0.019235\n",
      "\n",
      "✅ Processing complete!\n",
      "📊 Results: 996 successful, 4 failed\n",
      "📈 Success rate: 99.6%\n",
      "💰 Total estimated cost: $0.020389 USD\n",
      "💰 Average cost per request: $0.000020 USD\n",
      "💾 Results saved to: dolly_endpoint_results_20250701_223813.csv\n",
      "📋 Summary saved to: dolly_summary_20250701_223813.csv\n",
      "\n",
      "📊 Results Summary:\n",
      "Total samples processed: 1000\n",
      "Successful API calls: 996\n",
      "Failed API calls: 4\n",
      "Total estimated cost: $0.020389\n",
      "\n",
      "📋 Sample Results Preview:\n",
      "   sample_id api_protocol         api_selected_model  api_estimated_cost_usd  \\\n",
      "0          0       minion  Qwen/Qwen2.5-14B-Instruct                0.000002   \n",
      "1          1                                                        0.000000   \n",
      "2          2       minion  Qwen/Qwen2.5-14B-Instruct                0.000109   \n",
      "3          3       minion  Qwen/Qwen2.5-14B-Instruct                0.000004   \n",
      "4          4       minion  Qwen/Qwen2.5-14B-Instruct                0.000003   \n",
      "5          5       minion  Qwen/Qwen2.5-14B-Instruct                0.000008   \n",
      "6          6       minion          google/flan-t5-xl                0.000027   \n",
      "7          7       minion  Qwen/Qwen2.5-14B-Instruct                0.000019   \n",
      "8          8       minion  Qwen/Qwen2.5-14B-Instruct                0.000052   \n",
      "9          9                                                        0.000000   \n",
      "\n",
      "   api_success  response_time_seconds  \n",
      "0         True               0.436237  \n",
      "1        False               8.199031  \n",
      "2         True              14.257940  \n",
      "3         True               0.376712  \n",
      "4         True               0.342828  \n",
      "5         True               0.274033  \n",
      "6         True               7.757959  \n",
      "7         True               0.292118  \n",
      "8         True               0.297420  \n",
      "9        False               8.162640  \n",
      "\n",
      "📈 Model Selection Distribution:\n",
      "api_selected_model\n",
      "Qwen/Qwen2.5-14B-Instruct     887\n",
      "microsoft/deberta-v3-large     61\n",
      "google/flan-t5-xl              46\n",
      "                                6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "💰 Cost by Model:\n",
      "                            count       sum      mean\n",
      "api_selected_model                                   \n",
      "                                6  0.000000  0.000000\n",
      "Qwen/Qwen2.5-14B-Instruct     887  0.018774  0.000021\n",
      "google/flan-t5-xl              46  0.001428  0.000031\n",
      "microsoft/deberta-v3-large     61  0.000187  0.000003\n",
      "\n",
      "📊 Final CSV Columns:\n",
      "Total columns: 16\n",
      "Column list: ['sample_id', 'instruction', 'context', 'full_prompt', 'input_token_estimate', 'output_token_estimate', 'api_success', 'api_status_code', 'api_error', 'response_time_seconds', 'timestamp', 'api_protocol', 'api_selected_model', 'api_selected_provider', 'api_estimated_cost_usd', 'api_full_response']\n"
     ]
    }
   ],
   "source": [
    "# Process samples through endpoint using configuration\n",
    "if endpoint_accessible:\n",
    "    print(f\"🚀 Processing {MAX_SAMPLES_TO_PROCESS} samples through endpoint...\")\n",
    "\n",
    "    # Check if dolly_info is available\n",
    "    if 'dolly_info' in globals() and 'error' not in dolly_info:\n",
    "        # Process samples using configuration values\n",
    "        results_1000 = process_dataset_with_endpoint(\n",
    "            dolly_info,\n",
    "            ENDPOINT_URL,\n",
    "            max_samples=MAX_SAMPLES_TO_PROCESS,\n",
    "            save_csv=SAVE_CSV_RESULTS\n",
    "        )\n",
    "\n",
    "        if not results_1000.empty:\n",
    "            print(\"\\n📊 Results Summary:\")\n",
    "            print(f\"Total samples processed: {len(results_1000)}\")\n",
    "            print(f\"Successful API calls: {results_1000['api_success'].sum()}\")\n",
    "            print(f\"Failed API calls: {(~results_1000['api_success']).sum()}\")\n",
    "            print(f\"Total estimated cost: ${results_1000['api_estimated_cost_usd'].sum():.6f}\")\n",
    "\n",
    "            print(\"\\n📋 Sample Results Preview:\")\n",
    "            preview_cols = ['sample_id', 'api_protocol', 'api_selected_model',\n",
    "                           'api_estimated_cost_usd', 'api_success', 'response_time_seconds']\n",
    "            print(results_1000[preview_cols].head(10))\n",
    "\n",
    "            print(\"\\n📈 Model Selection Distribution:\")\n",
    "            if results_1000['api_selected_model'].notna().any():\n",
    "                model_counts = results_1000['api_selected_model'].value_counts()\n",
    "                print(model_counts.head())\n",
    "\n",
    "            print(\"\\n💰 Cost by Model:\")\n",
    "            if results_1000['api_selected_model'].notna().any():\n",
    "                cost_by_model = results_1000.groupby('api_selected_model')['api_estimated_cost_usd'].agg(['count', 'sum', 'mean'])\n",
    "                print(cost_by_model.head())\n",
    "\n",
    "            print(\"\\n📊 Final CSV Columns:\")\n",
    "            print(f\"Total columns: {len(results_1000.columns)}\")\n",
    "            print(\"Column list:\", list(results_1000.columns))\n",
    "        else:\n",
    "            print(\"❌ No results to process\")\n",
    "    else:\n",
    "        print(\"⚠️ Dataset not loaded yet. Please run the dataset loading cell first.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Endpoint not accessible - cannot process samples\")\n",
    "    print(\"💡 Make sure to update ENDPOINT_URL in the configuration section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive framework for:\n",
    "\n",
    "1. **Dataset Loading**: Efficient loading with sampling and streaming options\n",
    "2. **Structure Analysis**: Understanding dataset schema and statistics\n",
    "3. **Quality Assessment**: Evaluating dataset suitability for instruction tuning\n",
    "4. **Comparative Analysis**: Comparing multiple datasets side by side\n",
    "5. **Visualization**: Creating plots and tables for better understanding\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "- **For Experimentation**: Use `sample_size` parameter to work with smaller subsets\n",
    "- **For Large Datasets**: Use `streaming=True` to avoid memory issues\n",
    "- **For Production**: Consider the load times and implement caching strategies\n",
    "- **For Quality**: Pay attention to empty fields and text length distributions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Extend this notebook to include more datasets\n",
    "2. Add more sophisticated quality metrics\n",
    "3. Implement data preprocessing pipelines\n",
    "4. Create automated benchmarking workflows\n",
    "5. Add model evaluation capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
