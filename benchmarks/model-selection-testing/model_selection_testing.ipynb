{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Protocol Testing\n",
    "\n",
    "Get 100 samples from HuggingFace dataset and test them on adaptive_ai `/predict` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Simple Protocol Testing\n",
      "📊 Getting 100 samples from HuggingFace and testing on /predict\n",
      "✅ Service is running\n",
      "\n",
      "📥 Loading 100 samples from HuggingFace...\n",
      "   📊 Loaded 25/100 samples...\n",
      "   📊 Loaded 50/100 samples...\n",
      "   📊 Loaded 75/100 samples...\n",
      "   📊 Loaded 100/100 samples...\n",
      "✅ Got 100 prompts\n",
      "\n",
      "🧪 Testing 100 prompts on http://localhost:8000/predict...\n",
      "   📊 Tested 20/100 - Success: 6/20 (30.0%)\n",
      "   📊 Tested 40/100 - Success: 14/40 (35.0%)\n",
      "   📊 Tested 60/100 - Success: 22/60 (36.7%)\n",
      "   📊 Tested 80/100 - Success: 26/80 (32.5%)\n",
      "   📊 Tested 100/100 - Success: 31/100 (31.0%)\n",
      "\n",
      "✅ Testing completed!\n",
      "\n",
      "📊 RESULTS SUMMARY\n",
      "✅ Successful: 31/100 (31.0%)\n",
      "❌ Failed: 69/100 (69.0%)\n",
      "⏱️ Average response time: 0.185s\n",
      "\n",
      "🔄 Protocol Usage:\n",
      "   minion: 26 (83.9%)\n",
      "   standard_llm: 5 (16.1%)\n",
      "\n",
      "🤖 Top Models:\n",
      "   microsoft/DialoGPT-medium: 15 (48.4%)\n",
      "   microsoft/codebert-base: 6 (19.4%)\n",
      "   unknown: 5 (16.1%)\n",
      "   allenai/scibert_scivocab_uncased: 3 (9.7%)\n",
      "   gpt2-medium: 2 (6.5%)\n",
      "\n",
      "🏢 Provider Usage:\n",
      "   groq: 26 (83.9%)\n",
      "   unknown: 5 (16.1%)\n",
      "\n",
      "❌ Sample Failures:\n",
      "   3. HTTP 500 - Write a short rhyming poem explaining Einstein's t...\n",
      "   4. HTTP 500 - Replace the words \"desktop, articles, allows, user...\n",
      "   6. HTTP 500 - I am trying to name a consumer product that sorts ...\n",
      "\n",
      "🎉 Protocol testing completed!\n",
      "📈 Dataset: routellm/gpt4_dataset (first 100 samples)\n",
      "🎯 Service: http://localhost:8000/predict\n",
      "🗃️ Results stored in 'df' DataFrame for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Simple Protocol Testing - 100 samples from HuggingFace to /predict\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"🚀 Simple Protocol Testing\")\n",
    "print(\"📊 Getting 100 samples from HuggingFace and testing on /predict\")\n",
    "\n",
    "# Configuration\n",
    "SERVICE_URL = \"http://localhost:8000\"\n",
    "SAMPLE_COUNT = 100\n",
    "\n",
    "# Check service is running\n",
    "try:\n",
    "    response = requests.get(f\"{SERVICE_URL}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Service is running\")\n",
    "    else:\n",
    "        print(f\"❌ Service error: {response.status_code}\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Service not available: {e}\")\n",
    "    print(\"💡 Start service: cd adaptive_ai && uv run python -m adaptive_ai.main\")\n",
    "    exit()\n",
    "\n",
    "# Load dataset and get samples\n",
    "print(f\"\\n📥 Loading {SAMPLE_COUNT} samples from HuggingFace...\")\n",
    "dataset = load_dataset(\"routellm/gpt4_dataset\", split=\"validation\", streaming=True)\n",
    "\n",
    "prompts = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= SAMPLE_COUNT:\n",
    "        break\n",
    "    if item.get(\"prompt\"):\n",
    "        prompts.append(item[\"prompt\"])\n",
    "\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"   📊 Loaded {i + 1}/{SAMPLE_COUNT} samples...\")\n",
    "\n",
    "print(f\"✅ Got {len(prompts)} prompts\")\n",
    "\n",
    "# Test each prompt on /predict\n",
    "print(f\"\\n🧪 Testing {len(prompts)} prompts on {SERVICE_URL}/predict...\")\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    request_data = {\"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{SERVICE_URL}/predict\", json=request_data, timeout=30\n",
    "        )\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            protocol = result.get(\"protocol\", \"unknown\")\n",
    "\n",
    "            # Extract model info\n",
    "            model = \"unknown\"\n",
    "            provider = \"unknown\"\n",
    "\n",
    "            if protocol == \"minion\" and \"minion\" in result:\n",
    "                model = result[\"minion\"].get(\"model\", \"unknown\")\n",
    "                provider = result[\"minion\"].get(\"provider\", \"unknown\")\n",
    "            elif protocol == \"standard\" and \"standard\" in result:\n",
    "                model = result[\"standard\"].get(\"model\", \"unknown\")\n",
    "                provider = result[\"standard\"].get(\"provider\", \"unknown\")\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "                    \"success\": True,\n",
    "                    \"protocol\": protocol,\n",
    "                    \"model\": model,\n",
    "                    \"provider\": provider,\n",
    "                    \"response_time\": execution_time,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "                    \"success\": False,\n",
    "                    \"protocol\": \"failed\",\n",
    "                    \"model\": \"failed\",\n",
    "                    \"provider\": \"failed\",\n",
    "                    \"response_time\": execution_time,\n",
    "                    \"error\": f\"HTTP {response.status_code}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "                \"success\": False,\n",
    "                \"protocol\": \"error\",\n",
    "                \"model\": \"error\",\n",
    "                \"provider\": \"error\",\n",
    "                \"response_time\": execution_time,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Progress update\n",
    "    if (i + 1) % 20 == 0:\n",
    "        successful = sum(1 for r in results if r[\"success\"])\n",
    "        print(\n",
    "            f\"   📊 Tested {i + 1}/{len(prompts)} - Success: {successful}/{i + 1} ({successful/(i+1)*100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "print(\"\\n✅ Testing completed!\")\n",
    "\n",
    "# Results summary\n",
    "df = pd.DataFrame(results)\n",
    "successful = df[df[\"success\"]]\n",
    "failed = df[~df[\"success\"]]\n",
    "\n",
    "print(f\"\\n📊 RESULTS SUMMARY\")\n",
    "print(\n",
    "    f\"✅ Successful: {len(successful)}/{len(df)} ({len(successful)/len(df)*100:.1f}%)\"\n",
    ")\n",
    "print(f\"❌ Failed: {len(failed)}/{len(df)} ({len(failed)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(successful) > 0:\n",
    "    avg_time = successful[\"response_time\"].mean()\n",
    "    print(f\"⏱️ Average response time: {avg_time:.3f}s\")\n",
    "\n",
    "    # Protocol usage\n",
    "    protocol_counts = successful[\"protocol\"].value_counts()\n",
    "    print(f\"\\n🔄 Protocol Usage:\")\n",
    "    for protocol, count in protocol_counts.items():\n",
    "        percentage = count / len(successful) * 100\n",
    "        print(f\"   {protocol}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Model usage\n",
    "    model_counts = successful[\"model\"].value_counts()\n",
    "    print(f\"\\n🤖 Top Models:\")\n",
    "    for model, count in model_counts.head(5).items():\n",
    "        percentage = count / len(successful) * 100\n",
    "        print(f\"   {model}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    # Provider usage\n",
    "    provider_counts = successful[\"provider\"].value_counts()\n",
    "    print(f\"\\n🏢 Provider Usage:\")\n",
    "    for provider, count in provider_counts.items():\n",
    "        percentage = count / len(successful) * 100\n",
    "        print(f\"   {provider}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show some failures if any\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\n❌ Sample Failures:\")\n",
    "    for i, row in failed.head(3).iterrows():\n",
    "        print(f\"   {i+1}. {row['error']} - {row['prompt'][:50]}...\")\n",
    "\n",
    "print(f\"\\n🎉 Protocol testing completed!\")\n",
    "print(f\"📈 Dataset: routellm/gpt4_dataset (first {SAMPLE_COUNT} samples)\")\n",
    "print(f\"🎯 Service: {SERVICE_URL}/predict\")\n",
    "print(f\"🗃️ Results stored in 'df' DataFrame for further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUG MODE - Finding HTTP 500 causes\n",
      "📊 Testing with detailed error reporting\n",
      "✅ Service is running\n",
      "📋 Health response: ok\n",
      "\n",
      "📥 Loading 10 samples from HuggingFace...\n",
      "✅ Got 10 prompts\n",
      "\n",
      "🔍 DEBUGGING 10 prompts with detailed error analysis...\n",
      "\n",
      "🧪 Testing prompt 1/10:\n",
      "📝 Prompt length: 60 chars\n",
      "📝 Prompt preview: Write c++ code, which calculates and outputs n digits of pi....\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=False\n",
      "📡 Request size: 107 bytes\n",
      "⏱️ Response time: 0.670s\n",
      "📊 Status code: 200\n",
      "✅ SUCCESS - Protocol: standard_llm\n",
      "🤖 Model: unknown\n",
      "🏢 Provider: unknown\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 2/10:\n",
      "📝 Prompt length: 957 chars\n",
      "📝 Prompt preview: [Partner Cooperation Team] Share webinar schedule for Cafe24 employees\n",
      "hello. Th...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=True, Quotes=True\n",
      "📡 Request size: 1017 bytes\n",
      "⏱️ Response time: 0.629s\n",
      "📊 Status code: 200\n",
      "✅ SUCCESS - Protocol: minion\n",
      "🤖 Model: gpt2-medium\n",
      "🏢 Provider: groq\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 3/10:\n",
      "📝 Prompt length: 104 chars\n",
      "📝 Prompt preview: Write a short rhyming poem explaining Einstein's theory of general relativity in...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=True\n",
      "📡 Request size: 151 bytes\n",
      "⏱️ Response time: 0.600s\n",
      "📊 Status code: 500\n",
      "❌ HTTP ERROR 500\n",
      "📋 Error response: {\"detail\":\"Internal server error\"}...\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 4/10:\n",
      "📝 Prompt length: 268 chars\n",
      "📝 Prompt preview: Replace the words \"desktop, articles, allows, user\" in following sentence, don't...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=True\n",
      "📡 Request size: 317 bytes\n",
      "⏱️ Response time: 0.253s\n",
      "📊 Status code: 500\n",
      "❌ HTTP ERROR 500\n",
      "📋 Error response: {\"detail\":\"Internal server error\"}...\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 5/10:\n",
      "📝 Prompt length: 47 chars\n",
      "📝 Prompt preview: What is the goal of creating quantum computers?...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=False\n",
      "📡 Request size: 94 bytes\n",
      "⏱️ Response time: 0.219s\n",
      "📊 Status code: 200\n",
      "✅ SUCCESS - Protocol: minion\n",
      "🤖 Model: microsoft/DialoGPT-medium\n",
      "🏢 Provider: groq\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 6/10:\n",
      "📝 Prompt length: 396 chars\n",
      "📝 Prompt preview: I am trying to name a consumer product that sorts collectible cards.  I would li...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=True\n",
      "📡 Request size: 449 bytes\n",
      "⏱️ Response time: 0.290s\n",
      "📊 Status code: 500\n",
      "❌ HTTP ERROR 500\n",
      "📋 Error response: {\"detail\":\"Internal server error\"}...\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 7/10:\n",
      "📝 Prompt length: 26 chars\n",
      "📝 Prompt preview: append new row to datframe...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=False\n",
      "📡 Request size: 73 bytes\n",
      "⏱️ Response time: 0.215s\n",
      "📊 Status code: 200\n",
      "✅ SUCCESS - Protocol: minion\n",
      "🤖 Model: microsoft/codebert-base\n",
      "🏢 Provider: groq\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 8/10:\n",
      "📝 Prompt length: 89 chars\n",
      "📝 Prompt preview: What's the word used to describe the thing instructors use to evaluate students'...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=True\n",
      "📡 Request size: 136 bytes\n",
      "⏱️ Response time: 0.221s\n",
      "📊 Status code: 500\n",
      "❌ HTTP ERROR 500\n",
      "📋 Error response: {\"detail\":\"Internal server error\"}...\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 9/10:\n",
      "📝 Prompt length: 1416 chars\n",
      "📝 Prompt preview: In this task, you will be presented with a context from an academic paper and a ...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=True, Quotes=True\n",
      "📡 Request size: 1484 bytes\n",
      "⏱️ Response time: 0.695s\n",
      "📊 Status code: 500\n",
      "❌ HTTP ERROR 500\n",
      "📋 Error response: {\"detail\":\"Internal server error\"}...\n",
      "============================================================\n",
      "\n",
      "🧪 Testing prompt 10/10:\n",
      "📝 Prompt length: 65 chars\n",
      "📝 Prompt preview: How many batches are normal when training a deep learning method?...\n",
      "📊 Prompt analysis: Unicode=False, Newlines=False, Quotes=False\n",
      "📡 Request size: 112 bytes\n",
      "⏱️ Response time: 0.230s\n",
      "📊 Status code: 500\n",
      "❌ HTTP ERROR 500\n",
      "📋 Error response: {\"detail\":\"Internal server error\"}...\n",
      "============================================================\n",
      "\n",
      "🔍 DEBUGGING COMPLETED!\n",
      "\n",
      "📊 DETAILED DEBUG RESULTS\n",
      "✅ Successful: 4/10 (40.0%)\n",
      "❌ Failed: 6/10 (60.0%)\n",
      "\n",
      "❌ FIRST 3 FAILURES WITH DETAILS:\n",
      "\n",
      "   Failure 3:\n",
      "   📊 Length: 104 chars\n",
      "   🔢 Status: 500\n",
      "   ❌ Error: HTTP 500\n",
      "   📋 Details: {'detail': 'Internal server error'}...\n",
      "\n",
      "   Failure 4:\n",
      "   📊 Length: 268 chars\n",
      "   🔢 Status: 500\n",
      "   ❌ Error: HTTP 500\n",
      "   📋 Details: {'detail': 'Internal server error'}...\n",
      "\n",
      "   Failure 6:\n",
      "   📊 Length: 396 chars\n",
      "   🔢 Status: 500\n",
      "   ❌ Error: HTTP 500\n",
      "   📋 Details: {'detail': 'Internal server error'}...\n",
      "\n",
      "🎯 DEBUG COMPLETE - Check server logs for more details!\n",
      "🗃️ Debug data available in 'df_debug' DataFrame\n"
     ]
    }
   ],
   "source": [
    "# DEBUG VERSION - Find HTTP 500 causes\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "print(\"🔍 DEBUG MODE - Finding HTTP 500 causes\")\n",
    "print(\"📊 Testing with detailed error reporting\")\n",
    "\n",
    "# Configuration\n",
    "SERVICE_URL = \"http://localhost:8000\"\n",
    "SAMPLE_COUNT = 10  # Small sample for debugging\n",
    "\n",
    "# Check service is running\n",
    "try:\n",
    "    response = requests.get(f\"{SERVICE_URL}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Service is running\")\n",
    "        print(f\"📋 Health response: {response.text}\")\n",
    "    else:\n",
    "        print(f\"❌ Service error: {response.status_code}\")\n",
    "        print(f\"📋 Response: {response.text}\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Service not available: {e}\")\n",
    "    print(\"💡 Start service: cd adaptive_ai && uv run python -m adaptive_ai.main\")\n",
    "    exit()\n",
    "\n",
    "# Load dataset and get samples\n",
    "print(f\"\\n📥 Loading {SAMPLE_COUNT} samples from HuggingFace...\")\n",
    "dataset = load_dataset(\"routellm/gpt4_dataset\", split=\"validation\", streaming=True)\n",
    "\n",
    "prompts = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= SAMPLE_COUNT:\n",
    "        break\n",
    "    if item.get(\"prompt\"):\n",
    "        prompts.append(item[\"prompt\"])\n",
    "\n",
    "print(f\"✅ Got {len(prompts)} prompts\")\n",
    "\n",
    "# Test each prompt with detailed debugging\n",
    "print(f\"\\n🔍 DEBUGGING {len(prompts)} prompts with detailed error analysis...\")\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\n🧪 Testing prompt {i+1}/{len(prompts)}:\")\n",
    "    print(f\"📝 Prompt length: {len(prompt)} chars\")\n",
    "    print(f\"📝 Prompt preview: {prompt[:80]}...\")\n",
    "\n",
    "    # Check for potential problematic characters\n",
    "    has_unicode = any(ord(char) > 127 for char in prompt)\n",
    "    has_newlines = \"\\n\" in prompt\n",
    "    has_quotes = '\"' in prompt or \"'\" in prompt\n",
    "\n",
    "    print(\n",
    "        f\"📊 Prompt analysis: Unicode={has_unicode}, Newlines={has_newlines}, Quotes={has_quotes}\"\n",
    "    )\n",
    "\n",
    "    request_data = {\"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "\n",
    "    print(f\"📡 Request size: {len(json.dumps(request_data))} bytes\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{SERVICE_URL}/predict\",\n",
    "            json=request_data,\n",
    "            timeout=30,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "        )\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"⏱️ Response time: {execution_time:.3f}s\")\n",
    "        print(f\"📊 Status code: {response.status_code}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                result = response.json()\n",
    "                protocol = result.get(\"protocol\", \"unknown\")\n",
    "                print(f\"✅ SUCCESS - Protocol: {protocol}\")\n",
    "\n",
    "                # Extract model info\n",
    "                model = \"unknown\"\n",
    "                provider = \"unknown\"\n",
    "\n",
    "                if protocol == \"minion\" and \"minion\" in result:\n",
    "                    model = result[\"minion\"].get(\"model\", \"unknown\")\n",
    "                    provider = result[\"minion\"].get(\"provider\", \"unknown\")\n",
    "                elif protocol == \"standard\" and \"standard\" in result:\n",
    "                    model = result[\"standard\"].get(\"model\", \"unknown\")\n",
    "                    provider = result[\"standard\"].get(\"provider\", \"unknown\")\n",
    "\n",
    "                print(f\"🤖 Model: {model}\")\n",
    "                print(f\"🏢 Provider: {provider}\")\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"prompt_id\": i + 1,\n",
    "                        \"success\": True,\n",
    "                        \"protocol\": protocol,\n",
    "                        \"model\": model,\n",
    "                        \"provider\": provider,\n",
    "                        \"response_time\": execution_time,\n",
    "                        \"status_code\": response.status_code,\n",
    "                        \"prompt_length\": len(prompt),\n",
    "                        \"has_unicode\": has_unicode,\n",
    "                        \"has_newlines\": has_newlines,\n",
    "                        \"has_quotes\": has_quotes,\n",
    "                    }\n",
    "                )\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"❌ JSON decode error: {e}\")\n",
    "                print(f\"📋 Raw response: {response.text[:200]}...\")\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"prompt_id\": i + 1,\n",
    "                        \"success\": False,\n",
    "                        \"error\": f\"JSON decode error: {e}\",\n",
    "                        \"status_code\": response.status_code,\n",
    "                        \"response_time\": execution_time,\n",
    "                        \"prompt_length\": len(prompt),\n",
    "                        \"raw_response\": response.text[:500],\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            print(f\"❌ HTTP ERROR {response.status_code}\")\n",
    "            print(f\"📋 Error response: {response.text[:200]}...\")\n",
    "\n",
    "            # Try to get more details from the error response\n",
    "            error_details = \"Unknown error\"\n",
    "            try:\n",
    "                error_json = response.json()\n",
    "                error_details = str(error_json)\n",
    "            except:\n",
    "                error_details = response.text[:200]\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"prompt_id\": i + 1,\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"HTTP {response.status_code}\",\n",
    "                    \"error_details\": error_details,\n",
    "                    \"status_code\": response.status_code,\n",
    "                    \"response_time\": execution_time,\n",
    "                    \"prompt_length\": len(prompt),\n",
    "                    \"has_unicode\": has_unicode,\n",
    "                    \"has_newlines\": has_newlines,\n",
    "                    \"has_quotes\": has_quotes,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"❌ EXCEPTION: {e}\")\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": i + 1,\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Exception: {e}\",\n",
    "                \"response_time\": execution_time,\n",
    "                \"prompt_length\": len(prompt),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n🔍 DEBUGGING COMPLETED!\")\n",
    "\n",
    "# Detailed analysis\n",
    "df_debug = pd.DataFrame(results)\n",
    "successful = df_debug[df_debug[\"success\"]]\n",
    "failed = df_debug[~df_debug[\"success\"]]\n",
    "\n",
    "print(f\"\\n📊 DETAILED DEBUG RESULTS\")\n",
    "print(\n",
    "    f\"✅ Successful: {len(successful)}/{len(df_debug)} ({len(successful)/len(df_debug)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"❌ Failed: {len(failed)}/{len(df_debug)} ({len(failed)/len(df_debug)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\n❌ FIRST 3 FAILURES WITH DETAILS:\")\n",
    "    for i, row in failed.head(3).iterrows():\n",
    "        print(f\"\\n   Failure {row['prompt_id']}:\")\n",
    "        print(f\"   📊 Length: {row['prompt_length']} chars\")\n",
    "        print(f\"   🔢 Status: {row['status_code']}\")\n",
    "        print(f\"   ❌ Error: {row['error']}\")\n",
    "        if \"error_details\" in row and pd.notna(row[\"error_details\"]):\n",
    "            print(f\"   📋 Details: {str(row['error_details'])[:150]}...\")\n",
    "\n",
    "print(f\"\\n🎯 DEBUG COMPLETE - Check server logs for more details!\")\n",
    "print(f\"🗃️ Debug data available in 'df_debug' DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Benchmark-Optimized Task Rankings\n",
      "📊 Testing different task types to see model selection\n",
      "======================================================================\n",
      "\n",
      "[1/7] Testing BRAINSTORMING...\n",
      "Prompt: Generate 5 creative ideas for a sustainable transportation startup that could re...\n",
      "❌ Error: HTTP 500: {\"detail\":\"Internal server error\"}\n",
      "\n",
      "[2/7] Testing OPEN_QA...\n",
      "Prompt: What are the main causes of climate change and how do they interact with each ot...\n",
      "✅ Task: unknown | Protocol: minion | Model: unknown:unknown\n",
      "⚠️  Expected OPEN_QA, got unknown\n",
      "🌍 Domain: unknown\n",
      "\n",
      "[3/7] Testing CODE_GENERATION...\n",
      "Prompt: Write a Python function that implements a binary search algorithm with proper er...\n",
      "✅ Task: unknown | Protocol: minion | Model: unknown:unknown\n",
      "⚠️  Expected CODE_GENERATION, got unknown\n",
      "🌍 Domain: unknown\n",
      "\n",
      "[4/7] Testing CLOSED_QA...\n",
      "Prompt: Based on the context of renewable energy, what is the efficiency of modern solar...\n",
      "✅ Task: unknown | Protocol: minion | Model: unknown:unknown\n",
      "⚠️  Expected CLOSED_QA, got unknown\n",
      "🌍 Domain: unknown\n",
      "\n",
      "[5/7] Testing TEXT_GENERATION...\n",
      "Prompt: Write a compelling product description for a smart home device that learns user ...\n",
      "✅ Task: unknown | Protocol: standard_llm | Model: unknown:unknown\n",
      "⚠️  Expected TEXT_GENERATION, got unknown\n",
      "🌍 Domain: unknown\n",
      "\n",
      "[6/7] Testing CLASSIFICATION...\n",
      "Prompt: Classify this email as spam or not spam: 'Congratulations! You've won $1000000! ...\n",
      "✅ Task: unknown | Protocol: minion | Model: unknown:unknown\n",
      "⚠️  Expected CLASSIFICATION, got unknown\n",
      "🌍 Domain: unknown\n",
      "\n",
      "[7/7] Testing SUMMARIZATION...\n",
      "Prompt: Summarize the key points from this article about artificial intelligence develop...\n",
      "✅ Task: unknown | Protocol: minion | Model: unknown:unknown\n",
      "⚠️  Expected SUMMARIZATION, got unknown\n",
      "🌍 Domain: unknown\n",
      "\n",
      "======================================================================\n",
      "📊 BENCHMARK-OPTIMIZED RANKING RESULTS\n",
      "======================================================================\n",
      "✅ Successful tests: 6/7\n",
      "🎯 Task classification accuracy: 0/6 (0.0%)\n",
      "\n",
      "📈 Protocol usage:\n",
      "  minion: 5\n",
      "  standard_llm: 1\n",
      "\n",
      "🤖 Model usage:\n",
      "  unknown:unknown: 6\n",
      "\n",
      "🎯 Task → Model assignments (verifying benchmark optimization):\n",
      "  unknown → unknown:unknown: 6\n",
      "\n",
      "🏆 Benchmark optimization verification:\n",
      "\n",
      "❌ Failed tests: 1\n",
      "  BRAINSTORMING: HTTP 500: {\"detail\":\"Internal server error\"}\n",
      "\n",
      "🎉 Benchmark-optimized ranking test completed!\n",
      "📈 Rankings updated based on MMLU-Pro, HumanEval, GSM8K, GPQA, TruthfulQA benchmarks\n",
      "🗃️ Results stored in 'results' list for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Test Different Prompt Types - Benchmark-Optimized Rankings\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"🧪 Testing Benchmark-Optimized Task Rankings\")\n",
    "print(\"📊 Testing different task types to see model selection\")\n",
    "\n",
    "# Test endpoint\n",
    "url = \"http://localhost:8000/predict\"\n",
    "\n",
    "# Test prompts for different task types\n",
    "test_prompts = [\n",
    "    {\n",
    "        \"name\": \"BRAINSTORMING\",\n",
    "        \"prompt\": \"Generate 5 creative ideas for a sustainable transportation startup that could revolutionize urban mobility\",\n",
    "        \"expected_task\": \"BRAINSTORMING\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"OPEN_QA\",\n",
    "        \"prompt\": \"What are the main causes of climate change and how do they interact with each other?\",\n",
    "        \"expected_task\": \"OPEN_QA\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CODE_GENERATION\",\n",
    "        \"prompt\": \"Write a Python function that implements a binary search algorithm with proper error handling\",\n",
    "        \"expected_task\": \"CODE_GENERATION\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CLOSED_QA\",\n",
    "        \"prompt\": \"Based on the context of renewable energy, what is the efficiency of modern solar panels?\",\n",
    "        \"expected_task\": \"CLOSED_QA\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TEXT_GENERATION\",\n",
    "        \"prompt\": \"Write a compelling product description for a smart home device that learns user preferences\",\n",
    "        \"expected_task\": \"TEXT_GENERATION\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CLASSIFICATION\",\n",
    "        \"prompt\": \"Classify this email as spam or not spam: 'Congratulations! You've won $1000000! Click here to claim now!'\",\n",
    "        \"expected_task\": \"CLASSIFICATION\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SUMMARIZATION\",\n",
    "        \"prompt\": \"Summarize the key points from this article about artificial intelligence developments in 2024: AI has made significant advances in reasoning capabilities, with models like GPT-4o and Claude showing improved performance on complex tasks.\",\n",
    "        \"expected_task\": \"SUMMARIZATION\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def test_prompt(prompt_data):\n",
    "    \"\"\"Test a single prompt and return results\"\"\"\n",
    "    payload = {\"messages\": [{\"role\": \"user\", \"content\": prompt_data[\"prompt\"]}]}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"prompt_name\": prompt_data[\"name\"],\n",
    "                \"expected_task\": prompt_data[\"expected_task\"],\n",
    "                \"actual_task\": (\n",
    "                    result.get(\"task_classification\", {}).get(\n",
    "                        \"task_type_1\", [\"unknown\"]\n",
    "                    )[0]\n",
    "                    if result.get(\"task_classification\")\n",
    "                    else \"unknown\"\n",
    "                ),\n",
    "                \"protocol\": result.get(\"protocol\", \"unknown\"),\n",
    "                \"provider\": result.get(\"provider\", \"unknown\"),\n",
    "                \"model\": result.get(\"model\", \"unknown\"),\n",
    "                \"domain\": (\n",
    "                    result.get(\"domain_classification\", {}).get(\"domain\", \"unknown\")\n",
    "                    if result.get(\"domain_classification\")\n",
    "                    else \"unknown\"\n",
    "                ),\n",
    "                \"full_response\": result,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"prompt_name\": prompt_data[\"name\"],\n",
    "                \"error\": f\"HTTP {response.status_code}: {response.text}\",\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"prompt_name\": prompt_data[\"name\"], \"error\": str(e)}\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "for i, prompt_data in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n[{i}/{len(test_prompts)}] Testing {prompt_data['name']}...\")\n",
    "    print(f\"Prompt: {prompt_data['prompt'][:80]}...\")\n",
    "\n",
    "    result = test_prompt(prompt_data)\n",
    "    results.append(result)\n",
    "\n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(\n",
    "            f\"✅ Task: {result['actual_task']} | Protocol: {result['protocol']} | Model: {result['provider']}:{result['model']}\"\n",
    "        )\n",
    "        if result[\"expected_task\"] != result[\"actual_task\"]:\n",
    "            print(f\"⚠️  Expected {result['expected_task']}, got {result['actual_task']}\")\n",
    "        print(f\"🌍 Domain: {result['domain']}\")\n",
    "    else:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 BENCHMARK-OPTIMIZED RANKING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "successful_tests = [r for r in results if r[\"status\"] == \"success\"]\n",
    "\n",
    "if successful_tests:\n",
    "    print(f\"✅ Successful tests: {len(successful_tests)}/{len(test_prompts)}\")\n",
    "\n",
    "    # Task classification accuracy\n",
    "    correct_classifications = sum(\n",
    "        1 for r in successful_tests if r[\"expected_task\"] == r[\"actual_task\"]\n",
    "    )\n",
    "    print(\n",
    "        f\"🎯 Task classification accuracy: {correct_classifications}/{len(successful_tests)} ({correct_classifications/len(successful_tests)*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # Model selection distribution\n",
    "    model_usage = {}\n",
    "    protocol_usage = {}\n",
    "    task_model_combinations = {}\n",
    "\n",
    "    for r in successful_tests:\n",
    "        model_key = f\"{r['provider']}:{r['model']}\"\n",
    "        model_usage[model_key] = model_usage.get(model_key, 0) + 1\n",
    "        protocol_usage[r[\"protocol\"]] = protocol_usage.get(r[\"protocol\"], 0) + 1\n",
    "\n",
    "        # Track task-model combinations to verify benchmark optimization\n",
    "        task_model_key = f\"{r['actual_task']} → {model_key}\"\n",
    "        task_model_combinations[task_model_key] = (\n",
    "            task_model_combinations.get(task_model_key, 0) + 1\n",
    "        )\n",
    "\n",
    "    print(f\"\\n📈 Protocol usage:\")\n",
    "    for protocol, count in protocol_usage.items():\n",
    "        print(f\"  {protocol}: {count}\")\n",
    "\n",
    "    print(f\"\\n🤖 Model usage:\")\n",
    "    for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {model}: {count}\")\n",
    "\n",
    "    print(f\"\\n🎯 Task → Model assignments (verifying benchmark optimization):\")\n",
    "    for task_model, count in sorted(task_model_combinations.items()):\n",
    "        print(f\"  {task_model}: {count}\")\n",
    "\n",
    "    # Show specific benchmark-optimized selections\n",
    "    print(f\"\\n🏆 Benchmark optimization verification:\")\n",
    "    for r in successful_tests:\n",
    "        task = r[\"actual_task\"]\n",
    "        model = r[\"model\"]\n",
    "        if task == \"BRAINSTORMING\" and (\"o1\" in model or \"reasoner\" in model):\n",
    "            print(f\"  ✅ {task}: Using reasoning model {model} (benchmark-optimized)\")\n",
    "        elif task == \"OPEN_QA\" and \"grok\" in model.lower():\n",
    "            print(f\"  ✅ {task}: Using Grok {model} (92.7% MMLU leader)\")\n",
    "        elif task == \"CODE_GENERATION\" and \"deepseek\" in model.lower():\n",
    "            print(f\"  ✅ {task}: Using DeepSeek {model} (HumanEval leader)\")\n",
    "        elif task in [\"CLASSIFICATION\", \"SUMMARIZATION\"] and \"mini\" in model.lower():\n",
    "            print(f\"  ✅ {task}: Using efficient model {model} (cost-optimized)\")\n",
    "else:\n",
    "    print(\"❌ No successful tests\")\n",
    "\n",
    "# Show any errors\n",
    "error_tests = [r for r in results if r[\"status\"] == \"error\"]\n",
    "if error_tests:\n",
    "    print(f\"\\n❌ Failed tests: {len(error_tests)}\")\n",
    "    for r in error_tests:\n",
    "        print(f\"  {r['prompt_name']}: {r['error']}\")\n",
    "\n",
    "print(f\"\\n🎉 Benchmark-optimized ranking test completed!\")\n",
    "print(\n",
    "    f\"📈 Rankings updated based on MMLU-Pro, HumanEval, GSM8K, GPQA, TruthfulQA benchmarks\"\n",
    ")\n",
    "print(f\"🗃️ Results stored in 'results' list for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
