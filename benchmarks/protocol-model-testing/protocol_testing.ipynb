{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ADAPTIVE AI SERVICE PROTOCOL TESTING\n",
      "==================================================\n",
      "📡 Testing connection to http://localhost:8000...\n",
      "✅ Service is healthy and ready!\n",
      "🌐 Streaming routellm/gpt4_dataset from HuggingFace...\n",
      "✅ Dataset stream initialized successfully!\n",
      "📥 Collecting 10000 samples from stream...\n",
      "✅ Collected 10000 samples\n",
      "\n",
      "🧪 TESTING SERVICE WITH REAL PROMPTS\n",
      "========================================\n",
      "📊 Processed 10/10000 requests\n",
      "📊 Processed 20/10000 requests\n",
      "📊 Processed 30/10000 requests\n",
      "📊 Processed 40/10000 requests\n",
      "📊 Processed 50/10000 requests\n",
      "📊 Processed 60/10000 requests\n",
      "📊 Processed 70/10000 requests\n",
      "📊 Processed 80/10000 requests\n",
      "📊 Processed 90/10000 requests\n",
      "📊 Processed 100/10000 requests\n",
      "📊 Processed 110/10000 requests\n",
      "📊 Processed 120/10000 requests\n",
      "📊 Processed 130/10000 requests\n",
      "📊 Processed 140/10000 requests\n",
      "📊 Processed 150/10000 requests\n",
      "📊 Processed 160/10000 requests\n",
      "📊 Processed 170/10000 requests\n",
      "📊 Processed 180/10000 requests\n",
      "📊 Processed 190/10000 requests\n",
      "📊 Processed 200/10000 requests\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSERVICE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/predict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m         json\u001b[38;5;241m=\u001b[39mrequest_data,\n\u001b[1;32m     96\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complete Protocol Testing for Adaptive AI Service\n",
    "# ================================================\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 ADAPTIVE AI SERVICE PROTOCOL TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuration\n",
    "SERVICE_URL = \"http://localhost:8000\"\n",
    "SAMPLE_SIZE = 10000  # Reasonable sample size for testing\n",
    "DATASET_NAME = \"routellm/gpt4_dataset\"\n",
    "\n",
    "# Test service connection\n",
    "def test_service():\n",
    "    try:\n",
    "        response = requests.get(f\"{SERVICE_URL}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(f\"📡 Testing connection to {SERVICE_URL}...\")\n",
    "if not test_service():\n",
    "    print(\"❌ Service is not available!\")\n",
    "    print(\"💡 Please start the service with: uv run python -m adaptive_ai.main\")\n",
    "    exit()\n",
    "\n",
    "print(\"✅ Service is healthy and ready!\")\n",
    "\n",
    "# Load dataset from HuggingFace (streaming)\n",
    "print(f\"🌐 Streaming {DATASET_NAME} from HuggingFace...\")\n",
    "try:\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"validation\", streaming=True)\n",
    "    print(\"✅ Dataset stream initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Collect sample data\n",
    "print(f\"📥 Collecting {SAMPLE_SIZE} samples from stream...\")\n",
    "sample_data = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= SAMPLE_SIZE:\n",
    "        break\n",
    "    sample_data.append(item)\n",
    "\n",
    "print(f\"✅ Collected {len(sample_data)} samples\")\n",
    "\n",
    "# Test the service with real prompts\n",
    "print(\"\\n🧪 TESTING SERVICE WITH REAL PROMPTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "results = []\n",
    "successful_tests = 0\n",
    "total_tests = len(sample_data)\n",
    "\n",
    "for i, item in enumerate(sample_data):\n",
    "    prompt = item.get('prompt', '')\n",
    "    if not prompt:\n",
    "        continue\n",
    "    \n",
    "    # Create proper message format for the service\n",
    "    request_data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{SERVICE_URL}/predict\",\n",
    "            json=request_data,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            \n",
    "            # Extract information from response\n",
    "            protocol = result.get('protocol', 'unknown')\n",
    "            selected_model = 'unknown'\n",
    "            provider = 'unknown'\n",
    "            \n",
    "            # Extract model and provider based on protocol\n",
    "            if protocol == 'minion' and 'minion' in result:\n",
    "                selected_model = result['minion'].get('model', 'unknown')\n",
    "                provider = result['minion'].get('provider', 'unknown')\n",
    "            elif protocol == 'standard' and 'standard' in result:\n",
    "                selected_model = result['standard'].get('model', 'unknown')\n",
    "                provider = result['standard'].get('provider', 'unknown')\n",
    "            \n",
    "            results.append({\n",
    "                'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "                'protocol': protocol,\n",
    "                'selected_model': selected_model,\n",
    "                'provider': provider,\n",
    "                'execution_time': execution_time,\n",
    "                'success': True,\n",
    "                'response': result\n",
    "            })\n",
    "            \n",
    "            successful_tests += 1\n",
    "            \n",
    "        else:\n",
    "            results.append({\n",
    "                'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "                'protocol': 'unknown',\n",
    "                'selected_model': 'unknown',\n",
    "                'provider': 'unknown',\n",
    "                'execution_time': execution_time,\n",
    "                'success': False,\n",
    "                'error': f\"HTTP {response.status_code}: {response.text}\"\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        results.append({\n",
    "            'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "            'protocol': 'unknown',\n",
    "            'selected_model': 'unknown',\n",
    "            'provider': 'unknown',\n",
    "            'execution_time': execution_time,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Progress update\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"📊 Processed {i + 1}/{total_tests} requests\")\n",
    "\n",
    "print(f\"\\n✅ Testing completed!\")\n",
    "print(f\"📈 Total tests: {total_tests}\")\n",
    "print(f\"✅ Successful tests: {successful_tests}\")\n",
    "print(f\"📊 Success rate: {successful_tests/total_tests:.2%}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "successful_df = df[df['success'] == True]\n",
    "\n",
    "print(f\"\\n📊 PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if not successful_df.empty:\n",
    "    avg_time = successful_df['execution_time'].mean()\n",
    "    print(f\"⏱️  Average response time: {avg_time:.4f}s\")\n",
    "    print(f\"📈 Min response time: {successful_df['execution_time'].min():.4f}s\")\n",
    "    print(f\"📈 Max response time: {successful_df['execution_time'].max():.4f}s\")\n",
    "    \n",
    "    # Protocol distribution\n",
    "    protocol_counts = successful_df['protocol'].value_counts()\n",
    "    print(f\"\\n🔄 Protocol Usage:\")\n",
    "    for protocol, count in protocol_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        print(f\"  {protocol}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Model distribution\n",
    "    model_counts = successful_df['selected_model'].value_counts()\n",
    "    print(f\"\\n🤖 Model Usage:\")\n",
    "    for model, count in model_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        print(f\"  {model}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Provider distribution\n",
    "    provider_counts = successful_df['provider'].value_counts()\n",
    "    print(f\"\\n🏢 Provider Usage:\")\n",
    "    for provider, count in provider_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        print(f\"  {provider}: {count} uses ({percentage:.1f}%)\")\n",
    "\n",
    "# Show some example responses\n",
    "print(f\"\\n📝 EXAMPLE RESPONSES\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "for i, result in enumerate(successful_df.head(3).to_dict('records')):\n",
    "    print(f\"\\n📋 Example {i+1}:\")\n",
    "    print(f\"  💬 Prompt: {result['prompt']}\")\n",
    "    print(f\"  🔄 Protocol: {result['protocol']}\")\n",
    "    print(f\"  🤖 Model: {result['selected_model']}\")\n",
    "    print(f\"  🏢 Provider: {result['provider']}\")\n",
    "    print(f\"  ⏱️ Time: {result['execution_time']:.4f}s\")\n",
    "\n",
    "# Create visualizations\n",
    "if not successful_df.empty:\n",
    "    print(f\"\\n📊 GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Response time distribution\n",
    "    ax1.hist(successful_df['execution_time'], bins=15, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "    ax1.set_title('Response Time Distribution', fontweight='bold')\n",
    "    ax1.set_xlabel('Response Time (seconds)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.axvline(avg_time, color='red', linestyle='--', label=f'Mean: {avg_time:.4f}s')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Success rate pie chart\n",
    "    success_counts = [successful_tests, total_tests - successful_tests]\n",
    "    labels = ['Successful', 'Failed']\n",
    "    colors = ['#96CEB4', '#FF6B6B']\n",
    "    ax2.pie(success_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Success Rate Distribution', fontweight='bold')\n",
    "    \n",
    "    # 3. Protocol usage\n",
    "    protocol_counts.plot(kind='bar', ax=ax3, color='#45B7D1', alpha=0.7)\n",
    "    ax3.set_title('Protocol Usage Distribution', fontweight='bold')\n",
    "    ax3.set_xlabel('Protocol')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Model usage\n",
    "    model_counts.plot(kind='bar', ax=ax4, color='#FFEAA7', alpha=0.7)\n",
    "    ax4.set_title('Model Usage Distribution', fontweight='bold')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Visualizations generated successfully!\")\n",
    "\n",
    "# Save results\n",
    "print(f\"\\n💾 SAVING RESULTS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrame\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = f\"{results_dir}/adaptive_ai_test_results_{timestamp}.csv\"\n",
    "df.to_csv(results_file, index=False)\n",
    "print(f\"📊 Results saved to: {results_file}\")\n",
    "\n",
    "# Generate and save report\n",
    "report = f\"\"\"Adaptive AI Service Testing Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Service URL: {SERVICE_URL}\n",
    "Dataset: {DATASET_NAME} (streamed from HuggingFace)\n",
    "Sample Size: {SAMPLE_SIZE}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "==================\n",
    "Total Tests: {total_tests}\n",
    "Successful Tests: {successful_tests}\n",
    "Success Rate: {successful_tests/total_tests:.2%}\n",
    "Average Response Time: {avg_time:.4f}s\n",
    "\n",
    "PROTOCOL USAGE:\n",
    "===============\n",
    "\"\"\"\n",
    "\n",
    "if not successful_df.empty:\n",
    "    for protocol, count in protocol_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        report += f\"{protocol}: {count} uses ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    report += \"\\nMODEL USAGE:\\n============\\n\"\n",
    "    for model, count in model_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        report += f\"{model}: {count} uses ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    report += \"\\nPROVIDER USAGE:\\n===============\\n\"\n",
    "    for provider, count in provider_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        report += f\"{provider}: {count} uses ({percentage:.1f}%)\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "TECHNICAL DETAILS:\n",
    "==================\n",
    "- Dataset streamed directly from HuggingFace\n",
    "- No local files created for dataset\n",
    "- Real-time testing of adaptive_ai service\n",
    "- HTTP requests to /predict endpoint\n",
    "- Service running on port 8000\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "================\n",
    "1. Monitor response times under load\n",
    "2. Implement service health monitoring  \n",
    "3. Add request queuing for high traffic\n",
    "4. Track model performance over time\n",
    "5. Consider load balancing for scaling\n",
    "\"\"\"\n",
    "\n",
    "report_file = f\"{results_dir}/adaptive_ai_test_report_{timestamp}.txt\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"📄 Report saved to: {report_file}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n🎯 TESTING SUMMARY\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"✅ Service tested successfully with {SAMPLE_SIZE} real prompts\")\n",
    "print(f\"📊 Success rate: {successful_tests/total_tests:.2%}\")\n",
    "print(f\"⏱️ Average response time: {avg_time:.4f}s\")\n",
    "print(f\"🌐 Data streamed directly from HuggingFace\")\n",
    "print(f\"🗑️ No local dataset files created\")\n",
    "print(f\"🚀 Service is ready for production!\")\n",
    "\n",
    "print(f\"\\n🎉 PROTOCOL TESTING COMPLETED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol and Model Selection Testing\n",
    "\n",
    "This notebook tests the MinionS protocol and model selection using the routellm/gpt4_dataset from HuggingFace.\n",
    "\n",
    "## Overview\n",
    "- Stream dataset directly from HuggingFace (no local storage)\n",
    "- Test adaptive_ai service running on port 8000\n",
    "- Evaluate protocol performance and model selection\n",
    "- Generate analysis reports\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Start the adaptive_ai service: `python adaptive_ai/adaptive_ai/main.py` (port 8000)\n",
    "2. Dataset is streamed directly from HuggingFace - no local files are created or stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "📡 Dataset will be streamed directly from HuggingFace (no local storage)\n",
      "🚀 Testing adaptive_ai service on port 8000\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our modules\n",
    "from src.data_loader import GPT4DatasetLoader\n",
    "from src.model_selector import ModelSelector, TaskType\n",
    "from src.protocol_tester import ProtocolTester\n",
    "from src.adaptive_ai_client import AdaptiveAIClient\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(\"📡 Dataset will be streamed directly from HuggingFace (no local storage)\")\n",
    "print(\"🚀 Testing adaptive_ai service on port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Service Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing connection to http://localhost:8000...\n",
      "\n",
      "📊 Connection Test Results:\n",
      "  🏥 Health Check: ✅ PASS\n",
      "  📡 Base URL: http://localhost:8000\n",
      "  ✅ Service is running and accessible\n",
      "  🧪 Test Request: ❌ FAILED - HTTP 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"messages\"],\"msg\":\"Field required\",\"input\":{\"prompt\":\"Hello, world!\"}}]}\n"
     ]
    }
   ],
   "source": [
    "# Test connection to adaptive_ai service\n",
    "SERVICE_URL = \"http://localhost:8000\"\n",
    "client = AdaptiveAIClient(SERVICE_URL)\n",
    "\n",
    "print(f\"🔍 Testing connection to {SERVICE_URL}...\")\n",
    "connection_test = client.test_connection()\n",
    "\n",
    "print(f\"\\n📊 Connection Test Results:\")\n",
    "print(f\"  🏥 Health Check: {'✅ PASS' if connection_test['health_check'] else '❌ FAIL'}\")\n",
    "print(f\"  📡 Base URL: {connection_test['base_url']}\")\n",
    "\n",
    "if connection_test['health_check']:\n",
    "    print(f\"  ✅ Service is running and accessible\")\n",
    "    \n",
    "    # Test request\n",
    "    test_req = connection_test.get('test_request', {})\n",
    "    if test_req.get('success'):\n",
    "        print(f\"  🧪 Test Request: ✅ SUCCESS\")\n",
    "        print(f\"    - Model: {test_req.get('selected_model')}\")\n",
    "        print(f\"    - Protocol: {test_req.get('protocol')}\")\n",
    "        print(f\"    - Response Time: {test_req.get('execution_time', 0):.4f}s\")\n",
    "    else:\n",
    "        print(f\"  🧪 Test Request: ❌ FAILED - {test_req.get('error')}\")\n",
    "else:\n",
    "    print(f\"  ❌ Service is not accessible\")\n",
    "    print(f\"  💡 Make sure to start the service with: python adaptive_ai/adaptive_ai/main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset (Streaming Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Streaming routellm/gpt4_dataset from HuggingFace...\n",
      "✅ Dataset stream initialized successfully!\n",
      "🔄 Dataset features: {'prompt': Value(dtype='string', id=None), 'source': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'gpt4_response': Value(dtype='string', id=None), 'mixtral_response': Value(dtype='string', id=None), 'mixtral_score': Value(dtype='int64', id=None)}\n",
      "📝 Note: Dataset is streamed - no local files created\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "loader = GPT4DatasetLoader()\n",
    "\n",
    "# Load dataset in streaming mode (no local download)\n",
    "print(\"🌐 Streaming routellm/gpt4_dataset from HuggingFace...\")\n",
    "try:\n",
    "    dataset = loader.load_dataset()  # streaming=True by default\n",
    "    print(f\"✅ Dataset stream initialized successfully!\")\n",
    "    print(f\"🔄 Dataset features: {dataset.features}\")\n",
    "    print(\"📝 Note: Dataset is streamed - no local files created\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    print(\"Note: Make sure you have internet connection and HuggingFace datasets installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Streaming first 100 samples...\n",
      "✅ Streamed 100 samples (no local storage)\n",
      "📊 DataFrame shape: (100, 5)\n",
      "📋 Columns: ['prompt', 'source', 'gpt4_response', 'mixtral_response', 'mixtral_score']\n"
     ]
    }
   ],
   "source": [
    "# Get a sample for testing (streaming - no local storage)\n",
    "sample_size = 100  # Smaller sample for testing the service\n",
    "print(f\"📥 Streaming first {sample_size} samples...\")\n",
    "sample_data = loader.get_sample(sample_size)\n",
    "print(f\"✅ Streamed {len(sample_data)} samples (no local storage)\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"📊 DataFrame shape: {df.shape}\")\n",
    "print(f\"📋 Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Extracting conversations from stream...\n",
      "✅ Extracted 100 conversations from stream\n",
      "\n",
      "📝 Sample conversation structure:\n",
      "📨 Conversation messages: 0\n",
      "🏷️ Metadata keys: ['prompt', 'source', 'gpt4_response', 'mixtral_response', 'mixtral_score']\n"
     ]
    }
   ],
   "source": [
    "# Extract conversations (streaming - no local storage)\n",
    "print(\"🔄 Extracting conversations from stream...\")\n",
    "conversations = loader.get_conversations(sample_size)\n",
    "print(f\"✅ Extracted {len(conversations)} conversations from stream\")\n",
    "\n",
    "# Show sample conversation structure\n",
    "if conversations:\n",
    "    print(\"\\n📝 Sample conversation structure:\")\n",
    "    sample_conv = conversations[0]\n",
    "    print(f\"📨 Conversation messages: {len(sample_conv['conversation'])}\")\n",
    "    print(f\"🏷️ Metadata keys: {list(sample_conv['metadata'].keys())}\")\n",
    "    \n",
    "    # Show first message\n",
    "    if sample_conv['conversation']:\n",
    "        first_msg = sample_conv['conversation'][0]\n",
    "        print(f\"\\n💬 First message preview: {str(first_msg)[:200]}...\")\n",
    "else:\n",
    "    print(\"⚠️ No conversations found in stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Adaptive AI Service with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Getting service statistics...\n",
      "\n",
      "🔧 Service Statistics:\n",
      "  📡 Service URL: http://localhost:8000\n",
      "  🏥 Service Available: ✅ YES\n",
      "  🧪 Test Request: ❌ FAILED\n",
      "\n",
      "✅ Service is ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Initialize protocol tester with service URL\n",
    "protocol_tester = ProtocolTester(SERVICE_URL)\n",
    "\n",
    "# Get service statistics\n",
    "print(\"📊 Getting service statistics...\")\n",
    "service_stats = protocol_tester.get_service_stats()\n",
    "\n",
    "print(f\"\\n🔧 Service Statistics:\")\n",
    "print(f\"  📡 Service URL: {service_stats['service_url']}\")\n",
    "print(f\"  🏥 Service Available: {'✅ YES' if service_stats['service_available'] else '❌ NO'}\")\n",
    "print(f\"  🧪 Test Request: {'✅ SUCCESS' if service_stats['test_request'].get('success') else '❌ FAILED'}\")\n",
    "\n",
    "if not service_stats['service_available']:\n",
    "    print(\"\\n⚠️ Service is not available. Please start the adaptive_ai service first.\")\n",
    "    print(\"💡 Run: python adaptive_ai/adaptive_ai/main.py\")\n",
    "else:\n",
    "    print(\"\\n✅ Service is ready for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing individual requests to adaptive_ai service...\n",
      "📨 Valid conversations for testing: 0\n",
      "\n",
      "🔍 Individual request examples:\n"
     ]
    }
   ],
   "source": [
    "# Test a few individual requests first\n",
    "print(\"🧪 Testing individual requests to adaptive_ai service...\")\n",
    "\n",
    "# Extract conversation messages for testing\n",
    "conversation_messages = [conv['conversation'] for conv in conversations if conv['conversation']]\n",
    "print(f\"📨 Valid conversations for testing: {len(conversation_messages)}\")\n",
    "\n",
    "# Test first few conversations\n",
    "print(\"\\n🔍 Individual request examples:\")\n",
    "for i in range(min(3, len(conversation_messages))):\n",
    "    conv = conversation_messages[i]\n",
    "    if conv and conv[0].get('content'):\n",
    "        user_prompt = conv[0]['content']\n",
    "        print(f\"\\n📝 Request {i+1}:\")\n",
    "        print(f\"  💬 Prompt: {user_prompt[:100]}...\")\n",
    "        \n",
    "        # Make request to service\n",
    "        try:\n",
    "            response = client.make_request(user_prompt)\n",
    "            print(f\"  ✅ Success: {response.success}\")\n",
    "            print(f\"  🤖 Selected Model: {response.selected_model}\")\n",
    "            print(f\"  🔄 Protocol: {response.protocol}\")\n",
    "            print(f\"  ⏱️ Response Time: {response.execution_time:.4f}s\")\n",
    "            \n",
    "            if not response.success:\n",
    "                print(f\"  ❌ Error: {response.error_message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Protocol Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running comprehensive protocol testing...\n",
      "📊 Testing 0 conversations from HuggingFace stream\n",
      "🎯 Target: adaptive_ai service at http://localhost:8000\n",
      "\n",
      "✅ Protocol testing completed!\n",
      "📈 Processed 0 conversations\n",
      "🗑️ No local files created - all data streamed directly\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive protocol testing\n",
    "print(\"🚀 Running comprehensive protocol testing...\")\n",
    "print(f\"📊 Testing {len(conversation_messages)} conversations from HuggingFace stream\")\n",
    "print(f\"🎯 Target: adaptive_ai service at {SERVICE_URL}\")\n",
    "\n",
    "try:\n",
    "    results = protocol_tester.test_model_selection(conversation_messages)\n",
    "    print(f\"\\n✅ Protocol testing completed!\")\n",
    "    print(f\"📈 Processed {len(results)} conversations\")\n",
    "    print(\"🗑️ No local files created - all data streamed directly\")\n",
    "except ConnectionError as e:\n",
    "    print(f\"\\n❌ Connection Error: {e}\")\n",
    "    print(\"💡 Please ensure the adaptive_ai service is running on port 8000\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during testing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Protocol Performance Analysis:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m analysis \u001b[38;5;241m=\u001b[39m protocol_tester\u001b[38;5;241m.\u001b[39manalyze_results()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Protocol Performance Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  📝 Total Tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tests\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ✅ Successful Tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccessful_tests\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  📈 Success Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_tests'"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "if 'results' in locals():\n",
    "    analysis = protocol_tester.analyze_results()\n",
    "    \n",
    "    print(\"📊 Protocol Performance Analysis:\")\n",
    "    print(f\"  📝 Total Tests: {analysis['total_tests']}\")\n",
    "    print(f\"  ✅ Successful Tests: {analysis['successful_tests']}\")\n",
    "    print(f\"  📈 Success Rate: {analysis['success_rate']:.2%}\")\n",
    "    print(f\"  ⏱️ Average Execution Time: {analysis['avg_execution_time']:.4f}s\")\n",
    "    print(f\"  🌐 Data Source: HuggingFace stream (no local storage)\")\n",
    "    print(f\"  🎯 Service: {SERVICE_URL}\")\n",
    "    \n",
    "    # Show top models and protocols\n",
    "    if analysis['model_usage']:\n",
    "        print(f\"\\n🏆 Top 3 Selected Models:\")\n",
    "        top_models = sorted(analysis['model_usage'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for i, (model, count) in enumerate(top_models, 1):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {i}. {model}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    if analysis['protocol_usage']:\n",
    "        print(f\"\\n🔄 Protocol Usage:\")\n",
    "        for protocol, count in sorted(analysis['protocol_usage'].items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {protocol}: {count} uses ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️ No results to analyze - testing may have failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "if 'results' in locals():\n",
    "    results_df = protocol_tester.to_dataframe()\n",
    "    print(f\"📊 Results DataFrame shape: {results_df.shape}\")\n",
    "    print(\"\\n📋 First few results:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    print(f\"\\n📊 Success Rate: {results_df['success'].mean():.2%}\")\n",
    "    print(f\"⏱️ Average Response Time: {results_df[results_df['success']]['execution_time'].mean():.4f}s\")\n",
    "else:\n",
    "    print(\"⚠️ No results DataFrame available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance metrics\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    \n",
    "    if not successful_results.empty:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Execution time histogram\n",
    "        ax1.hist(successful_results['execution_time'], bins=20, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "        ax1.set_title('Execution Time Distribution (Adaptive AI Service)', fontweight='bold')\n",
    "        ax1.set_xlabel('Execution Time (seconds)')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.axvline(successful_results['execution_time'].mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {successful_results[\"execution_time\"].mean():.4f}s')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 2. Success rate pie chart\n",
    "        success_counts = [len(successful_results), len(results_df) - len(successful_results)]\n",
    "        labels = ['Successful', 'Failed']\n",
    "        colors = ['#96CEB4', '#FF6B6B']\n",
    "        ax2.pie(success_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Success Rate Distribution', fontweight='bold')\n",
    "        \n",
    "        # 3. Model selection distribution\n",
    "        model_counts = successful_results['selected_model'].value_counts()\n",
    "        ax3.bar(range(len(model_counts)), model_counts.values, color='#45B7D1', alpha=0.7)\n",
    "        ax3.set_title('Selected Model Distribution', fontweight='bold')\n",
    "        ax3.set_xlabel('Model Index')\n",
    "        ax3.set_ylabel('Usage Count')\n",
    "        ax3.set_xticks(range(len(model_counts)))\n",
    "        ax3.set_xticklabels([m.split('/')[-1] if '/' in m else m for m in model_counts.index], \n",
    "                           rotation=45, ha='right')\n",
    "        \n",
    "        # 4. Protocol distribution\n",
    "        protocol_counts = successful_results['protocol'].value_counts()\n",
    "        ax4.bar(protocol_counts.index, protocol_counts.values, color='#FFEAA7', alpha=0.7)\n",
    "        ax4.set_title('Protocol Distribution', fontweight='bold')\n",
    "        ax4.set_xlabel('Protocol Type')\n",
    "        ax4.set_ylabel('Usage Count')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"✅ Performance visualization complete\")\n",
    "    else:\n",
    "        print(\"⚠️ No successful results to visualize\")\n",
    "else:\n",
    "    print(\"⚠️ No results data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task vs Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task type vs Model selection analysis\n",
    "if 'successful_results' in locals() and not successful_results.empty:\n",
    "    # Create cross-tabulation\n",
    "    cross_tab = pd.crosstab(successful_results['task_type'], successful_results['selected_model'])\n",
    "    \n",
    "    if not cross_tab.empty:\n",
    "        # Visualize as heatmap\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': 'Count'}, linewidths=0.5)\n",
    "        plt.title('Task Type vs Selected Model Heatmap (Adaptive AI Service)', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Selected Model', fontsize=12)\n",
    "        plt.ylabel('Task Type', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"✅ Task-Model correlation analysis complete\")\n",
    "    else:\n",
    "        print(\"⚠️ No cross-tabulation data available\")\n",
    "else:\n",
    "    print(\"⚠️ No successful results for task analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics by task type\n",
    "if 'successful_results' in locals() and not successful_results.empty:\n",
    "    task_performance = successful_results.groupby('task_type').agg({\n",
    "        'execution_time': ['mean', 'std', 'count'],\n",
    "        'selected_model': lambda x: x.nunique()  # Number of unique models per task\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"📊 Performance Metrics by Task Type:\")\n",
    "    print(task_performance)\n",
    "    print(\"\\n✅ Task performance analysis complete\")\n",
    "else:\n",
    "    print(\"⚠️ No successful results for task performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "if 'results' in locals():\n",
    "    report = protocol_tester.generate_report()\n",
    "    print(\"📄 COMPREHENSIVE ADAPTIVE AI SERVICE TESTING REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(report)\n",
    "    print(\"\\n🌐 Data Source: HuggingFace streaming API\")\n",
    "    print(\"🎯 Service Tested: adaptive_ai on port 8000\")\n",
    "    print(\"🗑️ No local files created or stored\")\n",
    "    print(\"✅ All data processed directly from stream\")\n",
    "else:\n",
    "    print(\"⚠️ No results available for report generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (optional - creates local files only if needed)\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = \"results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results DataFrame\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{results_dir}/adaptive_ai_service_results_{timestamp}.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"💾 Results saved to {results_file}\")\n",
    "    \n",
    "    # Save enhanced report\n",
    "    if 'report' in locals():\n",
    "        report_file = f\"{results_dir}/adaptive_ai_service_report_{timestamp}.txt\"\n",
    "        enhanced_report = f\"\"\"Adaptive AI Service Testing Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Service URL: {SERVICE_URL}\n",
    "Data Source: routellm/gpt4_dataset (streamed from HuggingFace)\n",
    "Local Storage: None - all data streamed directly\n",
    "\n",
    "{report}\n",
    "\n",
    "Technical Details:\n",
    "- Dataset streamed using HuggingFace datasets library\n",
    "- No local dataset files created or stored\n",
    "- Real-time testing of adaptive_ai service\n",
    "- Service running on port 8000\n",
    "- HTTP requests made to /predict endpoint\n",
    "\"\"\"\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(enhanced_report)\n",
    "        print(f\"📄 Enhanced report saved to {report_file}\")\n",
    "else:\n",
    "    print(\"⚠️ No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"🎯 ADAPTIVE AI SERVICE TESTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'analysis' in locals():\n",
    "    print(f\"📊 Service: {SERVICE_URL}\")\n",
    "    print(f\"📈 Dataset: routellm/gpt4_dataset (streamed from HuggingFace)\")\n",
    "    print(f\"📊 Sample Size: {analysis['total_tests']} conversations\")\n",
    "    print(f\"✅ Success Rate: {analysis['success_rate']:.2%}\")\n",
    "    print(f\"⚡ Average Response Time: {analysis['avg_execution_time']:.4f}s\")\n",
    "    print(f\"🌐 Data Processing: 100% streamed (no local storage)\")\n",
    "    \n",
    "    print(\"\\n🏆 Top Models Used by Service:\")\n",
    "    if analysis['model_usage']:\n",
    "        top_models = sorted(analysis['model_usage'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for i, (model, count) in enumerate(top_models, 1):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {i}. {model}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n🔄 Protocol Usage:\")\n",
    "    if analysis['protocol_usage']:\n",
    "        for protocol, count in sorted(analysis['protocol_usage'].items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {protocol}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n📋 Task Distribution:\")\n",
    "    if analysis['task_distribution']:\n",
    "        top_tasks = sorted(analysis['task_distribution'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for i, (task, count) in enumerate(top_tasks, 1):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {i}. {task}: {count} instances ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️ No analysis data available\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"💡 SERVICE TESTING RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"🔧 1. Service Performance:\")\n",
    "print(\"   - Monitor response times under different loads\")\n",
    "print(\"   - Implement service health monitoring\")\n",
    "print(\"   - Add request queuing for high-traffic scenarios\")\n",
    "\n",
    "print(\"\\n⚡ 2. Protocol Optimization:\")\n",
    "print(\"   - Analyze protocol selection patterns\")\n",
    "print(\"   - Optimize model selection algorithms\")\n",
    "print(\"   - Implement caching for frequently requested patterns\")\n",
    "\n",
    "print(\"\\n📊 3. Monitoring & Analytics:\")\n",
    "print(\"   - Set up continuous testing with diverse datasets\")\n",
    "print(\"   - Track model performance over time\")\n",
    "print(\"   - Monitor service availability and response times\")\n",
    "\n",
    "print(\"\\n🌐 4. Scalability:\")\n",
    "print(\"   - Test with larger datasets\")\n",
    "print(\"   - Implement load balancing\")\n",
    "print(\"   - Consider distributed processing for high volume\")\n",
    "\n",
    "print(\"\\n✅ ADAPTIVE AI SERVICE TESTING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"🎉 Service tested with real HuggingFace data\")\n",
    "print(\"🗑️ Zero local storage footprint achieved\")\n",
    "print(\"🚀 Ready for production deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
