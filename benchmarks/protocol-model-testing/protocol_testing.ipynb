{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ADAPTIVE AI SERVICE PROTOCOL TESTING\n",
      "==================================================\n",
      "üì° Testing connection to http://localhost:8000...\n",
      "‚úÖ Service is healthy and ready!\n",
      "üåê Streaming routellm/gpt4_dataset from HuggingFace...\n",
      "‚úÖ Dataset stream initialized successfully!\n",
      "üì• Collecting 10000 samples from stream...\n",
      "‚úÖ Collected 10000 samples\n",
      "\n",
      "üß™ TESTING SERVICE WITH REAL PROMPTS\n",
      "========================================\n",
      "üìä Processed 10/10000 requests\n",
      "üìä Processed 20/10000 requests\n",
      "üìä Processed 30/10000 requests\n",
      "üìä Processed 40/10000 requests\n",
      "üìä Processed 50/10000 requests\n",
      "üìä Processed 60/10000 requests\n",
      "üìä Processed 70/10000 requests\n",
      "üìä Processed 80/10000 requests\n",
      "üìä Processed 90/10000 requests\n",
      "üìä Processed 100/10000 requests\n",
      "üìä Processed 110/10000 requests\n",
      "üìä Processed 120/10000 requests\n",
      "üìä Processed 130/10000 requests\n",
      "üìä Processed 140/10000 requests\n",
      "üìä Processed 150/10000 requests\n",
      "üìä Processed 160/10000 requests\n",
      "üìä Processed 170/10000 requests\n",
      "üìä Processed 180/10000 requests\n",
      "üìä Processed 190/10000 requests\n",
      "üìä Processed 200/10000 requests\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSERVICE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/predict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m         json\u001b[38;5;241m=\u001b[39mrequest_data,\n\u001b[1;32m     96\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    790\u001b[0m     conn,\n\u001b[1;32m    791\u001b[0m     method,\n\u001b[1;32m    792\u001b[0m     url,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    802\u001b[0m )\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complete Protocol Testing for Adaptive AI Service\n",
    "# ================================================\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ ADAPTIVE AI SERVICE PROTOCOL TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuration\n",
    "SERVICE_URL = \"http://localhost:8000\"\n",
    "SAMPLE_SIZE = 10000  # Reasonable sample size for testing\n",
    "DATASET_NAME = \"routellm/gpt4_dataset\"\n",
    "\n",
    "# Test service connection\n",
    "def test_service():\n",
    "    try:\n",
    "        response = requests.get(f\"{SERVICE_URL}/health\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(f\"üì° Testing connection to {SERVICE_URL}...\")\n",
    "if not test_service():\n",
    "    print(\"‚ùå Service is not available!\")\n",
    "    print(\"üí° Please start the service with: uv run python -m adaptive_ai.main\")\n",
    "    exit()\n",
    "\n",
    "print(\"‚úÖ Service is healthy and ready!\")\n",
    "\n",
    "# Load dataset from HuggingFace (streaming)\n",
    "print(f\"üåê Streaming {DATASET_NAME} from HuggingFace...\")\n",
    "try:\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"validation\", streaming=True)\n",
    "    print(\"‚úÖ Dataset stream initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Collect sample data\n",
    "print(f\"üì• Collecting {SAMPLE_SIZE} samples from stream...\")\n",
    "sample_data = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= SAMPLE_SIZE:\n",
    "        break\n",
    "    sample_data.append(item)\n",
    "\n",
    "print(f\"‚úÖ Collected {len(sample_data)} samples\")\n",
    "\n",
    "# Test the service with real prompts\n",
    "print(\"\\nüß™ TESTING SERVICE WITH REAL PROMPTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "results = []\n",
    "successful_tests = 0\n",
    "total_tests = len(sample_data)\n",
    "\n",
    "for i, item in enumerate(sample_data):\n",
    "    prompt = item.get('prompt', '')\n",
    "    if not prompt:\n",
    "        continue\n",
    "    \n",
    "    # Create proper message format for the service\n",
    "    request_data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{SERVICE_URL}/predict\",\n",
    "            json=request_data,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            \n",
    "            # Extract information from response\n",
    "            protocol = result.get('protocol', 'unknown')\n",
    "            selected_model = 'unknown'\n",
    "            provider = 'unknown'\n",
    "            \n",
    "            # Extract model and provider based on protocol\n",
    "            if protocol == 'minion' and 'minion' in result:\n",
    "                selected_model = result['minion'].get('model', 'unknown')\n",
    "                provider = result['minion'].get('provider', 'unknown')\n",
    "            elif protocol == 'standard' and 'standard' in result:\n",
    "                selected_model = result['standard'].get('model', 'unknown')\n",
    "                provider = result['standard'].get('provider', 'unknown')\n",
    "            \n",
    "            results.append({\n",
    "                'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "                'protocol': protocol,\n",
    "                'selected_model': selected_model,\n",
    "                'provider': provider,\n",
    "                'execution_time': execution_time,\n",
    "                'success': True,\n",
    "                'response': result\n",
    "            })\n",
    "            \n",
    "            successful_tests += 1\n",
    "            \n",
    "        else:\n",
    "            results.append({\n",
    "                'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "                'protocol': 'unknown',\n",
    "                'selected_model': 'unknown',\n",
    "                'provider': 'unknown',\n",
    "                'execution_time': execution_time,\n",
    "                'success': False,\n",
    "                'error': f\"HTTP {response.status_code}: {response.text}\"\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        results.append({\n",
    "            'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "            'protocol': 'unknown',\n",
    "            'selected_model': 'unknown',\n",
    "            'provider': 'unknown',\n",
    "            'execution_time': execution_time,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Progress update\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"üìä Processed {i + 1}/{total_tests} requests\")\n",
    "\n",
    "print(f\"\\n‚úÖ Testing completed!\")\n",
    "print(f\"üìà Total tests: {total_tests}\")\n",
    "print(f\"‚úÖ Successful tests: {successful_tests}\")\n",
    "print(f\"üìä Success rate: {successful_tests/total_tests:.2%}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "successful_df = df[df['success'] == True]\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if not successful_df.empty:\n",
    "    avg_time = successful_df['execution_time'].mean()\n",
    "    print(f\"‚è±Ô∏è  Average response time: {avg_time:.4f}s\")\n",
    "    print(f\"üìà Min response time: {successful_df['execution_time'].min():.4f}s\")\n",
    "    print(f\"üìà Max response time: {successful_df['execution_time'].max():.4f}s\")\n",
    "    \n",
    "    # Protocol distribution\n",
    "    protocol_counts = successful_df['protocol'].value_counts()\n",
    "    print(f\"\\nüîÑ Protocol Usage:\")\n",
    "    for protocol, count in protocol_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        print(f\"  {protocol}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Model distribution\n",
    "    model_counts = successful_df['selected_model'].value_counts()\n",
    "    print(f\"\\nü§ñ Model Usage:\")\n",
    "    for model, count in model_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        print(f\"  {model}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Provider distribution\n",
    "    provider_counts = successful_df['provider'].value_counts()\n",
    "    print(f\"\\nüè¢ Provider Usage:\")\n",
    "    for provider, count in provider_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        print(f\"  {provider}: {count} uses ({percentage:.1f}%)\")\n",
    "\n",
    "# Show some example responses\n",
    "print(f\"\\nüìù EXAMPLE RESPONSES\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "for i, result in enumerate(successful_df.head(3).to_dict('records')):\n",
    "    print(f\"\\nüìã Example {i+1}:\")\n",
    "    print(f\"  üí¨ Prompt: {result['prompt']}\")\n",
    "    print(f\"  üîÑ Protocol: {result['protocol']}\")\n",
    "    print(f\"  ü§ñ Model: {result['selected_model']}\")\n",
    "    print(f\"  üè¢ Provider: {result['provider']}\")\n",
    "    print(f\"  ‚è±Ô∏è Time: {result['execution_time']:.4f}s\")\n",
    "\n",
    "# Create visualizations\n",
    "if not successful_df.empty:\n",
    "    print(f\"\\nüìä GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Response time distribution\n",
    "    ax1.hist(successful_df['execution_time'], bins=15, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "    ax1.set_title('Response Time Distribution', fontweight='bold')\n",
    "    ax1.set_xlabel('Response Time (seconds)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.axvline(avg_time, color='red', linestyle='--', label=f'Mean: {avg_time:.4f}s')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Success rate pie chart\n",
    "    success_counts = [successful_tests, total_tests - successful_tests]\n",
    "    labels = ['Successful', 'Failed']\n",
    "    colors = ['#96CEB4', '#FF6B6B']\n",
    "    ax2.pie(success_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Success Rate Distribution', fontweight='bold')\n",
    "    \n",
    "    # 3. Protocol usage\n",
    "    protocol_counts.plot(kind='bar', ax=ax3, color='#45B7D1', alpha=0.7)\n",
    "    ax3.set_title('Protocol Usage Distribution', fontweight='bold')\n",
    "    ax3.set_xlabel('Protocol')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Model usage\n",
    "    model_counts.plot(kind='bar', ax=ax4, color='#FFEAA7', alpha=0.7)\n",
    "    ax4.set_title('Model Usage Distribution', fontweight='bold')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations generated successfully!\")\n",
    "\n",
    "# Save results\n",
    "print(f\"\\nüíæ SAVING RESULTS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save DataFrame\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = f\"{results_dir}/adaptive_ai_test_results_{timestamp}.csv\"\n",
    "df.to_csv(results_file, index=False)\n",
    "print(f\"üìä Results saved to: {results_file}\")\n",
    "\n",
    "# Generate and save report\n",
    "report = f\"\"\"Adaptive AI Service Testing Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Service URL: {SERVICE_URL}\n",
    "Dataset: {DATASET_NAME} (streamed from HuggingFace)\n",
    "Sample Size: {SAMPLE_SIZE}\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "==================\n",
    "Total Tests: {total_tests}\n",
    "Successful Tests: {successful_tests}\n",
    "Success Rate: {successful_tests/total_tests:.2%}\n",
    "Average Response Time: {avg_time:.4f}s\n",
    "\n",
    "PROTOCOL USAGE:\n",
    "===============\n",
    "\"\"\"\n",
    "\n",
    "if not successful_df.empty:\n",
    "    for protocol, count in protocol_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        report += f\"{protocol}: {count} uses ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    report += \"\\nMODEL USAGE:\\n============\\n\"\n",
    "    for model, count in model_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        report += f\"{model}: {count} uses ({percentage:.1f}%)\\n\"\n",
    "    \n",
    "    report += \"\\nPROVIDER USAGE:\\n===============\\n\"\n",
    "    for provider, count in provider_counts.items():\n",
    "        percentage = (count / len(successful_df)) * 100\n",
    "        report += f\"{provider}: {count} uses ({percentage:.1f}%)\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "TECHNICAL DETAILS:\n",
    "==================\n",
    "- Dataset streamed directly from HuggingFace\n",
    "- No local files created for dataset\n",
    "- Real-time testing of adaptive_ai service\n",
    "- HTTP requests to /predict endpoint\n",
    "- Service running on port 8000\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "================\n",
    "1. Monitor response times under load\n",
    "2. Implement service health monitoring  \n",
    "3. Add request queuing for high traffic\n",
    "4. Track model performance over time\n",
    "5. Consider load balancing for scaling\n",
    "\"\"\"\n",
    "\n",
    "report_file = f\"{results_dir}/adaptive_ai_test_report_{timestamp}.txt\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"üìÑ Report saved to: {report_file}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüéØ TESTING SUMMARY\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"‚úÖ Service tested successfully with {SAMPLE_SIZE} real prompts\")\n",
    "print(f\"üìä Success rate: {successful_tests/total_tests:.2%}\")\n",
    "print(f\"‚è±Ô∏è Average response time: {avg_time:.4f}s\")\n",
    "print(f\"üåê Data streamed directly from HuggingFace\")\n",
    "print(f\"üóëÔ∏è No local dataset files created\")\n",
    "print(f\"üöÄ Service is ready for production!\")\n",
    "\n",
    "print(f\"\\nüéâ PROTOCOL TESTING COMPLETED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol and Model Selection Testing\n",
    "\n",
    "This notebook tests the MinionS protocol and model selection using the routellm/gpt4_dataset from HuggingFace.\n",
    "\n",
    "## Overview\n",
    "- Stream dataset directly from HuggingFace (no local storage)\n",
    "- Test adaptive_ai service running on port 8000\n",
    "- Evaluate protocol performance and model selection\n",
    "- Generate analysis reports\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Start the adaptive_ai service: `python adaptive_ai/adaptive_ai/main.py` (port 8000)\n",
    "2. Dataset is streamed directly from HuggingFace - no local files are created or stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "üì° Dataset will be streamed directly from HuggingFace (no local storage)\n",
      "üöÄ Testing adaptive_ai service on port 8000\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our modules\n",
    "from src.data_loader import GPT4DatasetLoader\n",
    "from src.model_selector import ModelSelector, TaskType\n",
    "from src.protocol_tester import ProtocolTester\n",
    "from src.adaptive_ai_client import AdaptiveAIClient\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(\"üì° Dataset will be streamed directly from HuggingFace (no local storage)\")\n",
    "print(\"üöÄ Testing adaptive_ai service on port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Service Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing connection to http://localhost:8000...\n",
      "\n",
      "üìä Connection Test Results:\n",
      "  üè• Health Check: ‚úÖ PASS\n",
      "  üì° Base URL: http://localhost:8000\n",
      "  ‚úÖ Service is running and accessible\n",
      "  üß™ Test Request: ‚ùå FAILED - HTTP 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"messages\"],\"msg\":\"Field required\",\"input\":{\"prompt\":\"Hello, world!\"}}]}\n"
     ]
    }
   ],
   "source": [
    "# Test connection to adaptive_ai service\n",
    "SERVICE_URL = \"http://localhost:8000\"\n",
    "client = AdaptiveAIClient(SERVICE_URL)\n",
    "\n",
    "print(f\"üîç Testing connection to {SERVICE_URL}...\")\n",
    "connection_test = client.test_connection()\n",
    "\n",
    "print(f\"\\nüìä Connection Test Results:\")\n",
    "print(f\"  üè• Health Check: {'‚úÖ PASS' if connection_test['health_check'] else '‚ùå FAIL'}\")\n",
    "print(f\"  üì° Base URL: {connection_test['base_url']}\")\n",
    "\n",
    "if connection_test['health_check']:\n",
    "    print(f\"  ‚úÖ Service is running and accessible\")\n",
    "    \n",
    "    # Test request\n",
    "    test_req = connection_test.get('test_request', {})\n",
    "    if test_req.get('success'):\n",
    "        print(f\"  üß™ Test Request: ‚úÖ SUCCESS\")\n",
    "        print(f\"    - Model: {test_req.get('selected_model')}\")\n",
    "        print(f\"    - Protocol: {test_req.get('protocol')}\")\n",
    "        print(f\"    - Response Time: {test_req.get('execution_time', 0):.4f}s\")\n",
    "    else:\n",
    "        print(f\"  üß™ Test Request: ‚ùå FAILED - {test_req.get('error')}\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Service is not accessible\")\n",
    "    print(f\"  üí° Make sure to start the service with: python adaptive_ai/adaptive_ai/main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset (Streaming Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Streaming routellm/gpt4_dataset from HuggingFace...\n",
      "‚úÖ Dataset stream initialized successfully!\n",
      "üîÑ Dataset features: {'prompt': Value(dtype='string', id=None), 'source': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'gpt4_response': Value(dtype='string', id=None), 'mixtral_response': Value(dtype='string', id=None), 'mixtral_score': Value(dtype='int64', id=None)}\n",
      "üìù Note: Dataset is streamed - no local files created\n"
     ]
    }
   ],
   "source": [
    "# Initialize data loader\n",
    "loader = GPT4DatasetLoader()\n",
    "\n",
    "# Load dataset in streaming mode (no local download)\n",
    "print(\"üåê Streaming routellm/gpt4_dataset from HuggingFace...\")\n",
    "try:\n",
    "    dataset = loader.load_dataset()  # streaming=True by default\n",
    "    print(f\"‚úÖ Dataset stream initialized successfully!\")\n",
    "    print(f\"üîÑ Dataset features: {dataset.features}\")\n",
    "    print(\"üìù Note: Dataset is streamed - no local files created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"Note: Make sure you have internet connection and HuggingFace datasets installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Streaming first 100 samples...\n",
      "‚úÖ Streamed 100 samples (no local storage)\n",
      "üìä DataFrame shape: (100, 5)\n",
      "üìã Columns: ['prompt', 'source', 'gpt4_response', 'mixtral_response', 'mixtral_score']\n"
     ]
    }
   ],
   "source": [
    "# Get a sample for testing (streaming - no local storage)\n",
    "sample_size = 100  # Smaller sample for testing the service\n",
    "print(f\"üì• Streaming first {sample_size} samples...\")\n",
    "sample_data = loader.get_sample(sample_size)\n",
    "print(f\"‚úÖ Streamed {len(sample_data)} samples (no local storage)\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"üìä DataFrame shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting conversations from stream...\n",
      "‚úÖ Extracted 100 conversations from stream\n",
      "\n",
      "üìù Sample conversation structure:\n",
      "üì® Conversation messages: 0\n",
      "üè∑Ô∏è Metadata keys: ['prompt', 'source', 'gpt4_response', 'mixtral_response', 'mixtral_score']\n"
     ]
    }
   ],
   "source": [
    "# Extract conversations (streaming - no local storage)\n",
    "print(\"üîÑ Extracting conversations from stream...\")\n",
    "conversations = loader.get_conversations(sample_size)\n",
    "print(f\"‚úÖ Extracted {len(conversations)} conversations from stream\")\n",
    "\n",
    "# Show sample conversation structure\n",
    "if conversations:\n",
    "    print(\"\\nüìù Sample conversation structure:\")\n",
    "    sample_conv = conversations[0]\n",
    "    print(f\"üì® Conversation messages: {len(sample_conv['conversation'])}\")\n",
    "    print(f\"üè∑Ô∏è Metadata keys: {list(sample_conv['metadata'].keys())}\")\n",
    "    \n",
    "    # Show first message\n",
    "    if sample_conv['conversation']:\n",
    "        first_msg = sample_conv['conversation'][0]\n",
    "        print(f\"\\nüí¨ First message preview: {str(first_msg)[:200]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversations found in stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Adaptive AI Service with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Getting service statistics...\n",
      "\n",
      "üîß Service Statistics:\n",
      "  üì° Service URL: http://localhost:8000\n",
      "  üè• Service Available: ‚úÖ YES\n",
      "  üß™ Test Request: ‚ùå FAILED\n",
      "\n",
      "‚úÖ Service is ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Initialize protocol tester with service URL\n",
    "protocol_tester = ProtocolTester(SERVICE_URL)\n",
    "\n",
    "# Get service statistics\n",
    "print(\"üìä Getting service statistics...\")\n",
    "service_stats = protocol_tester.get_service_stats()\n",
    "\n",
    "print(f\"\\nüîß Service Statistics:\")\n",
    "print(f\"  üì° Service URL: {service_stats['service_url']}\")\n",
    "print(f\"  üè• Service Available: {'‚úÖ YES' if service_stats['service_available'] else '‚ùå NO'}\")\n",
    "print(f\"  üß™ Test Request: {'‚úÖ SUCCESS' if service_stats['test_request'].get('success') else '‚ùå FAILED'}\")\n",
    "\n",
    "if not service_stats['service_available']:\n",
    "    print(\"\\n‚ö†Ô∏è Service is not available. Please start the adaptive_ai service first.\")\n",
    "    print(\"üí° Run: python adaptive_ai/adaptive_ai/main.py\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Service is ready for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing individual requests to adaptive_ai service...\n",
      "üì® Valid conversations for testing: 0\n",
      "\n",
      "üîç Individual request examples:\n"
     ]
    }
   ],
   "source": [
    "# Test a few individual requests first\n",
    "print(\"üß™ Testing individual requests to adaptive_ai service...\")\n",
    "\n",
    "# Extract conversation messages for testing\n",
    "conversation_messages = [conv['conversation'] for conv in conversations if conv['conversation']]\n",
    "print(f\"üì® Valid conversations for testing: {len(conversation_messages)}\")\n",
    "\n",
    "# Test first few conversations\n",
    "print(\"\\nüîç Individual request examples:\")\n",
    "for i in range(min(3, len(conversation_messages))):\n",
    "    conv = conversation_messages[i]\n",
    "    if conv and conv[0].get('content'):\n",
    "        user_prompt = conv[0]['content']\n",
    "        print(f\"\\nüìù Request {i+1}:\")\n",
    "        print(f\"  üí¨ Prompt: {user_prompt[:100]}...\")\n",
    "        \n",
    "        # Make request to service\n",
    "        try:\n",
    "            response = client.make_request(user_prompt)\n",
    "            print(f\"  ‚úÖ Success: {response.success}\")\n",
    "            print(f\"  ü§ñ Selected Model: {response.selected_model}\")\n",
    "            print(f\"  üîÑ Protocol: {response.protocol}\")\n",
    "            print(f\"  ‚è±Ô∏è Response Time: {response.execution_time:.4f}s\")\n",
    "            \n",
    "            if not response.success:\n",
    "                print(f\"  ‚ùå Error: {response.error_message}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Protocol Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running comprehensive protocol testing...\n",
      "üìä Testing 0 conversations from HuggingFace stream\n",
      "üéØ Target: adaptive_ai service at http://localhost:8000\n",
      "\n",
      "‚úÖ Protocol testing completed!\n",
      "üìà Processed 0 conversations\n",
      "üóëÔ∏è No local files created - all data streamed directly\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive protocol testing\n",
    "print(\"üöÄ Running comprehensive protocol testing...\")\n",
    "print(f\"üìä Testing {len(conversation_messages)} conversations from HuggingFace stream\")\n",
    "print(f\"üéØ Target: adaptive_ai service at {SERVICE_URL}\")\n",
    "\n",
    "try:\n",
    "    results = protocol_tester.test_model_selection(conversation_messages)\n",
    "    print(f\"\\n‚úÖ Protocol testing completed!\")\n",
    "    print(f\"üìà Processed {len(results)} conversations\")\n",
    "    print(\"üóëÔ∏è No local files created - all data streamed directly\")\n",
    "except ConnectionError as e:\n",
    "    print(f\"\\n‚ùå Connection Error: {e}\")\n",
    "    print(\"üí° Please ensure the adaptive_ai service is running on port 8000\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during testing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Protocol Performance Analysis:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m analysis \u001b[38;5;241m=\u001b[39m protocol_tester\u001b[38;5;241m.\u001b[39manalyze_results()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Protocol Performance Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  üìù Total Tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tests\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ‚úÖ Successful Tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccessful_tests\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  üìà Success Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_tests'"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "if 'results' in locals():\n",
    "    analysis = protocol_tester.analyze_results()\n",
    "    \n",
    "    print(\"üìä Protocol Performance Analysis:\")\n",
    "    print(f\"  üìù Total Tests: {analysis['total_tests']}\")\n",
    "    print(f\"  ‚úÖ Successful Tests: {analysis['successful_tests']}\")\n",
    "    print(f\"  üìà Success Rate: {analysis['success_rate']:.2%}\")\n",
    "    print(f\"  ‚è±Ô∏è Average Execution Time: {analysis['avg_execution_time']:.4f}s\")\n",
    "    print(f\"  üåê Data Source: HuggingFace stream (no local storage)\")\n",
    "    print(f\"  üéØ Service: {SERVICE_URL}\")\n",
    "    \n",
    "    # Show top models and protocols\n",
    "    if analysis['model_usage']:\n",
    "        print(f\"\\nüèÜ Top 3 Selected Models:\")\n",
    "        top_models = sorted(analysis['model_usage'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for i, (model, count) in enumerate(top_models, 1):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {i}. {model}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    if analysis['protocol_usage']:\n",
    "        print(f\"\\nüîÑ Protocol Usage:\")\n",
    "        for protocol, count in sorted(analysis['protocol_usage'].items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {protocol}: {count} uses ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to analyze - testing may have failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "if 'results' in locals():\n",
    "    results_df = protocol_tester.to_dataframe()\n",
    "    print(f\"üìä Results DataFrame shape: {results_df.shape}\")\n",
    "    print(\"\\nüìã First few results:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    print(f\"\\nüìä Success Rate: {results_df['success'].mean():.2%}\")\n",
    "    print(f\"‚è±Ô∏è Average Response Time: {results_df[results_df['success']]['execution_time'].mean():.4f}s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results DataFrame available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance metrics\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    \n",
    "    if not successful_results.empty:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Execution time histogram\n",
    "        ax1.hist(successful_results['execution_time'], bins=20, alpha=0.7, color='#4ECDC4', edgecolor='black')\n",
    "        ax1.set_title('Execution Time Distribution (Adaptive AI Service)', fontweight='bold')\n",
    "        ax1.set_xlabel('Execution Time (seconds)')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.axvline(successful_results['execution_time'].mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {successful_results[\"execution_time\"].mean():.4f}s')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 2. Success rate pie chart\n",
    "        success_counts = [len(successful_results), len(results_df) - len(successful_results)]\n",
    "        labels = ['Successful', 'Failed']\n",
    "        colors = ['#96CEB4', '#FF6B6B']\n",
    "        ax2.pie(success_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Success Rate Distribution', fontweight='bold')\n",
    "        \n",
    "        # 3. Model selection distribution\n",
    "        model_counts = successful_results['selected_model'].value_counts()\n",
    "        ax3.bar(range(len(model_counts)), model_counts.values, color='#45B7D1', alpha=0.7)\n",
    "        ax3.set_title('Selected Model Distribution', fontweight='bold')\n",
    "        ax3.set_xlabel('Model Index')\n",
    "        ax3.set_ylabel('Usage Count')\n",
    "        ax3.set_xticks(range(len(model_counts)))\n",
    "        ax3.set_xticklabels([m.split('/')[-1] if '/' in m else m for m in model_counts.index], \n",
    "                           rotation=45, ha='right')\n",
    "        \n",
    "        # 4. Protocol distribution\n",
    "        protocol_counts = successful_results['protocol'].value_counts()\n",
    "        ax4.bar(protocol_counts.index, protocol_counts.values, color='#FFEAA7', alpha=0.7)\n",
    "        ax4.set_title('Protocol Distribution', fontweight='bold')\n",
    "        ax4.set_xlabel('Protocol Type')\n",
    "        ax4.set_ylabel('Usage Count')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Performance visualization complete\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No successful results to visualize\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task vs Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task type vs Model selection analysis\n",
    "if 'successful_results' in locals() and not successful_results.empty:\n",
    "    # Create cross-tabulation\n",
    "    cross_tab = pd.crosstab(successful_results['task_type'], successful_results['selected_model'])\n",
    "    \n",
    "    if not cross_tab.empty:\n",
    "        # Visualize as heatmap\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': 'Count'}, linewidths=0.5)\n",
    "        plt.title('Task Type vs Selected Model Heatmap (Adaptive AI Service)', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Selected Model', fontsize=12)\n",
    "        plt.ylabel('Task Type', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"‚úÖ Task-Model correlation analysis complete\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No cross-tabulation data available\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No successful results for task analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics by task type\n",
    "if 'successful_results' in locals() and not successful_results.empty:\n",
    "    task_performance = successful_results.groupby('task_type').agg({\n",
    "        'execution_time': ['mean', 'std', 'count'],\n",
    "        'selected_model': lambda x: x.nunique()  # Number of unique models per task\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"üìä Performance Metrics by Task Type:\")\n",
    "    print(task_performance)\n",
    "    print(\"\\n‚úÖ Task performance analysis complete\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No successful results for task performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "if 'results' in locals():\n",
    "    report = protocol_tester.generate_report()\n",
    "    print(\"üìÑ COMPREHENSIVE ADAPTIVE AI SERVICE TESTING REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(report)\n",
    "    print(\"\\nüåê Data Source: HuggingFace streaming API\")\n",
    "    print(\"üéØ Service Tested: adaptive_ai on port 8000\")\n",
    "    print(\"üóëÔ∏è No local files created or stored\")\n",
    "    print(\"‚úÖ All data processed directly from stream\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for report generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (optional - creates local files only if needed)\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = \"results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results DataFrame\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"{results_dir}/adaptive_ai_service_results_{timestamp}.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"üíæ Results saved to {results_file}\")\n",
    "    \n",
    "    # Save enhanced report\n",
    "    if 'report' in locals():\n",
    "        report_file = f\"{results_dir}/adaptive_ai_service_report_{timestamp}.txt\"\n",
    "        enhanced_report = f\"\"\"Adaptive AI Service Testing Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Service URL: {SERVICE_URL}\n",
    "Data Source: routellm/gpt4_dataset (streamed from HuggingFace)\n",
    "Local Storage: None - all data streamed directly\n",
    "\n",
    "{report}\n",
    "\n",
    "Technical Details:\n",
    "- Dataset streamed using HuggingFace datasets library\n",
    "- No local dataset files created or stored\n",
    "- Real-time testing of adaptive_ai service\n",
    "- Service running on port 8000\n",
    "- HTTP requests made to /predict endpoint\n",
    "\"\"\"\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(enhanced_report)\n",
    "        print(f\"üìÑ Enhanced report saved to {report_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéØ ADAPTIVE AI SERVICE TESTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'analysis' in locals():\n",
    "    print(f\"üìä Service: {SERVICE_URL}\")\n",
    "    print(f\"üìà Dataset: routellm/gpt4_dataset (streamed from HuggingFace)\")\n",
    "    print(f\"üìä Sample Size: {analysis['total_tests']} conversations\")\n",
    "    print(f\"‚úÖ Success Rate: {analysis['success_rate']:.2%}\")\n",
    "    print(f\"‚ö° Average Response Time: {analysis['avg_execution_time']:.4f}s\")\n",
    "    print(f\"üåê Data Processing: 100% streamed (no local storage)\")\n",
    "    \n",
    "    print(\"\\nüèÜ Top Models Used by Service:\")\n",
    "    if analysis['model_usage']:\n",
    "        top_models = sorted(analysis['model_usage'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for i, (model, count) in enumerate(top_models, 1):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {i}. {model}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüîÑ Protocol Usage:\")\n",
    "    if analysis['protocol_usage']:\n",
    "        for protocol, count in sorted(analysis['protocol_usage'].items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {protocol}: {count} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüìã Task Distribution:\")\n",
    "    if analysis['task_distribution']:\n",
    "        top_tasks = sorted(analysis['task_distribution'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for i, (task, count) in enumerate(top_tasks, 1):\n",
    "            percentage = (count / analysis['successful_tests']) * 100\n",
    "            print(f\"  {i}. {task}: {count} instances ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No analysis data available\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üí° SERVICE TESTING RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üîß 1. Service Performance:\")\n",
    "print(\"   - Monitor response times under different loads\")\n",
    "print(\"   - Implement service health monitoring\")\n",
    "print(\"   - Add request queuing for high-traffic scenarios\")\n",
    "\n",
    "print(\"\\n‚ö° 2. Protocol Optimization:\")\n",
    "print(\"   - Analyze protocol selection patterns\")\n",
    "print(\"   - Optimize model selection algorithms\")\n",
    "print(\"   - Implement caching for frequently requested patterns\")\n",
    "\n",
    "print(\"\\nüìä 3. Monitoring & Analytics:\")\n",
    "print(\"   - Set up continuous testing with diverse datasets\")\n",
    "print(\"   - Track model performance over time\")\n",
    "print(\"   - Monitor service availability and response times\")\n",
    "\n",
    "print(\"\\nüåê 4. Scalability:\")\n",
    "print(\"   - Test with larger datasets\")\n",
    "print(\"   - Implement load balancing\")\n",
    "print(\"   - Consider distributed processing for high volume\")\n",
    "\n",
    "print(\"\\n‚úÖ ADAPTIVE AI SERVICE TESTING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"üéâ Service tested with real HuggingFace data\")\n",
    "print(\"üóëÔ∏è Zero local storage footprint achieved\")\n",
    "print(\"üöÄ Ready for production deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
