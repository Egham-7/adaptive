{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Backend API Testing\n",
    "\n",
    "This notebook allows direct testing of the Azure backend API endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU Testing Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMLU Complete Dataset Benchmarking with Adaptive Backend\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "\n",
    "\n",
    "def run_complete_mmlu_benchmark():\n",
    "    \"\"\"\n",
    "    Run MMLU benchmark against the adaptive backend on the COMPLETE dataset\n",
    "    Tests ALL 59 subjects with ALL questions (approximately 14,000+ questions)\n",
    "    \n",
    "    WARNING: This will take several hours to complete!\n",
    "    \"\"\"\n",
    "    print(\"=== COMPLETE MMLU DATASET BENCHMARKING ===\")\n",
    "    print(\"🚨 WARNING: This will test ~14,000 questions across 59 subjects!\")\n",
    "    print(\"⏰ Estimated time: 3-6 hours depending on response times\")\n",
    "    print(\"💰 Cost: Significant API usage costs\\n\")\n",
    "\n",
    "    # Confirmation\n",
    "    print(\"⚡ Starting in 10 seconds... (stop the cell if you want to cancel)\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Load MMLU dataset\n",
    "    print(\"📚 Loading complete MMLU dataset...\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "        test_data = dataset[\"test\"]\n",
    "        print(f\"✅ Loaded {len(test_data)} total test questions\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get all subjects\n",
    "    all_subjects = list(set(test_data[\"subject\"]))\n",
    "    all_subjects.sort()  # Sort for consistent order\n",
    "    print(f\"📖 Testing ALL {len(all_subjects)} subjects\")\n",
    "    print(f\"📋 Subjects: {', '.join(all_subjects[:10])}{'...' if len(all_subjects) > 10 else ''}\")\n",
    "\n",
    "    results = []\n",
    "    total_questions = 0\n",
    "    correct_answers = 0\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    if API_KEY != \"your-api-key\":\n",
    "        headers[\"X-Stainless-API-Key\"] = API_KEY\n",
    "\n",
    "    # Process each subject\n",
    "    for subject_idx, subject in enumerate(all_subjects, 1):\n",
    "        print(f\"\\n🔬 [{subject_idx}/{len(all_subjects)}] Testing subject: {subject}\")\n",
    "\n",
    "        # Filter questions for this subject\n",
    "        subject_questions = [q for q in test_data if q[\"subject\"] == subject]\n",
    "        subject_total = len(subject_questions)\n",
    "        subject_correct = 0\n",
    "\n",
    "        print(f\"   📊 {subject_total} questions in this subject\")\n",
    "\n",
    "        for i, question in enumerate(subject_questions):\n",
    "            if (i + 1) % 10 == 0 or i == 0:  # Progress indicator\n",
    "                print(f\"   Question {i+1}/{subject_total}...\", end=\" \")\n",
    "\n",
    "            # Format question for the API\n",
    "            choices = [question[\"choices\"][j] for j in range(len(question[\"choices\"]))]\n",
    "            question_text = f\"\"\"Question: {question['question']}\n",
    "            \n",
    "A) {choices[0]}\n",
    "B) {choices[1]}\n",
    "C) {choices[2]}\n",
    "D) {choices[3]}\n",
    "\n",
    "Please answer with only the letter (A, B, C, or D).\"\"\"\n",
    "\n",
    "            # Prepare API request with varied parameters for better testing\n",
    "            chat_data = {\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": question_text}],\n",
    "                \"max_tokens\": 10,\n",
    "                \"temperature\": 0.1,  # Low temperature for consistent answers\n",
    "                \"provider_constraint\": [\"openai\", \"deepseek\", \"anthropic\"],  # Multiple providers\n",
    "                \"cost_bias\": random.uniform(0.2, 0.8)  # Vary cost bias to test routing\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                request_start_time = time.time()\n",
    "                response = requests.post(\n",
    "                    f\"{BASE_URL}/v1/chat/completions\",\n",
    "                    json=chat_data,\n",
    "                    headers=headers,\n",
    "                    timeout=60  # Longer timeout for complete test\n",
    "                )\n",
    "                response_time = time.time() - request_start_time\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    ai_answer = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "                    # Extract adaptive backend selection info\n",
    "                    selected_model = result.get(\"model\", \"unknown\")\n",
    "                    selected_provider = result.get(\"provider\", \"unknown\")\n",
    "\n",
    "                    # Extract letter from AI response\n",
    "                    ai_letter = None\n",
    "                    for char in ai_answer.upper():\n",
    "                        if char in ['A', 'B', 'C', 'D']:\n",
    "                            ai_letter = char\n",
    "                            break\n",
    "\n",
    "                    # Convert correct answer index to letter\n",
    "                    correct_letter = ['A', 'B', 'C', 'D'][question[\"answer\"]]\n",
    "\n",
    "                    is_correct = ai_letter == correct_letter\n",
    "                    if is_correct:\n",
    "                        subject_correct += 1\n",
    "                        correct_answers += 1\n",
    "\n",
    "                    # Store result with detailed backend selection info\n",
    "                    results.append({\n",
    "                        \"subject\": subject,\n",
    "                        \"question_id\": f\"{subject}_{i}\",\n",
    "                        \"question\": question[\"question\"][:200] + \"...\",  # Longer preview\n",
    "                        \"correct_answer\": correct_letter,\n",
    "                        \"ai_answer\": ai_letter,\n",
    "                        \"ai_response_full\": ai_answer,\n",
    "                        \"is_correct\": is_correct,\n",
    "                        \"response_time\": response_time,\n",
    "                        \"selected_model\": selected_model,\n",
    "                        \"selected_provider\": selected_provider,\n",
    "                        \"model_provider_combo\": f\"{selected_provider}/{selected_model}\",\n",
    "                        \"completion_tokens\": result.get(\"usage\", {}).get(\"completion_tokens\", 0),\n",
    "                        \"prompt_tokens\": result.get(\"usage\", {}).get(\"prompt_tokens\", 0),\n",
    "                        \"total_tokens\": result.get(\"usage\", {}).get(\"total_tokens\", 0),\n",
    "                        \"cost_bias_used\": chat_data[\"cost_bias\"],\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    })\n",
    "\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        accuracy_so_far = (subject_correct / (i + 1)) * 100\n",
    "                        print(f\"✅ Accuracy: {accuracy_so_far:.1f}%\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"❌ API Error: {response.status_code}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Request failed: {str(e)}\")\n",
    "\n",
    "            total_questions += 1\n",
    "\n",
    "            # Save progress every 100 questions\n",
    "            if total_questions % 100 == 0:\n",
    "                temp_df = pd.DataFrame(results)\n",
    "                temp_filename = f\"mmlu_progress_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                temp_df.to_csv(temp_filename, index=False)\n",
    "\n",
    "                elapsed = datetime.now() - start_time\n",
    "                rate = total_questions / elapsed.total_seconds() * 60  # questions per minute\n",
    "                remaining = (len(test_data) - total_questions) / rate if rate > 0 else 0\n",
    "\n",
    "                print(f\"\\n💾 Progress saved: {total_questions}/{len(test_data)} questions\")\n",
    "                print(f\"⏱️  Rate: {rate:.1f} questions/min | ETA: {remaining:.0f} minutes\")\n",
    "\n",
    "            # Rate limiting\n",
    "            time.sleep(0.05)  # Small delay to avoid overwhelming the API\n",
    "\n",
    "        subject_accuracy = (subject_correct / subject_total) * 100\n",
    "        print(f\"   📊 {subject}: {subject_correct}/{subject_total} ({subject_accuracy:.1f}%)\")\n",
    "\n",
    "        # Save intermediate results after each subject\n",
    "        if results:\n",
    "            intermediate_df = pd.DataFrame(results)\n",
    "            intermediate_filename = f\"mmlu_intermediate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            intermediate_df.to_csv(intermediate_filename, index=False)\n",
    "\n",
    "    # Calculate final results\n",
    "    overall_accuracy = (correct_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "    total_time = datetime.now() - start_time\n",
    "\n",
    "    print(\"\\n🎯 FINAL COMPLETE DATASET RESULTS:\")\n",
    "    print(f\"📊 Overall Accuracy: {correct_answers}/{total_questions} ({overall_accuracy:.1f}%)\")\n",
    "    print(f\"⏱️  Total Time: {total_time}\")\n",
    "    print(f\"⚡ Average Response Time: {sum(r['response_time'] for r in results) / len(results):.2f}s\")\n",
    "\n",
    "    # Create comprehensive results DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if len(df) > 0:\n",
    "        print(\"\\n📈 COMPREHENSIVE PERFORMANCE ANALYSIS:\")\n",
    "\n",
    "        # Subject-wise performance\n",
    "        subject_stats = df.groupby('subject').agg({\n",
    "            'is_correct': ['count', 'sum', 'mean'],\n",
    "            'response_time': 'mean',\n",
    "            'selected_model': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'selected_provider': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'total_tokens': 'sum'\n",
    "        }).round(3)\n",
    "\n",
    "        subject_stats.columns = ['Questions', 'Correct', 'Accuracy', 'Avg_Time', 'Most_Used_Model', 'Most_Used_Provider', 'Total_Tokens']\n",
    "\n",
    "        # Show top and bottom performing subjects\n",
    "        subject_stats_sorted = subject_stats.sort_values('Accuracy', ascending=False)\n",
    "        print(\"\\n🏆 TOP 10 PERFORMING SUBJECTS:\")\n",
    "        print(subject_stats_sorted.head(10)[['Questions', 'Correct', 'Accuracy', 'Most_Used_Model']])\n",
    "\n",
    "        print(\"\\n📉 BOTTOM 10 PERFORMING SUBJECTS:\")\n",
    "        print(subject_stats_sorted.tail(10)[['Questions', 'Correct', 'Accuracy', 'Most_Used_Model']])\n",
    "\n",
    "        print(\"\\n🤖 COMPLETE ADAPTIVE BACKEND MODEL SELECTION ANALYSIS:\")\n",
    "        model_provider_usage = df['model_provider_combo'].value_counts()\n",
    "        for combo, count in model_provider_usage.items():\n",
    "            accuracy = df[df['model_provider_combo'] == combo]['is_correct'].mean() * 100\n",
    "            avg_time = df[df['model_provider_combo'] == combo]['response_time'].mean()\n",
    "            print(f\"  {combo}: {count:,} questions ({count/len(df)*100:.1f}%) - Accuracy: {accuracy:.1f}% - Avg Time: {avg_time:.2f}s\")\n",
    "\n",
    "        print(\"\\n💰 COMPLETE TOKEN USAGE ANALYSIS:\")\n",
    "        total_tokens_used = df['total_tokens'].sum()\n",
    "        avg_tokens_per_question = df['total_tokens'].mean()\n",
    "        print(f\"  Total tokens consumed: {total_tokens_used:,}\")\n",
    "        print(f\"  Average tokens per question: {avg_tokens_per_question:.1f}\")\n",
    "        print(f\"  Estimated API cost (rough): ${total_tokens_used * 0.000002:.2f}\")  # Rough estimate\n",
    "\n",
    "        provider_token_usage = df.groupby('selected_provider')['total_tokens'].agg(['sum', 'mean', 'count']).round(1)\n",
    "        print(\"\\n📊 TOKEN USAGE BY PROVIDER:\")\n",
    "        for provider in provider_token_usage.index:\n",
    "            total = provider_token_usage.loc[provider, 'sum']\n",
    "            avg = provider_token_usage.loc[provider, 'mean']\n",
    "            count = provider_token_usage.loc[provider, 'count']\n",
    "            print(f\"  {provider}: {total:,} total tokens ({avg:.1f} avg) across {count:,} questions\")\n",
    "\n",
    "        # Cost bias analysis\n",
    "        print(\"\\n💸 COST BIAS EFFECTIVENESS ANALYSIS:\")\n",
    "        df['cost_bias_bin'] = pd.cut(df['cost_bias_used'], bins=[0, 0.3, 0.7, 1.0], labels=['Low (0-0.3)', 'Med (0.3-0.7)', 'High (0.7-1.0)'])\n",
    "        cost_bias_analysis = df.groupby('cost_bias_bin').agg({\n",
    "            'selected_model': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'selected_provider': lambda x: x.mode().iloc[0] if not x.empty else 'unknown',\n",
    "            'is_correct': 'mean',\n",
    "            'total_tokens': 'mean'\n",
    "        }).round(3)\n",
    "        print(cost_bias_analysis)\n",
    "\n",
    "        # Save final comprehensive results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"mmlu_complete_benchmark_results_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "        # Save summary statistics\n",
    "        summary_filename = f\"mmlu_complete_summary_{timestamp}.csv\"\n",
    "        subject_stats.to_csv(summary_filename)\n",
    "\n",
    "        print(\"\\n💾 COMPLETE RESULTS SAVED:\")\n",
    "        print(f\"  📋 Detailed results: {filename}\")\n",
    "        print(f\"  📊 Subject summary: {summary_filename}\")\n",
    "\n",
    "        print(\"\\n🔄 ADAPTIVE ROUTING EFFECTIVENESS (COMPLETE ANALYSIS):\")\n",
    "        print(f\"  Total questions tested: {len(df):,}\")\n",
    "        print(f\"  Subjects covered: {len(df['subject'].unique())}\")\n",
    "        print(f\"  Unique model/provider combinations used: {len(model_provider_usage)}\")\n",
    "        print(f\"  Most frequently selected: {model_provider_usage.index[0] if len(model_provider_usage) > 0 else 'N/A'}\")\n",
    "        print(f\"  Routing diversity: {len(model_provider_usage)} different combinations\")\n",
    "\n",
    "        # Performance by subject category analysis\n",
    "        print(\"\\n📚 SUBJECT CATEGORY ANALYSIS:\")\n",
    "        # Group subjects by domain (rough categorization)\n",
    "        science_subjects = [s for s in df['subject'].unique() if any(term in s.lower() for term in ['physics', 'chemistry', 'biology', 'astronomy', 'anatomy'])]\n",
    "        math_subjects = [s for s in df['subject'].unique() if any(term in s.lower() for term in ['math', 'statistics', 'calculus', 'algebra', 'geometry'])]\n",
    "        humanities_subjects = [s for s in df['subject'].unique() if any(term in s.lower() for term in ['history', 'philosophy', 'literature', 'religion'])]\n",
    "\n",
    "        for category, subjects in [('Science', science_subjects), ('Mathematics', math_subjects), ('Humanities', humanities_subjects)]:\n",
    "            if subjects:\n",
    "                category_df = df[df['subject'].isin(subjects)]\n",
    "                if len(category_df) > 0:\n",
    "                    category_accuracy = category_df['is_correct'].mean() * 100\n",
    "                    print(f\"  {category}: {category_accuracy:.1f}% accuracy across {len(subjects)} subjects\")\n",
    "\n",
    "    print(\"\\n✨ COMPLETE MMLU BENCHMARK FINISHED!\")\n",
    "    print(f\"🏁 Total time: {total_time}\")\n",
    "    print(f\"📊 Overall performance: {overall_accuracy:.1f}% accuracy\")\n",
    "    return df\n",
    "\n",
    "# Run the complete benchmark\n",
    "print(\"🚀 Starting COMPLETE MMLU dataset benchmark...\")\n",
    "print(\"📋 This will test your adaptive backend against ALL ~14,000 academic questions\")\n",
    "print(\"🔍 Comprehensive analysis of model selection across all 59 subjects\")\n",
    "print(\"⚠️  WARNING: This will take several hours and consume significant API credits!\\n\")\n",
    "\n",
    "# Run the complete benchmark\n",
    "complete_benchmark_results = run_complete_mmlu_benchmark()\n",
    "\n",
    "print(\"\\n🎉 COMPLETE BENCHMARK ANALYSIS FINISHED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL SELECTION COMPARISON TEST ===\n",
      "🧪 Test Prompt: 'Explain quantum computing in simple terms.'\n",
      "\n",
      "🔍 Testing Go Backend: /v1/chat/completions\n",
      "✅ Status: 200 | Time: 5.40s\n",
      "📄 Final Selection: openai/gpt-4o-mini-2024-07-18\n",
      "\n",
      "==================================================\n",
      "🔍 COMPARISON ANALYSIS:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m python_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m go_model \u001b[38;5;241m=\u001b[39m go_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m python_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     python_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpython_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminion\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m{})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpython_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminion\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m{})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m python_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_URL = \"https://backend-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "PYTHON_SERVICE_URL = \"https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io\"\n",
    "\n",
    "def test_service(url, endpoint, data, service_name):\n",
    "    \"\"\"Test a service and return the result\"\"\"\n",
    "    full_url = f\"{url.rstrip('/')}/{endpoint.lstrip('/')}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    print(f\"\\n🔍 Testing {service_name}: {endpoint}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        response = requests.post(full_url, headers=headers, json=data, timeout=15)\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        print(f\"✅ Status: {response.status_code} | Time: {response_time:.2f}s\")\n",
    "        result = response.json()\n",
    "\n",
    "        # Extract key info\n",
    "        if service_name == \"Python AI Service\":\n",
    "            protocol = result.get(\"protocol\", \"unknown\")\n",
    "            model_info = result.get(\"minion\", {}).get(\"model\") or result.get(\"standard\", {}).get(\"model\")\n",
    "            provider_info = result.get(\"minion\", {}).get(\"provider\") or result.get(\"standard\", {}).get(\"provider\")\n",
    "            print(f\"📄 Protocol: {protocol} | Model: {provider_info}/{model_info}\")\n",
    "        else:  # Go Backend\n",
    "            model = result.get(\"model\", \"unknown\")\n",
    "            provider = result.get(\"provider\", \"unknown\")\n",
    "            print(f\"📄 Final Selection: {provider}/{model}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "print(\"=== MODEL SELECTION COMPARISON TEST ===\")\n",
    "print(f\"🧪 Test Prompt: '{test_prompt}'\")\n",
    "\n",
    "# Test Python AI Service directly\n",
    "python_data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": test_prompt}],\n",
    "    \"provider_constraint\": [\"openai\", \"deepseek\"],\n",
    "    \"cost_bias\": 0.8\n",
    "}\n",
    "\n",
    "#python_result = test_service(PYTHON_SERVICE_URL, \"/predict\", python_data, \"Python AI Service\")\n",
    "python_result = None\n",
    "# Test Go Backend (which should call Python service internally)\n",
    "go_data = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": test_prompt}],\n",
    "    \"provider_constraint\": [\"openai\", \"deepseek\"],\n",
    "    \"cost_bias\": 0.8\n",
    "}\n",
    "\n",
    "go_result = test_service(BASE_URL, \"/v1/chat/completions\", go_data, \"Go Backend\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"🔍 COMPARISON ANALYSIS:\")\n",
    "\n",
    "if python_result or go_result:\n",
    "    # Extract models for comparison\n",
    "    python_model = None\n",
    "    go_model = go_result.get(\"model\")\n",
    "\n",
    "    if python_result.get(\"protocol\") == \"minion\":\n",
    "        python_model = f\"{python_result.get('minion', {}).get('provider')}/{python_result.get('minion', {}).get('model')}\"\n",
    "    elif python_result.get(\"protocol\") == \"standard\":\n",
    "        python_model = f\"{python_result.get('standard', {}).get('provider')}/{python_result.get('standard', {}).get('model')}\"\n",
    "\n",
    "    go_model_full = f\"{go_result.get('provider')}/{go_result.get('model')}\"\n",
    "\n",
    "    print(f\"🐍 Python Service Suggests: {python_model}\")\n",
    "    print(f\"🔧 Go Backend Actually Used: {go_model_full}\")\n",
    "\n",
    "    if python_model and python_model.lower() in go_model_full.lower():\n",
    "        print(\"✅ MATCH: Go backend used Python service recommendation!\")\n",
    "    else:\n",
    "        print(\"⚠️  DIFFERENT: Go backend used different model (fallback or override)\")\n",
    "else:\n",
    "    print(\"❌ Cannot compare - one or both services failed\")\n",
    "\n",
    "print(\"\\n✨ Comparison test complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
