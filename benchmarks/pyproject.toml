[project]
name = "adaptive-benchmarks"
version = "0.1.0"
description = "Benchmark tests and evaluations for the Adaptive project"
authors = [{ name = "Adaptive Team", email = "team@adaptive.com" }]
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
  "pandas>=2.0.0,<3.0.0",
  "numpy>=1.24.0,<2.0.0",
  "matplotlib>=3.7.0,<4.0.0",
  "requests>=2.31.0,<3.0.0",
  "tqdm>=4.65.0,<5.0.0",
  "pydantic>=2.0.0,<3.0.0",
  "httpx>=0.24.0,<1.0.0",
  "aiohttp>=3.8.0,<4.0.0",
  "jsonlines>=3.1.0,<4.0.0",
  "scikit-learn>=1.3.0,<2.0.0",
  "transformers>=4.30.0,<5.0.0",
  "torch>=2.0.0,<3.0.0",
  "datasets>=2.14.0,<3.0.0",
  "seaborn>=0.12.0,<1.0.0"
]

[project.optional-dependencies]
dev = [
  "pytest>=8.0.0,<9.0.0",
  "pytest-cov>=4.0.0,<5.0.0",
  "pytest-asyncio>=0.21.0,<1.0.0",
  "pytest-benchmark>=4.0.0,<5.0.0",
  "mypy>=1.15.0,<2.0.0",
  "black>=25.0.0,<26.0.0",
  "ruff>=0.11.0,<0.12.0",
  "types-requests>=2.31.0,<3.0.0",
  "pandas-stubs>=2.0.0,<3.0.0",
  "types-tqdm>=4.65.0,<5.0.0",
  "types-seaborn>=0.12.0,<1.0.0"
]

[build-system]
requires = ["poetry-core>=2.0.0,<3.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "adaptive-benchmarks"
version = "0.1.0"
description = "Benchmark tests and evaluations for the Adaptive project"
authors = ["Adaptive Team <team@adaptive.com>"]
readme = "README.md"
packages = [
  {include = "arc_easy"},
  {include = "arc_hard"},
  {include = "fullstackbench"},
  {include = "test_complexity"}
]

[tool.poetry.dependencies]
python = "^3.12"
pandas = "^2.0.0"
numpy = "^1.24.0"
matplotlib = "^3.7.0"
requests = "^2.31.0"
tqdm = "^4.65.0"
pydantic = "^2.0.0"
httpx = "^0.24.0"
aiohttp = "^3.8.0"
jsonlines = "^3.1.0"
scikit-learn = "^1.3.0"
transformers = "^4.30.0"
torch = "^2.0.0"
datasets = "^2.14.0"
seaborn = "^0.12.0"

[tool.poetry.group.dev.dependencies]
pytest = "^8.0.0"
pytest-cov = "^4.0.0"
pytest-asyncio = "^0.21.0"
pytest-benchmark = "^4.0.0"
mypy = "^1.15.0"
black = "^25.0.0"
ruff = "^0.11.0"
types-requests = "^2.31.0"
pandas-stubs = "^2.0.0"
types-tqdm = "^4.65.0"
types-seaborn = "^0.12.0"

[tool.poetry.scripts]
run-arc-easy = "arc_easy:main"
run-arc-hard = "arc_hard:main"
run-fullstack-bench = "fullstackbench:main"
run-complexity-test = "test_complexity:main"

[tool.mypy]
python_version = "3.12"
warn_return_any = false
warn_unused_configs = true
disallow_untyped_defs = false
disallow_incomplete_defs = false
check_untyped_defs = false
disallow_untyped_decorators = false
no_implicit_optional = false
warn_redundant_casts = true
warn_unused_ignores = false
warn_no_return = false
warn_unreachable = false
strict_equality = false

[[tool.mypy.overrides]]
module = [
    "arc_easy.*",
    "arc_hard.*", 
    "fullstackbench.*",
    "test_complexity.*",
    "final_prompt_code_generation"
]
ignore_errors = true

[tool.black]
line-length = 88
target-version = ['py312']
include = '\.pyi?$'
extend-exclude = '''
/(
  \.git
  | \.mypy_cache
  | \.pytest_cache
  | \.venv
  | build
  | dist
)/
'''

[tool.ruff]
target-version = "py312"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py", "benchmark_*.py"]
python_classes = ["Test*", "Benchmark*"]
python_functions = ["test_*", "benchmark_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--cov=.",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
    "--benchmark-disable"  # Disable benchmarks by default, enable with --benchmark-enable
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "benchmark: marks tests as benchmarks",
    "arc: marks tests related to ARC datasets",
    "fullstack: marks tests related to fullstack benchmarks"
]