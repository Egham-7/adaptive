import litserve as ls  # type:ignore
from minion_service.model_manager import ModelManager, ModelManagerConfig
import time
from vllm import SamplingParams


class VLLMOpenAIAPI(ls.LitAPI):
    def setup(self, device: str) -> None:
        supported_models = [
            "meta-llama/Meta-Llama-3-8B-Instruct",  # SENSITIVE_SUBJECTS - Most reliable
            "google/codegemma-7b-it",  # COMPUTERS_AND_ELECTRONICS - Google official
            "Qwen/Qwen2.5-7B-Instruct",  # NEWS/BUSINESS/GENERAL - Extensively tested
            "Qwen/Qwen2.5-Math-7B-Instruct",  # FINANCE/SCIENCE - Math specialist
            "microsoft/Phi-4-mini-reasoning",  # LAW_AND_GOVERNMENT - Reasoning specialist
            "HuggingFaceTB/SmolLM2-1.7B-Instruct",  # JOBS_AND_EDUCATION - Education focused
        ]

        # Configure model manager with memory reserve and circuit breaker
        config = ModelManagerConfig(
            memory_reserve_gb=2.0,  # Always keep 2GB free for system stability
        )
        self.model_manager = ModelManager(
            preload_models=supported_models, config=config
        )
        self.model_manager.set_logger_callback(lambda key, value: self.log(key, value))

    async def predict(self, prompt, context):
        """Process chat completion request with batching support.

        Args:
            prompt: List of message dictionaries from ChatCompletionRequest
            context: Request context with OpenAI parameters injected automatically
        """
        start_time = time.perf_counter()

        self.log("prompt", prompt)
        model_name = context.get("model", "")

        if not model_name:
            raise ValueError("Model name is required")

        model_load_start = time.perf_counter()
        llm = await self.model_manager.get_model(model_name)
        model_load_time = time.perf_counter() - model_load_start
        self.log("model_load_time", model_load_time)

        self.log("request_model", model_name)
        self.log("request_temperature", context.get("temperature", 0.7))
        self.log("request_max_tokens", context.get("max_tokens", 512))

        # Create sampling parameters for vLLM
        sampling_params = SamplingParams(
            max_tokens=context.get("max_tokens", 512),
            temperature=context.get("temperature", 0.7),
            top_p=context.get("top_p", 1.0),
        )

        # Generate response using vLLM chat interface
        inference_start = time.perf_counter()
        messages_dict = [msg.model_dump() for msg in prompt.messages]
        outputs = llm.chat(messages_dict, sampling_params, use_tqdm=False)
        inference_time = time.perf_counter() - inference_start
        self.log("inference_time", inference_time)

        total_tokens = 0
        if outputs and len(outputs) > 0 and outputs[0].outputs:
            generated_text = outputs[0].outputs[0].text.strip()
            total_tokens = len(generated_text.split())
        else:
            generated_text = ""

        if generated_text:
            self.log("generated_tokens", total_tokens)
            self.log("debug", f"Generated text: {repr(generated_text)}")

            # Stream the response word by word
            for word in generated_text.split():
                yield word + " "
        else:
            self.log("warning", "No text generated by model")
            yield "I apologize, but I couldn't generate a response."

        total_time = time.perf_counter() - start_time
        self.log("total_request_time", total_time)
        if total_tokens > 0:
            self.log("tokens_per_second", total_tokens / inference_time)
