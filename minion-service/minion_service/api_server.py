import time
import uuid
from typing import Optional

import litserve as ls  # type:ignore
from openai.types.chat import ChatCompletionChunk, ChatCompletionMessage
from openai.types.chat.chat_completion_chunk import Choice, ChoiceDelta
from openai.types import CompletionUsage
from vllm import SamplingParams

from minion_service.model_manager import ModelManager, ModelManagerConfig


class VLLMOpenAIAPI(ls.LitAPI):
    def setup(self, device: str) -> None:
        supported_models = [
            "meta-llama/Meta-Llama-3-8B-Instruct",  # SENSITIVE_SUBJECTS
            "google/codegemma-7b-it",  # COMPUTERS_AND_ELECTRONICS
            "Qwen/Qwen2.5-7B-Instruct",  # NEWS/BUSINESS/GENERAL + JOBS_AND_EDUCATION
            "microsoft/Phi-4-mini-reasoning",  # LAW_AND_GOVERNMENT + FINANCE/SCIENCE
        ]

        # Configure model manager with GPU memory management and circuit breaker
        # Only enable GPU memory management if CUDA is available
        try:
            import torch

            gpu_available = torch.cuda.is_available() and torch.cuda.device_count() > 0
        except ImportError:
            gpu_available = False

        self.log("gpu_available", gpu_available)

        config = ModelManagerConfig(
            gpu_memory_reserve_gb=2.0, enable_gpu_memory_management=gpu_available
        )
        self.model_manager = ModelManager(config=config)
        self.model_manager.set_logger_callback(
            lambda key, value: self.log(key, value))

        # Preload models synchronously (sequential to avoid CUDA conflicts)
        self.model_manager.preload_models_sync(supported_models)

    async def predict(self, prompt, context):
        """Process chat completion request with batching support.

        Args:
            prompt: List of message dictionaries from ChatCompletionRequest
            context: Request context with OpenAI parameters injected automatically
        """
        start_time = time.perf_counter()

        self.log("prompt", prompt)
        model_name = context.get("model", "")

        if not model_name:
            raise ValueError("Model name is required")

        model_load_start = time.perf_counter()
        llm = await self.model_manager.get_model(model_name)
        model_load_time = time.perf_counter() - model_load_start
        self.log("model_load_time", model_load_time)

        self.log("request_model", model_name)
        self.log("request_temperature", context.get("temperature", 0.7))
        self.log("request_max_tokens", context.get("max_tokens", 512))

        # Create sampling parameters for vLLM
        sampling_params = SamplingParams(
            max_tokens=context.get("max_tokens", 512),
            temperature=context.get("temperature", 0.7),
            top_p=context.get("top_p", 1.0),
        )

        # Generate response using vLLM chat interface
        inference_start = time.perf_counter()
        messages_dict = [msg.model_dump() for msg in prompt.messages]
        outputs = llm.chat(messages_dict, sampling_params, use_tqdm=False)
        inference_time = time.perf_counter() - inference_start
        self.log("inference_time", inference_time)

        total_tokens = 0
        prompt_tokens = 0
        completion_tokens = 0

        if outputs and len(outputs) > 0 and outputs[0].outputs:
            generated_text = outputs[0].outputs[0].text.strip()
            completion_tokens = len(generated_text.split())
            # Estimate prompt tokens (rough approximation)
            prompt_text = " ".join(
                [msg.content for msg in prompt.messages if hasattr(
                    msg, "content")]
            )
            prompt_tokens = len(prompt_text.split())
            total_tokens = prompt_tokens + completion_tokens
        else:
            generated_text = ""

        if generated_text:
            self.log("generated_tokens", completion_tokens)
            self.log("debug", f"Generated text: {repr(generated_text)}")

            # Stream the response word by word
            for word in generated_text.split():
                yield word + " "
        else:
            self.log("warning", "No text generated by model")
            yield "I apologize, but I couldn't generate a response."
            completion_tokens = 8  # Approximate tokens for fallback message

        # Store usage statistics in context for encode_response
        context["usage"] = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "completion_tokens_details": {
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0,
            },
        }

        total_time = time.perf_counter() - start_time
        self.log("total_request_time", total_time)
        if completion_tokens > 0:
            self.log("tokens_per_second", completion_tokens / inference_time)

    def encode_response(self, output_stream, context):
        """Custom encode_response to emit usage statistics as final chunk."""
        chunk_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        created = int(time.time())
        model = context.get("model", "unknown")

        # Yield all content chunks first
        for output in output_stream:
            chunk = ChatCompletionChunk(
                id=chunk_id,
                object="chat.completion.chunk",
                created=created,
                model=model,
                choices=[
                    Choice(
                        index=0,
                        delta=ChoiceDelta(content=output, role="assistant"),
                        finish_reason=None,
                    )
                ],
            )
            yield chunk.model_dump()

        # Emit final chunk with finish_reason and usage
        if "usage" in context:
            usage = CompletionUsage(**context["usage"])
            final_chunk = ChatCompletionChunk(
                id=chunk_id,
                object="chat.completion.chunk",
                created=created,
                model=model,
                choices=[Choice(index=0, delta=ChoiceDelta(),
                                finish_reason="stop")],
                usage=usage,
            )
            yield final_chunk.model_dump()
