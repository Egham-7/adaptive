import litserve as ls  # type:ignore
from minion_service.model_manager import ModelManager
import time
from vllm import SamplingParams


class VLLMOpenAIAPI(ls.LitAPI):
    def setup(self, device: str) -> None:
        supported_models = [
            "Trelis/Llama-2-7b-chat-hf-function-calling-v2",
            "Qwen/Qwen2.5-14B-Instruct",  # BUSINESS_AND_INDUSTRIAL/HEALTH
            "Qwen/Qwen2.5-7B-Instruct",  # NEWS / OTHERDOMAINS / REAL_ESTATE
            # COMPUTERS_AND_ELECTRONICS/INTERNET_AND_TELECOM
            "codellama/CodeLlama-7b-Instruct-hf",
            "Qwen/Qwen2.5-Math-7B-Instruct",  # FINANCE/SCIENCE
            "HuggingFaceTB/SmolLM2-1.7B-Instruct",  # JOBS_AND_EDUCATION
            "microsoft/Phi-4-mini-reasoning",  # LAW_AND_GOVERNMENT
            "meta-llama/Meta-Llama-3-8B-Instruct",  # SENSITIVE_SUBJECTS
        ]

        # Auto-unload models after 30 minutes of inactivity with memory management
        self.model_manager = ModelManager(
            preload_models=supported_models,
            inactivity_timeout_minutes=30,
            memory_threshold_percent=85.0,
            memory_reserve_gb=2.0,
        )
        self.model_manager.set_logger_callback(lambda key, value: self.log(key, value))

    def predict(self, prompt, context):
        """Process chat completion request with batching support.

        Args:
            prompt: List of message dictionaries from ChatCompletionRequest
            context: Request context with OpenAI parameters injected automatically
        """
        start_time = time.perf_counter()

        self.log("prompt", prompt)
        model_name = context.get("model", "")

        if not model_name:
            raise ValueError("Model name is required")

        model_load_start = time.perf_counter()
        llm = self.model_manager.get_model(model_name)
        model_load_time = time.perf_counter() - model_load_start
        self.log("model_load_time", model_load_time)

        self.log("request_model", model_name)
        self.log("request_temperature", context.get("temperature", 0.7))
        self.log("request_max_tokens", context.get("max_tokens", 512))

        # Create sampling parameters for vLLM
        sampling_params = SamplingParams(
            max_tokens=context.get("max_tokens", 512),
            temperature=context.get("temperature", 0.7),
            top_p=context.get("top_p", 1.0),
        )

        # Generate response using vLLM chat interface
        inference_start = time.perf_counter()
        outputs = llm.chat(prompt.messages, sampling_params, use_tqdm=False)
        inference_time = time.perf_counter() - inference_start
        self.log("inference_time", inference_time)

        total_tokens = 0
        if outputs and len(outputs) > 0 and outputs[0].outputs:
            generated_text = outputs[0].outputs[0].text.strip()
            total_tokens = len(generated_text.split())
        else:
            generated_text = ""

        if generated_text:
            self.log("generated_tokens", total_tokens)
            self.log("debug", f"Generated text: {repr(generated_text)}")

            # Stream the response word by word
            for word in generated_text.split():
                yield word + " "
        else:
            self.log("warning", "No text generated by model")
            yield "I apologize, but I couldn't generate a response."

        total_time = time.perf_counter() - start_time
        self.log("total_request_time", total_time)
        if total_tokens > 0:
            self.log("tokens_per_second", total_tokens / inference_time)
