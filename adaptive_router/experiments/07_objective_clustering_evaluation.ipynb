{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üéØ Objective Clustering Evaluation\n",
    "\n",
    "**Problem Identified in Notebook 06:**\n",
    "- K=2-3 gives highest silhouette (0.595-0.604)\n",
    "- But these clusters are **dominated by dataset sources** (99% in one cluster)\n",
    "- Silhouette score is **misleading** - measures geometric separation, not routing utility\n",
    "\n",
    "## üî¨ Goal: Find Optimal K Using Objective Metrics\n",
    "\n",
    "We'll use **quantifiable metrics** instead of heuristic labels:\n",
    "\n",
    "### 1. Dataset Mixing Entropy ü•á (Most Important)\n",
    "- **What**: Measures if clusters are just separating datasets\n",
    "- **Target**: > 1.0 (higher = better mixing)\n",
    "- **Why**: Clusters should group by task properties, not dataset source\n",
    "\n",
    "### 2. Cluster Balance (Gini Coefficient) ü•â\n",
    "- **What**: Measures inequality in cluster sizes\n",
    "- **Target**: < 0.3 (lower = more balanced)\n",
    "- **Why**: Need enough samples per cluster for error rate estimation\n",
    "\n",
    "### 3. Prompt Length Variance üìè\n",
    "- **What**: Variance of prompt lengths within clusters\n",
    "- **Target**: Lower is better (clusters group similar complexity)\n",
    "- **Why**: Prompt length correlates with task complexity\n",
    "\n",
    "### 4. Combined Objective Score üéØ\n",
    "- Weighted combination of all metrics\n",
    "- Finds K that maximizes routing utility, not just geometric separation\n",
    "\n",
    "---\n",
    "**‚ö° GPU recommended for embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 0. üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages with version compatibility for Colab T4 GPU\n",
    "# Colab now uses NumPy 2.x as default for many packages\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# Use NumPy 2.x to avoid conflicts with Colab pre-installed packages\n",
    "!pip install -q 'numpy>=2.0.0'\n",
    "\n",
    "# Core ML packages - latest versions compatible with NumPy 2.x\n",
    "!pip install -q 'transformers>=4.40.0'\n",
    "!pip install -q 'datasets>=2.18.0'\n",
    "!pip install -q 'scikit-learn>=1.4.0'\n",
    "!pip install -q 'scipy>=1.12.0'\n",
    "\n",
    "# Visualization packages\n",
    "!pip install -q 'matplotlib>=3.8.0'\n",
    "!pip install -q 'seaborn>=0.13.0'\n",
    "!pip install -q 'pandas>=2.2.0'\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sklearn\n",
    "\n",
    "print(f'‚úÖ Packages installed!')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Transformers version: {transformers.__version__}')\n",
    "print(f'scikit-learn version: {sklearn.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA version: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f'Memory: {props.total_memory / 1e9:.1f} GB')\n",
    "    print(f'Compute Capability: {props.major}.{props.minor}')\n",
    "    \n",
    "    # T4 GPU specific optimizations\n",
    "    if 'T4' in torch.cuda.get_device_name(0):\n",
    "        print('‚úÖ T4 GPU detected - optimized for mixed precision training')\n",
    "        print('   Recommendation: Use batch_size=32-64 for best performance')\n",
    "    \n",
    "    # Clear cache to start fresh\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Available memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('üíª CPU mode')\n",
    "    print('   üí° Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. üì¶ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Embeddings\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Viz\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ Imports complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. üì• Load Coding Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coding_datasets(max_total=4000):\n",
    "    \"\"\"\n",
    "    Load diverse coding datasets with source tracking.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING CODING DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. SWE-bench\n",
    "    print(\"\\n1. Loading SWE-bench (GitHub issues)...\")\n",
    "    try:\n",
    "        swe_dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"test\")\n",
    "        count = 0\n",
    "        target = min(2000, len(swe_dataset))\n",
    "\n",
    "        for idx, item in enumerate(swe_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            problem = item.get(\"problem_statement\", \"\")\n",
    "            repo = item.get(\"repo\", \"\")\n",
    "\n",
    "            if problem:\n",
    "                questions.append({\n",
    "                    \"question\": problem,\n",
    "                    \"source\": \"swe_bench\",\n",
    "                    \"repo\": repo,\n",
    "                    \"prompt_length\": len(problem),\n",
    "                    \"word_count\": len(problem.split())\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} GitHub issues\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    # 2. DS-1000\n",
    "    print(\"\\n2. Loading DS-1000 (Data science tasks)...\")\n",
    "    try:\n",
    "        ds_dataset = load_dataset(\"xlangai/DS-1000\", split=\"test\")\n",
    "        count = 0\n",
    "\n",
    "        for item in ds_dataset:\n",
    "            prompt = item.get(\"prompt\", \"\")\n",
    "            metadata = item.get(\"metadata\", {})\n",
    "            library = metadata.get(\"library\", \"unknown\") if isinstance(metadata, dict) else \"unknown\"\n",
    "\n",
    "            if prompt:\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"source\": \"ds1000\",\n",
    "                    \"library\": library,\n",
    "                    \"prompt_length\": len(prompt),\n",
    "                    \"word_count\": len(prompt.split())\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} data science tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    # 3. BigCodeBench\n",
    "    print(\"\\n3. Loading BigCodeBench (API tasks)...\")\n",
    "    try:\n",
    "        bigcode_dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.2\")\n",
    "        count = 0\n",
    "        target = min(500, len(bigcode_dataset))\n",
    "\n",
    "        for idx, item in enumerate(bigcode_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            complete_prompt = item.get(\"complete_prompt\", \"\")\n",
    "            instruct_prompt = item.get(\"instruct_prompt\", \"\")\n",
    "            prompt = instruct_prompt if instruct_prompt else complete_prompt\n",
    "\n",
    "            if prompt:\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"source\": \"bigcodebench\",\n",
    "                    \"prompt_length\": len(prompt),\n",
    "                    \"word_count\": len(prompt.split())\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} API tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    # 4. DebugBench\n",
    "    print(\"\\n4. Loading DebugBench (Debugging tasks)...\")\n",
    "    try:\n",
    "        debug_dataset = load_dataset(\"Rtian/DebugBench\", split=\"test\")\n",
    "        count = 0\n",
    "        target = min(500, len(debug_dataset))\n",
    "\n",
    "        for idx, item in enumerate(debug_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            buggy_code = item.get(\"buggy_code\", \"\")\n",
    "            language = item.get(\"language\", \"python\").lower()\n",
    "\n",
    "            if buggy_code:\n",
    "                prompt = f\"Debug this code:\\n{buggy_code}\"\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"source\": \"debugbench\",\n",
    "                    \"language\": language,\n",
    "                    \"prompt_length\": len(prompt),\n",
    "                    \"word_count\": len(prompt.split())\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} debugging tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ Total: {len(questions)} coding tasks\")\n",
    "    print(f\"\\nDataset Distribution:\")\n",
    "    sources = [q['source'] for q in questions]\n",
    "    source_counts = Counter(sources)\n",
    "    for source, count in source_counts.most_common():\n",
    "        pct = count / len(questions) * 100\n",
    "        print(f\"  {source}: {count} ({pct:.1f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Load data\n",
    "questions = load_coding_datasets(max_total=4000)\n",
    "texts = [q['question'] for q in questions]\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df = pd.DataFrame(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-header",
   "metadata": {},
   "source": [
    "## 3. üß† Extract CodeBERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"Mean pooling - take average of all tokens\"\"\"\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def encode_with_codebert(model, tokenizer, texts, device, batch_size=32):\n",
    "    \"\"\"Encode texts using CodeBERT\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    print(f'üöÄ Encoding {len(texts)} texts with CodeBERT...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**encoded)\n",
    "            embeddings = mean_pooling(outputs.last_hidden_state, encoded['attention_mask'])\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            \n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f'  Processed {min(i+batch_size, len(texts))}/{len(texts)}', end='\\r')\n",
    "    \n",
    "    print(f'  Processed {len(texts)}/{len(texts)} ‚úì')\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CODEBERT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "model = AutoModel.from_pretrained('microsoft/codebert-base').to(device)\n",
    "\n",
    "codebert_embeddings = encode_with_codebert(model, tokenizer, texts, device, batch_size=32)\n",
    "codebert_norm = normalize(codebert_embeddings, norm='l2')\n",
    "\n",
    "print(f\"\\n‚úÖ CodeBERT embeddings: {codebert_norm.shape}\")\n",
    "print(f\"   Mean: {codebert_norm.mean():.4f}, Std: {codebert_norm.std():.4f}\")\n",
    "\n",
    "# Free memory\n",
    "del model\n",
    "del tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "## 4. üìä Define Objective Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_mixing_entropy(cluster_labels, dataset_sources):\n",
    "    \"\"\"\n",
    "    Measures how well datasets are mixed within clusters.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_entropy: Average entropy across all clusters (higher = better mixing)\n",
    "    - max_dataset_pct: Average maximum dataset percentage per cluster (lower = better)\n",
    "    - per_cluster_stats: Detailed stats for each cluster\n",
    "    \"\"\"\n",
    "    cluster_stats = []\n",
    "    \n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        mask = cluster_labels == cluster_id\n",
    "        cluster_sources = dataset_sources[mask]\n",
    "        \n",
    "        # Count datasets in this cluster\n",
    "        source_counts = Counter(cluster_sources)\n",
    "        total = sum(source_counts.values())\n",
    "        \n",
    "        # Calculate entropy (higher = more diverse)\n",
    "        probs = np.array([count/total for count in source_counts.values()])\n",
    "        cluster_entropy = entropy(probs)\n",
    "        \n",
    "        # Max percentage (lower = more balanced)\n",
    "        max_pct = max(source_counts.values()) / total\n",
    "        \n",
    "        cluster_stats.append({\n",
    "            'cluster': cluster_id,\n",
    "            'size': total,\n",
    "            'entropy': cluster_entropy,\n",
    "            'max_dataset_pct': max_pct,\n",
    "            'dominant_dataset': source_counts.most_common(1)[0][0],\n",
    "            'num_datasets': len(source_counts)\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(cluster_stats)\n",
    "    \n",
    "    return {\n",
    "        'avg_entropy': stats_df['entropy'].mean(),\n",
    "        'avg_max_dataset_pct': stats_df['max_dataset_pct'].mean(),\n",
    "        'per_cluster_stats': stats_df\n",
    "    }\n",
    "\n",
    "def gini_coefficient(cluster_sizes):\n",
    "    \"\"\"\n",
    "    Measures inequality in cluster sizes.\n",
    "    \n",
    "    Returns:\n",
    "    - 0.0: Perfect equality (all clusters same size)\n",
    "    - 1.0: Maximum inequality (all samples in one cluster)\n",
    "    \"\"\"\n",
    "    cluster_sizes = np.array(sorted(cluster_sizes))\n",
    "    n = len(cluster_sizes)\n",
    "    index = np.arange(1, n + 1)\n",
    "    return (2 * np.sum(index * cluster_sizes)) / (n * np.sum(cluster_sizes)) - (n + 1) / n\n",
    "\n",
    "def prompt_length_variance(cluster_labels, prompt_lengths):\n",
    "    \"\"\"\n",
    "    Measures variance of prompt lengths within clusters.\n",
    "    Lower = clusters group similar-length prompts (likely similar complexity)\n",
    "    \"\"\"\n",
    "    total_variance = 0\n",
    "    \n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        mask = cluster_labels == cluster_id\n",
    "        cluster_lengths = prompt_lengths[mask]\n",
    "        \n",
    "        # Weighted variance\n",
    "        total_variance += np.var(cluster_lengths) * len(cluster_lengths)\n",
    "    \n",
    "    return total_variance / len(cluster_labels)\n",
    "\n",
    "def word_count_variance(cluster_labels, word_counts):\n",
    "    \"\"\"\n",
    "    Measures variance of word counts within clusters.\n",
    "    Lower = clusters group similar-complexity prompts\n",
    "    \"\"\"\n",
    "    total_variance = 0\n",
    "    \n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        mask = cluster_labels == cluster_id\n",
    "        cluster_words = word_counts[mask]\n",
    "        \n",
    "        # Weighted variance\n",
    "        total_variance += np.var(cluster_words) * len(cluster_words)\n",
    "    \n",
    "    return total_variance / len(cluster_labels)\n",
    "\n",
    "def compute_objective_score(cluster_labels, dataset_sources, prompt_lengths, word_counts):\n",
    "    \"\"\"\n",
    "    Compute combined objective score for clustering quality.\n",
    "    \n",
    "    Higher score = better for routing!\n",
    "    \"\"\"\n",
    "    # 1. Dataset mixing entropy (weight: 40%)\n",
    "    mixing = dataset_mixing_entropy(cluster_labels, dataset_sources)\n",
    "    entropy_score = mixing['avg_entropy'] / 1.5  # Normalize (max ~1.5 for 4 datasets)\n",
    "    \n",
    "    # 2. Cluster balance (weight: 30%)\n",
    "    cluster_sizes = list(Counter(cluster_labels).values())\n",
    "    gini = gini_coefficient(cluster_sizes)\n",
    "    balance_score = 1 - gini  # Invert so higher is better\n",
    "    \n",
    "    # 3. Prompt length homogeneity (weight: 15%)\n",
    "    length_var = prompt_length_variance(cluster_labels, prompt_lengths)\n",
    "    # Normalize by overall variance\n",
    "    overall_length_var = np.var(prompt_lengths)\n",
    "    length_score = 1 - (length_var / overall_length_var)\n",
    "    \n",
    "    # 4. Word count homogeneity (weight: 15%)\n",
    "    word_var = word_count_variance(cluster_labels, word_counts)\n",
    "    overall_word_var = np.var(word_counts)\n",
    "    word_score = 1 - (word_var / overall_word_var)\n",
    "    \n",
    "    # Combined score (weighted average)\n",
    "    objective_score = (\n",
    "        0.40 * entropy_score +\n",
    "        0.30 * balance_score +\n",
    "        0.15 * length_score +\n",
    "        0.15 * word_score\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'objective_score': objective_score,\n",
    "        'entropy_score': entropy_score,\n",
    "        'balance_score': balance_score,\n",
    "        'length_score': length_score,\n",
    "        'word_score': word_score,\n",
    "        'avg_entropy': mixing['avg_entropy'],\n",
    "        'gini': gini,\n",
    "        'avg_max_dataset_pct': mixing['avg_max_dataset_pct']\n",
    "    }\n",
    "\n",
    "print('‚úÖ Objective metrics defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiments-header",
   "metadata": {},
   "source": [
    "## 5. üî¨ Comprehensive Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OBJECTIVE CLUSTERING EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing: complete, average, ward linkages\")\n",
    "print(\"K values: 2, 3, 4, 5, 6, 8, 10, 12, 15, 20, 25, 30\\n\")\n",
    "\n",
    "linkages = ['ward', 'complete', 'average']\n",
    "k_values = [2, 3, 4, 5, 6, 8, 10, 12, 15, 20, 25, 30]\n",
    "\n",
    "# Prepare data\n",
    "dataset_sources = df['source'].values\n",
    "prompt_lengths = df['prompt_length'].values\n",
    "word_counts = df['word_count'].values\n",
    "\n",
    "results = []\n",
    "\n",
    "for linkage_method in linkages:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing linkage: {linkage_method.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Fit clustering\n",
    "            agg = AgglomerativeClustering(\n",
    "                n_clusters=k,\n",
    "                linkage=linkage_method,\n",
    "                metric='euclidean' if linkage_method == 'ward' else 'cosine'\n",
    "            )\n",
    "            \n",
    "            labels = agg.fit_predict(codebert_norm)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Compute all metrics\n",
    "            obj_scores = compute_objective_score(\n",
    "                labels, dataset_sources, prompt_lengths, word_counts\n",
    "            )\n",
    "            \n",
    "            # Also compute silhouette for comparison\n",
    "            sil = silhouette_score(codebert_norm, labels, metric='cosine')\n",
    "            \n",
    "            # Cluster sizes\n",
    "            cluster_sizes = Counter(labels)\n",
    "            size_distribution = {f'cluster_{i}': cluster_sizes.get(i, 0) for i in range(k)}\n",
    "            \n",
    "            result = {\n",
    "                'linkage': linkage_method,\n",
    "                'k': k,\n",
    "                'objective_score': obj_scores['objective_score'],\n",
    "                'entropy_score': obj_scores['entropy_score'],\n",
    "                'balance_score': obj_scores['balance_score'],\n",
    "                'length_score': obj_scores['length_score'],\n",
    "                'word_score': obj_scores['word_score'],\n",
    "                'avg_entropy': obj_scores['avg_entropy'],\n",
    "                'gini': obj_scores['gini'],\n",
    "                'avg_max_dataset_pct': obj_scores['avg_max_dataset_pct'],\n",
    "                'silhouette': sil,\n",
    "                'time_sec': elapsed\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nK={k:2d}:\")\n",
    "            print(f\"  Objective Score: {obj_scores['objective_score']:.4f}\")\n",
    "            print(f\"  ‚îî‚îÄ Entropy:      {obj_scores['entropy_score']:.4f} (avg={obj_scores['avg_entropy']:.3f})\")\n",
    "            print(f\"  ‚îî‚îÄ Balance:      {obj_scores['balance_score']:.4f} (gini={obj_scores['gini']:.3f})\")\n",
    "            print(f\"  ‚îî‚îÄ Length:       {obj_scores['length_score']:.4f}\")\n",
    "            print(f\"  ‚îî‚îÄ Word:         {obj_scores['word_score']:.4f}\")\n",
    "            print(f\"  Silhouette:      {sil:.4f} (for reference)\")\n",
    "            print(f\"  Max Dataset %:   {obj_scores['avg_max_dataset_pct']:.1%}\")\n",
    "            print(f\"  Time:            {elapsed:.2f}s\")\n",
    "            \n",
    "            # Show if dataset-dominated\n",
    "            if obj_scores['avg_max_dataset_pct'] > 0.7:\n",
    "                print(f\"  ‚ö†Ô∏è  WARNING: Clusters dominated by single datasets!\")\n",
    "            elif obj_scores['avg_entropy'] > 1.0:\n",
    "                print(f\"  ‚úÖ GOOD: Datasets well-mixed across clusters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  K={k:2d}: Failed - {str(e)[:50]}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOP 10 BY OBJECTIVE SCORE\")\n",
    "print(f\"{'='*70}\")\n",
    "top_results = results_df.nlargest(10, 'objective_score')\n",
    "print(top_results[['linkage', 'k', 'objective_score', 'avg_entropy', 'gini', 'silhouette']])\n",
    "\n",
    "best_result = results_df.loc[results_df['objective_score'].idxmax()]\n",
    "print(f\"\\n‚úÖ BEST CONFIGURATION:\")\n",
    "print(f\"   Linkage: {best_result['linkage']}\")\n",
    "print(f\"   K: {int(best_result['k'])}\")\n",
    "print(f\"   Objective Score: {best_result['objective_score']:.4f}\")\n",
    "print(f\"   Avg Entropy: {best_result['avg_entropy']:.3f} {'‚úÖ' if best_result['avg_entropy'] > 1.0 else '‚ö†Ô∏è'}\")\n",
    "print(f\"   Gini: {best_result['gini']:.3f} {'‚úÖ' if best_result['gini'] < 0.3 else '‚ö†Ô∏è'}\")\n",
    "print(f\"   Silhouette: {best_result['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 6. üìä Objective Score vs Silhouette Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OBJECTIVE SCORE VS SILHOUETTE SCORE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best by silhouette\n",
    "best_by_silhouette = results_df.loc[results_df['silhouette'].idxmax()]\n",
    "best_by_objective = results_df.loc[results_df['objective_score'].idxmax()]\n",
    "\n",
    "print(f\"\\nüî∑ BEST BY SILHOUETTE (Traditional Metric):\")\n",
    "print(f\"   Config: {best_by_silhouette['linkage']} linkage, K={int(best_by_silhouette['k'])}\")\n",
    "print(f\"   Silhouette: {best_by_silhouette['silhouette']:.4f}\")\n",
    "print(f\"   Objective Score: {best_by_silhouette['objective_score']:.4f}\")\n",
    "print(f\"   Avg Entropy: {best_by_silhouette['avg_entropy']:.3f}\")\n",
    "print(f\"   Max Dataset %: {best_by_silhouette['avg_max_dataset_pct']:.1%}\")\n",
    "print(f\"   Gini: {best_by_silhouette['gini']:.3f}\")\n",
    "\n",
    "if best_by_silhouette['avg_max_dataset_pct'] > 0.7:\n",
    "    print(f\"   ‚ùå PROBLEM: Just separating datasets!\")\n",
    "\n",
    "print(f\"\\nüéØ BEST BY OBJECTIVE SCORE (Routing-Focused):\")\n",
    "print(f\"   Config: {best_by_objective['linkage']} linkage, K={int(best_by_objective['k'])}\")\n",
    "print(f\"   Objective Score: {best_by_objective['objective_score']:.4f}\")\n",
    "print(f\"   Avg Entropy: {best_by_objective['avg_entropy']:.3f}\")\n",
    "print(f\"   Max Dataset %: {best_by_objective['avg_max_dataset_pct']:.1%}\")\n",
    "print(f\"   Gini: {best_by_objective['gini']:.3f}\")\n",
    "print(f\"   Silhouette: {best_by_objective['silhouette']:.4f}\")\n",
    "\n",
    "if best_by_objective['avg_entropy'] > 1.0 and best_by_objective['gini'] < 0.3:\n",
    "    print(f\"   ‚úÖ GOOD: Datasets mixed, clusters balanced!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if abs(best_by_silhouette['k'] - best_by_objective['k']) > 2:\n",
    "    print(f\"‚ö†Ô∏è  Traditional silhouette score recommends K={int(best_by_silhouette['k'])}\")\n",
    "    print(f\"    But objective score shows K={int(best_by_objective['k'])} is better for routing!\")\n",
    "    print(f\"\\n    Why? Silhouette optimizes for geometric separation,\")\n",
    "    print(f\"    but we need dataset mixing and balanced clusters for routing.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Both metrics agree on K~{int(best_by_objective['k'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 7. üìä Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-heatmaps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for different metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for linkage_method in ['ward', 'complete', 'average']:\n",
    "    subset = results_df[results_df['linkage'] == linkage_method]\n",
    "    \n",
    "    # Objective Score\n",
    "    axes[0, 0].plot(subset['k'], subset['objective_score'], marker='o', label=linkage_method)\n",
    "    \n",
    "    # Entropy\n",
    "    axes[0, 1].plot(subset['k'], subset['avg_entropy'], marker='o', label=linkage_method)\n",
    "    \n",
    "    # Gini\n",
    "    axes[1, 0].plot(subset['k'], subset['gini'], marker='o', label=linkage_method)\n",
    "    \n",
    "    # Silhouette (for comparison)\n",
    "    axes[1, 1].plot(subset['k'], subset['silhouette'], marker='o', label=linkage_method)\n",
    "\n",
    "# Objective Score\n",
    "axes[0, 0].set_title('Objective Score (Higher = Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0, 0].set_ylabel('Objective Score')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "axes[0, 1].set_title('Dataset Mixing Entropy (Higher = Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Number of Clusters (K)')\n",
    "axes[0, 1].set_ylabel('Average Entropy')\n",
    "axes[0, 1].axhline(y=1.0, color='green', linestyle='--', alpha=0.7, label='Target (>1.0)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gini\n",
    "axes[1, 0].set_title('Cluster Balance (Lower = Better)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Number of Clusters (K)')\n",
    "axes[1, 0].set_ylabel('Gini Coefficient')\n",
    "axes[1, 0].axhline(y=0.3, color='green', linestyle='--', alpha=0.7, label='Target (<0.3)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette\n",
    "axes[1, 1].set_title('Silhouette Score (For Reference Only)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1, 1].set_ylabel('Silhouette Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Objective Score vs Silhouette Score\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for linkage_method in ['ward', 'complete', 'average']:\n",
    "    subset = results_df[results_df['linkage'] == linkage_method]\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        subset['silhouette'],\n",
    "        subset['objective_score'],\n",
    "        s=subset['k'] * 10,  # Size by K\n",
    "        alpha=0.6,\n",
    "        label=linkage_method\n",
    "    )\n",
    "    \n",
    "    # Annotate best by objective\n",
    "    best_idx = subset['objective_score'].idxmax()\n",
    "    best = subset.loc[best_idx]\n",
    "    ax.annotate(\n",
    "        f\"K={int(best['k'])}\",\n",
    "        (best['silhouette'], best['objective_score']),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=10,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Silhouette Score (Traditional Metric)', fontsize=12)\n",
    "ax.set_ylabel('Objective Score (Routing-Focused)', fontsize=12)\n",
    "ax.set_title('Objective Score vs Silhouette Score\\n(Bubble size = K)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add diagonal line\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Equal scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Points in upper-left: High objective but low silhouette = Good for routing!\")\n",
    "print(\"   Points in lower-right: High silhouette but low objective = Just separating datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 8. üîç Detailed Analysis of Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-config-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute best clustering for detailed analysis\n",
    "best_linkage = best_result['linkage']\n",
    "best_k = int(best_result['k'])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"DETAILED ANALYSIS: {best_linkage.upper()} LINKAGE, K={best_k}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "agg_best = AgglomerativeClustering(\n",
    "    n_clusters=best_k,\n",
    "    linkage=best_linkage,\n",
    "    metric='euclidean' if best_linkage == 'ward' else 'cosine'\n",
    ")\n",
    "\n",
    "labels_best = agg_best.fit_predict(codebert_norm)\n",
    "df['cluster'] = labels_best\n",
    "\n",
    "# Per-cluster analysis\n",
    "print(f\"\\nPER-CLUSTER STATISTICS:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for cluster_id in range(best_k):\n",
    "    cluster_df = df[df['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Size: {len(cluster_df)} ({len(cluster_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Dataset distribution\n",
    "    source_counts = cluster_df['source'].value_counts()\n",
    "    print(f\"  Dataset distribution:\")\n",
    "    for source, count in source_counts.items():\n",
    "        pct = count / len(cluster_df) * 100\n",
    "        print(f\"    {source}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Dataset entropy for this cluster\n",
    "    probs = source_counts.values / source_counts.sum()\n",
    "    cluster_entropy = entropy(probs)\n",
    "    print(f\"  Entropy: {cluster_entropy:.3f} {'‚úÖ' if cluster_entropy > 0.8 else '‚ö†Ô∏è'}\")\n",
    "    \n",
    "    # Prompt length stats\n",
    "    print(f\"  Prompt length: mean={cluster_df['prompt_length'].mean():.0f}, std={cluster_df['prompt_length'].std():.0f}\")\n",
    "    print(f\"  Word count: mean={cluster_df['word_count'].mean():.0f}, std={cluster_df['word_count'].std():.0f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 9. üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('objective_clustering_results.csv', index=False)\n",
    "\n",
    "# Save best configuration\n",
    "best_config = {\n",
    "    'experiment': 'objective_clustering_evaluation',\n",
    "    'embedding_model': 'CodeBERT',\n",
    "    'best_configuration': {\n",
    "        'linkage': best_linkage,\n",
    "        'k': best_k,\n",
    "        'metric': 'euclidean' if best_linkage == 'ward' else 'cosine',\n",
    "        'objective_score': float(best_result['objective_score']),\n",
    "        'avg_entropy': float(best_result['avg_entropy']),\n",
    "        'gini': float(best_result['gini']),\n",
    "        'avg_max_dataset_pct': float(best_result['avg_max_dataset_pct']),\n",
    "        'silhouette': float(best_result['silhouette'])\n",
    "    },\n",
    "    'comparison': {\n",
    "        'best_by_silhouette': {\n",
    "            'linkage': best_by_silhouette['linkage'],\n",
    "            'k': int(best_by_silhouette['k']),\n",
    "            'silhouette': float(best_by_silhouette['silhouette']),\n",
    "            'objective_score': float(best_by_silhouette['objective_score'])\n",
    "        },\n",
    "        'best_by_objective': {\n",
    "            'linkage': best_by_objective['linkage'],\n",
    "            'k': int(best_by_objective['k']),\n",
    "            'objective_score': float(best_by_objective['objective_score']),\n",
    "            'silhouette': float(best_by_objective['silhouette'])\n",
    "        }\n",
    "    },\n",
    "    'metrics_explanation': {\n",
    "        'objective_score': 'Combined score (higher = better for routing)',\n",
    "        'avg_entropy': 'Dataset mixing (>1.0 = good)',\n",
    "        'gini': 'Cluster balance (<0.3 = good)',\n",
    "        'avg_max_dataset_pct': 'Max dataset % per cluster (<0.5 = good)'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('objective_clustering_best_config.json', 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "print('‚úÖ Exported:')\n",
    "print('  - objective_clustering_results.csv')\n",
    "print('  - objective_clustering_best_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 10. üìù Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('OBJECTIVE CLUSTERING EVALUATION - FINAL SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nüìä EXPERIMENTS CONDUCTED:')\n",
    "print(f'  Linkage methods: ward, complete, average')\n",
    "print(f'  K values tested: 2, 3, 4, 5, 6, 8, 10, 12, 15, 20, 25, 30')\n",
    "print(f'  Total configurations: {len(results_df)}')\n",
    "\n",
    "print(f'\\nüèÜ BEST CONFIGURATION (By Objective Score):')\n",
    "print(f\"  Linkage: {best_linkage}\")\n",
    "print(f\"  K: {best_k}\")\n",
    "print(f\"  Objective Score: {best_result['objective_score']:.4f}\")\n",
    "print(f\"  Avg Entropy: {best_result['avg_entropy']:.3f} {'‚úÖ GOOD' if best_result['avg_entropy'] > 1.0 else '‚ö†Ô∏è WEAK'}\")\n",
    "print(f\"  Gini: {best_result['gini']:.3f} {'‚úÖ BALANCED' if best_result['gini'] < 0.3 else '‚ö†Ô∏è UNBALANCED'}\")\n",
    "print(f\"  Max Dataset %: {best_result['avg_max_dataset_pct']:.1%}\")\n",
    "print(f\"  Silhouette: {best_result['silhouette']:.4f}\")\n",
    "\n",
    "print(f'\\nüî∑ BEST BY SILHOUETTE (For Comparison):')\n",
    "print(f\"  Linkage: {best_by_silhouette['linkage']}\")\n",
    "print(f\"  K: {int(best_by_silhouette['k'])}\")\n",
    "print(f\"  Silhouette: {best_by_silhouette['silhouette']:.4f}\")\n",
    "print(f\"  Objective Score: {best_by_silhouette['objective_score']:.4f}\")\n",
    "print(f\"  Max Dataset %: {best_by_silhouette['avg_max_dataset_pct']:.1%}\")\n",
    "\n",
    "if best_by_silhouette['avg_max_dataset_pct'] > 0.7:\n",
    "    print(f\"  ‚ùå WARNING: This configuration just separates datasets!\")\n",
    "\n",
    "print(f'\\nüí° KEY FINDINGS:')\n",
    "print(f\"  1. Silhouette score is MISLEADING for routing tasks\")\n",
    "print(f\"  2. Best by silhouette: K={int(best_by_silhouette['k'])} (but {best_by_silhouette['avg_max_dataset_pct']:.0%} dataset-dominated)\")\n",
    "print(f\"  3. Best by objective: K={best_k} (datasets mixed, balanced clusters)\")\n",
    "print(f\"  4. Dataset mixing entropy is the most important metric\")\n",
    "print(f\"  5. Need K>={best_k} for meaningful routing differentiation\")\n",
    "\n",
    "print(f'\\nüéØ PRODUCTION RECOMMENDATION:')\n",
    "print(f\"  ‚úÖ Use: {best_linkage} linkage, K={best_k}\")\n",
    "print(f\"  Why:\")\n",
    "print(f\"    - Datasets well-mixed (entropy={best_result['avg_entropy']:.2f})\")\n",
    "print(f\"    - Balanced clusters (gini={best_result['gini']:.2f})\")\n",
    "print(f\"    - Groups by task properties, not dataset source\")\n",
    "print(f\"    - Sufficient granularity for error rate differentiation\")\n",
    "\n",
    "print(f'\\n‚ö†Ô∏è  DO NOT USE:')\n",
    "if best_by_silhouette['avg_max_dataset_pct'] > 0.7:\n",
    "    print(f\"  ‚ùå K={int(best_by_silhouette['k'])} (high silhouette but just separates datasets)\")\n",
    "    print(f\"     This would route ALL {best_by_silhouette['linkage']} tasks to same model!\")\n",
    "\n",
    "print(f'\\n{'='*80}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
