{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üéØ Hierarchical Clustering Optimization\n",
    "\n",
    "**Based on findings from Notebook 04:**\n",
    "- Best Model: **CodeBERT**\n",
    "- Best Preprocessing: **Pure embeddings** (no PCA)\n",
    "- Best Algorithm: **Agglomerative Single Linkage**\n",
    "- Best K: **2**\n",
    "- Baseline Silhouette: **0.5952**\n",
    "\n",
    "## üî¨ Goal: Deep Dive into Hierarchical Clustering\n",
    "\n",
    "Since Agglomerative clustering performed best, let's optimize:\n",
    "\n",
    "### 1. Linkage Methods (Comprehensive)\n",
    "- Single (current best: 0.5952)\n",
    "- Complete\n",
    "- Average\n",
    "- Ward\n",
    "- Weighted\n",
    "- Centroid\n",
    "- Median\n",
    "\n",
    "### 2. Distance Metrics\n",
    "- Cosine (current)\n",
    "- Euclidean\n",
    "- Manhattan\n",
    "- Correlation\n",
    "\n",
    "### 3. K Values (Focused Range)\n",
    "- Fine-grained around K=2: [2, 3, 4, 5, 6, 8, 10, 12, 15]\n",
    "\n",
    "### 4. Dendrogram Analysis\n",
    "- Find natural cluster cutoffs\n",
    "- Visualize hierarchical structure\n",
    "\n",
    "### 5. Multi-Level Hierarchical Clustering\n",
    "- Level 1: Coarse clusters\n",
    "- Level 2: Fine-grained sub-clusters\n",
    "\n",
    "---\n",
    "**‚ö° GPU recommended for embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 0. üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install packages with version compatibility for Colab T4 GPU\n# Colab now uses NumPy 2.x as default for many packages\n!pip install -q --upgrade pip\n\n# Use NumPy 2.x to avoid conflicts with Colab pre-installed packages\n!pip install -q 'numpy>=2.0.0'\n\n# Core ML packages - latest versions compatible with NumPy 2.x\n!pip install -q 'transformers>=4.40.0'\n!pip install -q 'datasets>=2.18.0'\n!pip install -q 'scikit-learn>=1.4.0'\n!pip install -q 'scipy>=1.12.0'\n\n# Visualization packages\n!pip install -q 'matplotlib>=3.8.0'\n!pip install -q 'seaborn>=0.13.0'\n!pip install -q 'pandas>=2.2.0'\n!pip install -q 'umap-learn>=0.5.5'\n\n# Verify installations\nimport torch\nimport numpy as np\nimport transformers\nimport sklearn\n\nprint(f'‚úÖ Packages installed!')\nprint(f'NumPy version: {np.__version__}')\nprint(f'PyTorch version: {torch.__version__}')\nprint(f'Transformers version: {transformers.__version__}')\nprint(f'scikit-learn version: {sklearn.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'CUDA version: {torch.version.cuda}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\n\nif device == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    props = torch.cuda.get_device_properties(0)\n    print(f'Memory: {props.total_memory / 1e9:.1f} GB')\n    print(f'Compute Capability: {props.major}.{props.minor}')\n    \n    # T4 GPU specific optimizations\n    if 'T4' in torch.cuda.get_device_name(0):\n        print('‚úÖ T4 GPU detected - optimized for mixed precision training')\n        print('   Recommendation: Use batch_size=32-64 for best performance')\n    \n    # Clear cache to start fresh\n    torch.cuda.empty_cache()\n    print(f'Available memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB')\nelse:\n    print('üíª CPU mode')\n    print('   üí° Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU')"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. üì¶ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Embeddings\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Viz\n",
    "import umap\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ Imports complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. üì• Load Coding Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coding_datasets(max_total=4000):\n",
    "    \"\"\"\n",
    "    Load diverse coding datasets.\n",
    "    Same as notebook 03/04.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING CODING DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. SWE-bench\n",
    "    print(\"\\n1. Loading SWE-bench (GitHub issues)...\")\n",
    "    try:\n",
    "        swe_dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"test\")\n",
    "        count = 0\n",
    "        target = min(2000, len(swe_dataset))\n",
    "\n",
    "        for idx, item in enumerate(swe_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            problem = item.get(\"problem_statement\", \"\")\n",
    "            repo = item.get(\"repo\", \"\")\n",
    "\n",
    "            if \"django\" in repo.lower() or \"flask\" in repo.lower():\n",
    "                domain = \"web_framework\"\n",
    "            elif \"sklearn\" in repo.lower() or \"pandas\" in repo.lower() or \"numpy\" in repo.lower():\n",
    "                domain = \"data_science\"\n",
    "            elif \"matplotlib\" in repo.lower() or \"seaborn\" in repo.lower():\n",
    "                domain = \"visualization\"\n",
    "            elif \"pytest\" in repo.lower() or \"test\" in repo.lower():\n",
    "                domain = \"testing\"\n",
    "            elif \"requests\" in repo.lower() or \"http\" in repo.lower():\n",
    "                domain = \"networking\"\n",
    "            else:\n",
    "                domain = \"general\"\n",
    "\n",
    "            problem_lower = problem.lower()\n",
    "            if \"bug\" in problem_lower or \"fix\" in problem_lower or \"error\" in problem_lower:\n",
    "                task_type = \"bug_fix\"\n",
    "            elif \"test\" in problem_lower:\n",
    "                task_type = \"testing\"\n",
    "            elif \"refactor\" in problem_lower or \"clean\" in problem_lower:\n",
    "                task_type = \"refactor\"\n",
    "            elif \"add\" in problem_lower or \"implement\" in problem_lower or \"feature\" in problem_lower:\n",
    "                task_type = \"feature\"\n",
    "            else:\n",
    "                task_type = \"general\"\n",
    "\n",
    "            if len(problem) < 200:\n",
    "                complexity = \"simple\"\n",
    "            elif len(problem) < 500:\n",
    "                complexity = \"medium\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "\n",
    "            if problem:\n",
    "                questions.append({\n",
    "                    \"question\": problem,\n",
    "                    \"language\": \"python\",\n",
    "                    \"domain\": domain,\n",
    "                    \"task_type\": task_type,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": f\"swe_bench_{repo}\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} GitHub issues\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    # 2. DS-1000\n",
    "    print(\"\\n2. Loading DS-1000 (Data science tasks)...\")\n",
    "    try:\n",
    "        ds_dataset = load_dataset(\"xlangai/DS-1000\", split=\"test\")\n",
    "        count = 0\n",
    "\n",
    "        for item in ds_dataset:\n",
    "            prompt = item.get(\"prompt\", \"\")\n",
    "            metadata = item.get(\"metadata\", {})\n",
    "            library = metadata.get(\"library\", \"unknown\") if isinstance(metadata, dict) else \"unknown\"\n",
    "\n",
    "            if library in [\"Numpy\", \"Pandas\", \"Scipy\"]:\n",
    "                domain = \"data_manipulation\"\n",
    "            elif library in [\"Matplotlib\"]:\n",
    "                domain = \"visualization\"\n",
    "            elif library in [\"Pytorch\", \"Tensorflow\", \"Sklearn\"]:\n",
    "                domain = \"machine_learning\"\n",
    "            else:\n",
    "                domain = \"data_science\"\n",
    "\n",
    "            if len(prompt) < 150:\n",
    "                complexity = \"simple\"\n",
    "            elif len(prompt) < 300:\n",
    "                complexity = \"medium\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "\n",
    "            if prompt:\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"language\": \"python\",\n",
    "                    \"domain\": domain,\n",
    "                    \"task_type\": \"code_generation\",\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": f\"ds1000_{library.lower()}\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} data science tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    # 3. BigCodeBench\n",
    "    print(\"\\n3. Loading BigCodeBench (API tasks)...\")\n",
    "    try:\n",
    "        bigcode_dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.2\")\n",
    "        count = 0\n",
    "        target = min(500, len(bigcode_dataset))\n",
    "\n",
    "        for idx, item in enumerate(bigcode_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            complete_prompt = item.get(\"complete_prompt\", \"\")\n",
    "            instruct_prompt = item.get(\"instruct_prompt\", \"\")\n",
    "            prompt = instruct_prompt if instruct_prompt else complete_prompt\n",
    "\n",
    "            if len(prompt) < 200:\n",
    "                complexity = \"simple\"\n",
    "            elif len(prompt) < 400:\n",
    "                complexity = \"medium\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "\n",
    "            if prompt:\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"language\": \"python\",\n",
    "                    \"domain\": \"api_usage\",\n",
    "                    \"task_type\": \"code_generation\",\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": \"bigcodebench\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} API tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    # 4. DebugBench\n",
    "    print(\"\\n4. Loading DebugBench (Debugging tasks)...\")\n",
    "    try:\n",
    "        debug_dataset = load_dataset(\"Rtian/DebugBench\", split=\"test\")\n",
    "        count = 0\n",
    "        target = min(500, len(debug_dataset))\n",
    "\n",
    "        for idx, item in enumerate(debug_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            buggy_code = item.get(\"buggy_code\", \"\")\n",
    "            language = item.get(\"language\", \"python\").lower()\n",
    "            difficulty = item.get(\"difficulty\", \"medium\").lower()\n",
    "\n",
    "            complexity_map = {\"easy\": \"simple\", \"medium\": \"medium\", \"hard\": \"complex\"}\n",
    "            complexity = complexity_map.get(difficulty, \"medium\")\n",
    "\n",
    "            if buggy_code:\n",
    "                questions.append({\n",
    "                    \"question\": f\"Debug this code:\\n{buggy_code}\",\n",
    "                    \"language\": language,\n",
    "                    \"domain\": \"algorithms\",\n",
    "                    \"task_type\": \"debugging\",\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": \"debugbench\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   ‚úì Loaded {count} debugging tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ Total: {len(questions)} coding tasks\")\n",
    "    print(f\"\\nBreakdown:\")\n",
    "    print(f\"  Languages: {Counter(q['language'] for q in questions)}\")\n",
    "    print(f\"  Domains: {Counter(q['domain'] for q in questions)}\")\n",
    "    print(f\"  Task Types: {Counter(q['task_type'] for q in questions)}\")\n",
    "    print(f\"  Complexity: {Counter(q['complexity'] for q in questions)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Load data\n",
    "questions = load_coding_datasets(max_total=4000)\n",
    "texts = [q['question'] for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-header",
   "metadata": {},
   "source": [
    "## 3. üß† Extract CodeBERT Embeddings (Winner from Notebook 04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"Mean pooling - take average of all tokens\"\"\"\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def encode_with_codebert(model, tokenizer, texts, device, batch_size=32):\n",
    "    \"\"\"Encode texts using CodeBERT\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    print(f'üöÄ Encoding {len(texts)} texts with CodeBERT...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**encoded)\n",
    "            embeddings = mean_pooling(outputs.last_hidden_state, encoded['attention_mask'])\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            \n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f'  Processed {min(i+batch_size, len(texts))}/{len(texts)}', end='\\r')\n",
    "    \n",
    "    print(f'  Processed {len(texts)}/{len(texts)} ‚úì')\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CODEBERT (Best Model from Notebook 04)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "model = AutoModel.from_pretrained('microsoft/codebert-base').to(device)\n",
    "\n",
    "codebert_embeddings = encode_with_codebert(model, tokenizer, texts, device, batch_size=32)\n",
    "codebert_norm = normalize(codebert_embeddings, norm='l2')\n",
    "\n",
    "print(f\"\\n‚úÖ CodeBERT embeddings: {codebert_norm.shape}\")\n",
    "print(f\"   Mean: {codebert_norm.mean():.4f}, Std: {codebert_norm.std():.4f}\")\n",
    "\n",
    "# Free memory\n",
    "del model\n",
    "del tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiments-header",
   "metadata": {},
   "source": [
    "## 4. üî¨ Comprehensive Hierarchical Clustering Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp1-header",
   "metadata": {},
   "source": [
    "### Experiment 1: All Linkage Methods with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1-linkages",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: LINKAGE METHOD COMPARISON (Sklearn)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing: single, complete, average, ward\")\n",
    "print(\"K values: 2, 3, 4, 5, 6, 8, 10, 12, 15\\n\")\n",
    "\n",
    "linkages = ['single', 'complete', 'average', 'ward']\n",
    "k_values = [2, 3, 4, 5, 6, 8, 10, 12, 15]\n",
    "\n",
    "linkage_results = []\n",
    "\n",
    "for linkage in linkages:\n",
    "    print(f\"\\nTesting linkage: {linkage}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            agg = AgglomerativeClustering(\n",
    "                n_clusters=k,\n",
    "                linkage=linkage,\n",
    "                metric='euclidean' if linkage == 'ward' else 'cosine'\n",
    "            )\n",
    "            \n",
    "            labels = agg.fit_predict(codebert_norm)\n",
    "            \n",
    "            # Calculate silhouette\n",
    "            sil = silhouette_score(codebert_norm, labels, metric='cosine')\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            linkage_results.append({\n",
    "                'linkage': linkage,\n",
    "                'k': k,\n",
    "                'silhouette': sil,\n",
    "                'time_sec': elapsed\n",
    "            })\n",
    "            \n",
    "            print(f\"  K={k:2d}: Silhouette={sil:.6f} ({elapsed:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  K={k:2d}: Failed - {str(e)[:50]}\")\n",
    "\n",
    "linkage_df = pd.DataFrame(linkage_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOP 10 RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(linkage_df.nlargest(10, 'silhouette')[['linkage', 'k', 'silhouette', 'time_sec']])\n",
    "\n",
    "best_linkage = linkage_df.loc[linkage_df['silhouette'].idxmax()]\n",
    "print(f\"\\n‚úÖ Best: {best_linkage['linkage']} linkage, K={int(best_linkage['k'])}, Silhouette={best_linkage['silhouette']:.6f}\")\n",
    "print(f\"   Baseline (from Notebook 04): single linkage, K=2, Silhouette=0.5952\")\n",
    "print(f\"   Improvement: {(best_linkage['silhouette'] - 0.5952) / 0.5952 * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp2-header",
   "metadata": {},
   "source": [
    "### Experiment 2: Scipy Linkage Methods (More Options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2-scipy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: SCIPY LINKAGE METHODS (Extended)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing: single, complete, average, weighted, centroid, median, ward\")\n",
    "print(\"Using scipy.cluster.hierarchy for more linkage options\\n\")\n",
    "\n",
    "# Scipy linkage methods\n",
    "scipy_linkages = ['single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward']\n",
    "k_values_focused = [2, 3, 4, 5, 6, 8, 10]\n",
    "\n",
    "scipy_results = []\n",
    "\n",
    "for link_method in scipy_linkages:\n",
    "    print(f\"\\nTesting scipy linkage: {link_method}\")\n",
    "    \n",
    "    try:\n",
    "        # Compute linkage matrix\n",
    "        if link_method == 'ward':\n",
    "            # Ward requires euclidean distance\n",
    "            Z = linkage(codebert_norm, method=link_method, metric='euclidean')\n",
    "        else:\n",
    "            # Use cosine for others\n",
    "            Z = linkage(codebert_norm, method=link_method, metric='cosine')\n",
    "        \n",
    "        # Test different K values\n",
    "        for k in k_values_focused:\n",
    "            labels = fcluster(Z, k, criterion='maxclust')\n",
    "            \n",
    "            # Calculate silhouette\n",
    "            sil = silhouette_score(codebert_norm, labels, metric='cosine')\n",
    "            \n",
    "            scipy_results.append({\n",
    "                'linkage': link_method,\n",
    "                'k': k,\n",
    "                'silhouette': sil,\n",
    "                'method': 'scipy'\n",
    "            })\n",
    "            \n",
    "            print(f\"  K={k:2d}: Silhouette={sil:.6f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed: {str(e)[:50]}\")\n",
    "\n",
    "scipy_df = pd.DataFrame(scipy_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOP 10 SCIPY RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "if len(scipy_df) > 0:\n",
    "    print(scipy_df.nlargest(10, 'silhouette')[['linkage', 'k', 'silhouette']])\n",
    "    \n",
    "    best_scipy = scipy_df.loc[scipy_df['silhouette'].idxmax()]\n",
    "    print(f\"\\n‚úÖ Best scipy: {best_scipy['linkage']} linkage, K={int(best_scipy['k'])}, Silhouette={best_scipy['silhouette']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp3-header",
   "metadata": {},
   "source": [
    "### Experiment 3: Distance Metrics (For Compatible Linkages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: DISTANCE METRICS (Single Linkage Only)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing metrics: cosine, euclidean, manhattan, correlation\")\n",
    "print(\"Using best linkage: single, K values: 2, 3, 4, 5\\n\")\n",
    "\n",
    "metrics = ['cosine', 'euclidean', 'manhattan', 'correlation']\n",
    "k_values_metrics = [2, 3, 4, 5]\n",
    "\n",
    "metric_results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"\\nTesting metric: {metric}\")\n",
    "    \n",
    "    for k in k_values_metrics:\n",
    "        try:\n",
    "            agg = AgglomerativeClustering(\n",
    "                n_clusters=k,\n",
    "                linkage='single',\n",
    "                metric=metric\n",
    "            )\n",
    "            \n",
    "            labels = agg.fit_predict(codebert_norm)\n",
    "            \n",
    "            # Calculate silhouette with cosine for consistency\n",
    "            sil = silhouette_score(codebert_norm, labels, metric='cosine')\n",
    "            \n",
    "            metric_results.append({\n",
    "                'metric': metric,\n",
    "                'k': k,\n",
    "                'silhouette': sil\n",
    "            })\n",
    "            \n",
    "            print(f\"  K={k}: Silhouette={sil:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  K={k}: Failed - {str(e)[:50]}\")\n",
    "\n",
    "metric_df = pd.DataFrame(metric_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"METRIC COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "if len(metric_df) > 0:\n",
    "    print(metric_df.nlargest(10, 'silhouette'))\n",
    "    \n",
    "    best_metric = metric_df.loc[metric_df['silhouette'].idxmax()]\n",
    "    print(f\"\\n‚úÖ Best metric: {best_metric['metric']}, K={int(best_metric['k'])}, Silhouette={best_metric['silhouette']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dendrogram-header",
   "metadata": {},
   "source": [
    "## 5. üìä Dendrogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dendrogram",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DENDROGRAM ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Visualizing hierarchical structure with best linkage method\\n\")\n",
    "\n",
    "# Use best linkage from experiments\n",
    "best_overall_linkage = best_linkage['linkage']\n",
    "print(f\"Using linkage: {best_overall_linkage}\")\n",
    "\n",
    "# Compute linkage matrix\n",
    "if best_overall_linkage == 'ward':\n",
    "    Z = linkage(codebert_norm, method=best_overall_linkage, metric='euclidean')\n",
    "else:\n",
    "    Z = linkage(codebert_norm, method=best_overall_linkage, metric='cosine')\n",
    "\n",
    "# Plot dendrogram (truncated for readability)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,  # Show last 30 merges\n",
    "    leaf_font_size=10,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(f'Hierarchical Clustering Dendrogram ({best_overall_linkage} linkage)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Cluster Index', fontsize=12)\n",
    "ax.set_ylabel('Distance', fontsize=12)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Possible cutoff')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Dendrogram shows hierarchical relationships between clusters\")\n",
    "print(\"   Large vertical lines indicate good separation points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multilevel-header",
   "metadata": {},
   "source": [
    "## 6. üî∫ Multi-Level Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multilevel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MULTI-LEVEL HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Level 1: Coarse clustering (K=2-5)\")\n",
    "print(\"Level 2: Fine-grained sub-clustering within each Level 1 cluster\\n\")\n",
    "\n",
    "# Level 1: Coarse clustering\n",
    "best_k_coarse = int(best_linkage['k'])\n",
    "print(f\"Level 1: Using K={best_k_coarse} ({best_overall_linkage} linkage)\")\n",
    "\n",
    "agg_l1 = AgglomerativeClustering(\n",
    "    n_clusters=best_k_coarse,\n",
    "    linkage=best_overall_linkage,\n",
    "    metric='euclidean' if best_overall_linkage == 'ward' else 'cosine'\n",
    ")\n",
    "\n",
    "labels_l1 = agg_l1.fit_predict(codebert_norm)\n",
    "sil_l1 = silhouette_score(codebert_norm, labels_l1, metric='cosine')\n",
    "\n",
    "print(f\"Level 1 Silhouette: {sil_l1:.6f}\\n\")\n",
    "\n",
    "# Level 2: Sub-cluster each Level 1 cluster\n",
    "print(\"Level 2: Sub-clustering within each Level 1 cluster\")\n",
    "\n",
    "labels_l2 = np.zeros(len(codebert_norm), dtype=int)\n",
    "cluster_offset = 0\n",
    "l2_silhouettes = []\n",
    "\n",
    "for l1_cluster_id in range(best_k_coarse):\n",
    "    mask_l1 = labels_l1 == l1_cluster_id\n",
    "    embeddings_l1 = codebert_norm[mask_l1]\n",
    "    indices_l1 = np.where(mask_l1)[0]\n",
    "    \n",
    "    print(f\"\\n  Cluster {l1_cluster_id} ({len(embeddings_l1)} samples):\")\n",
    "    \n",
    "    if len(embeddings_l1) < 20:\n",
    "        print(f\"    Too few samples, keeping as single cluster\")\n",
    "        labels_l2[indices_l1] = cluster_offset\n",
    "        cluster_offset += 1\n",
    "        continue\n",
    "    \n",
    "    # Test K=2,3,4 for sub-clustering\n",
    "    best_sub_k = 2\n",
    "    best_sub_sil = -1\n",
    "    \n",
    "    for sub_k in [2, 3, 4]:\n",
    "        if len(embeddings_l1) < sub_k * 5:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            agg_l2 = AgglomerativeClustering(\n",
    "                n_clusters=sub_k,\n",
    "                linkage=best_overall_linkage,\n",
    "                metric='euclidean' if best_overall_linkage == 'ward' else 'cosine'\n",
    "            )\n",
    "            \n",
    "            labels_l2_temp = agg_l2.fit_predict(embeddings_l1)\n",
    "            sil_l2_temp = silhouette_score(embeddings_l1, labels_l2_temp, metric='cosine')\n",
    "            \n",
    "            print(f\"    Sub-K={sub_k}: Silhouette={sil_l2_temp:.4f}\")\n",
    "            \n",
    "            if sil_l2_temp > best_sub_sil:\n",
    "                best_sub_sil = sil_l2_temp\n",
    "                best_sub_k = sub_k\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Apply best sub-clustering\n",
    "    agg_l2 = AgglomerativeClustering(\n",
    "        n_clusters=best_sub_k,\n",
    "        linkage=best_overall_linkage,\n",
    "        metric='euclidean' if best_overall_linkage == 'ward' else 'cosine'\n",
    "    )\n",
    "    \n",
    "    labels_l2_cluster = agg_l2.fit_predict(embeddings_l1)\n",
    "    labels_l2[indices_l1] = labels_l2_cluster + cluster_offset\n",
    "    cluster_offset += best_sub_k\n",
    "    \n",
    "    l2_silhouettes.append(best_sub_sil)\n",
    "    print(f\"    ‚úÖ Best sub-K: {best_sub_k}, Silhouette: {best_sub_sil:.4f}\")\n",
    "\n",
    "# Overall Level 2 silhouette\n",
    "sil_l2_overall = silhouette_score(codebert_norm, labels_l2, metric='cosine')\n",
    "sil_l2_weighted = np.mean(l2_silhouettes) if l2_silhouettes else 0\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MULTI-LEVEL RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Level 1 (K={best_k_coarse}): Silhouette={sil_l1:.6f}\")\n",
    "print(f\"Level 2 (K={cluster_offset}): Overall Silhouette={sil_l2_overall:.6f}\")\n",
    "print(f\"Level 2 Weighted Avg: {sil_l2_weighted:.6f}\")\n",
    "print(f\"\\nüí° Multi-level provides hierarchical organization for interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 7. üìä Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Linkage vs K\n",
    "if len(linkage_df) > 0:\n",
    "    pivot = linkage_df.pivot_table(values='silhouette', index='linkage', columns='k')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.4f', cmap='RdYlGn', center=0.4, ax=ax, cbar_kws={'label': 'Silhouette Score'})\n",
    "    ax.set_title('Hierarchical Clustering: Linkage Method vs K', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "    ax.set_ylabel('Linkage Method', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No linkage results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-umap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization of best clustering\n",
    "print(\"Running UMAP dimensionality reduction...\")\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "embeddings_2d = reducer.fit_transform(codebert_norm)\n",
    "\n",
    "# Get best clustering labels\n",
    "best_k = int(best_linkage['k'])\n",
    "agg_best = AgglomerativeClustering(\n",
    "    n_clusters=best_k,\n",
    "    linkage=best_overall_linkage,\n",
    "    metric='euclidean' if best_overall_linkage == 'ward' else 'cosine'\n",
    ")\n",
    "labels_best = agg_best.fit_predict(codebert_norm)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Left: Best single-level clustering\n",
    "scatter1 = axes[0].scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=labels_best,\n",
    "    cmap='tab10',\n",
    "    alpha=0.6,\n",
    "    s=20\n",
    ")\n",
    "axes[0].set_title(\n",
    "    f'Best Config: {best_overall_linkage} linkage, K={best_k}\\nSilhouette={best_linkage[\"silhouette\"]:.4f}',\n",
    "    fontweight='bold',\n",
    "    fontsize=12\n",
    ")\n",
    "axes[0].set_xlabel('UMAP 1')\n",
    "axes[0].set_ylabel('UMAP 2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster ID')\n",
    "\n",
    "# Right: Multi-level clustering\n",
    "scatter2 = axes[1].scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=labels_l2,\n",
    "    cmap='tab20',\n",
    "    alpha=0.6,\n",
    "    s=20\n",
    ")\n",
    "axes[1].set_title(\n",
    "    f'Multi-Level: L1={best_k_coarse}, L2={cluster_offset}\\nSilhouette={sil_l2_overall:.4f}',\n",
    "    fontweight='bold',\n",
    "    fontsize=12\n",
    ")\n",
    "axes[1].set_xlabel('UMAP 1')\n",
    "axes[1].set_ylabel('UMAP 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Sub-Cluster ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ UMAP visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 8. üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    linkage_df,\n",
    "    scipy_df if len(scipy_df) > 0 else pd.DataFrame(),\n",
    "    metric_df if len(metric_df) > 0 else pd.DataFrame()\n",
    "], ignore_index=True)\n",
    "\n",
    "all_results.to_csv('hierarchical_clustering_results.csv', index=False)\n",
    "\n",
    "# Best configuration\n",
    "best_config = {\n",
    "    'approach': 'hierarchical_clustering',\n",
    "    'embedding_model': 'CodeBERT',\n",
    "    'preprocessing': 'pure (no PCA)',\n",
    "    'best_single_level': {\n",
    "        'linkage': str(best_linkage['linkage']),\n",
    "        'k': int(best_linkage['k']),\n",
    "        'silhouette': float(best_linkage['silhouette']),\n",
    "        'time_sec': float(best_linkage['time_sec'])\n",
    "    },\n",
    "    'best_multi_level': {\n",
    "        'level1_k': int(best_k_coarse),\n",
    "        'level2_total_k': int(cluster_offset),\n",
    "        'level1_silhouette': float(sil_l1),\n",
    "        'level2_silhouette': float(sil_l2_overall),\n",
    "        'level2_weighted_avg': float(sil_l2_weighted)\n",
    "    },\n",
    "    'baseline_from_notebook04': {\n",
    "        'linkage': 'single',\n",
    "        'k': 2,\n",
    "        'silhouette': 0.5952\n",
    "    },\n",
    "    'improvement_pct': float((best_linkage['silhouette'] - 0.5952) / 0.5952 * 100)\n",
    "}\n",
    "\n",
    "with open('hierarchical_best_config.json', 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "print('‚úÖ Exported:')\n",
    "print('  - hierarchical_clustering_results.csv')\n",
    "print('  - hierarchical_best_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. üìù Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('HIERARCHICAL CLUSTERING OPTIMIZATION - FINAL SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nüìä EXPERIMENTS CONDUCTED:')\n",
    "print(f'  Experiment 1: Sklearn linkages (single, complete, average, ward)')\n",
    "print(f'  Experiment 2: Scipy linkages (+ weighted, centroid, median)')\n",
    "print(f'  Experiment 3: Distance metrics (cosine, euclidean, manhattan, correlation)')\n",
    "print(f'  Experiment 4: Dendrogram analysis')\n",
    "print(f'  Experiment 5: Multi-level hierarchical clustering')\n",
    "print(f'  Total configurations tested: {len(all_results)}')\n",
    "\n",
    "print(f'\\nüèÜ BEST SINGLE-LEVEL CONFIGURATION:')\n",
    "print(f\"  Linkage: {best_linkage['linkage']}\")\n",
    "print(f\"  K: {int(best_linkage['k'])}\")\n",
    "print(f\"  Silhouette: {best_linkage['silhouette']:.6f}\")\n",
    "print(f\"  Time: {best_linkage['time_sec']:.2f}s\")\n",
    "\n",
    "print(f'\\nüî∫ BEST MULTI-LEVEL CONFIGURATION:')\n",
    "print(f\"  Level 1: K={best_k_coarse} (Silhouette={sil_l1:.6f})\")\n",
    "print(f\"  Level 2: K={cluster_offset} (Silhouette={sil_l2_overall:.6f})\")\n",
    "print(f\"  Weighted Avg: {sil_l2_weighted:.6f}\")\n",
    "\n",
    "print(f'\\nüìà COMPARISON:')\n",
    "print(f\"  Notebook 04 baseline: 0.5952 (single, K=2)\")\n",
    "print(f\"  This optimization: {best_linkage['silhouette']:.6f}\")\n",
    "improvement = (best_linkage['silhouette'] - 0.5952) / 0.5952 * 100\n",
    "print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(f'\\nüí° KEY FINDINGS:')\n",
    "print(f\"  1. Best linkage method: {best_linkage['linkage']}\")\n",
    "print(f\"  2. Optimal K value: {int(best_linkage['k'])}\")\n",
    "if len(metric_df) > 0:\n",
    "    best_metric_name = metric_df.loc[metric_df['silhouette'].idxmax()]['metric']\n",
    "    print(f\"  3. Best distance metric: {best_metric_name}\")\n",
    "print(f\"  4. Multi-level clustering provides {cluster_offset} fine-grained clusters\")\n",
    "print(f\"  5. Pure CodeBERT embeddings (no PCA) work best\")\n",
    "\n",
    "print(f'\\nüéØ PRODUCTION RECOMMENDATION:')\n",
    "if best_linkage['silhouette'] > 0.5952:\n",
    "    print(f\"  ‚úÖ Use optimized config: {best_linkage['linkage']} linkage, K={int(best_linkage['k'])}\")\n",
    "    print(f\"     Expected silhouette: {best_linkage['silhouette']:.4f}\")\n",
    "else:\n",
    "    print(f\"  ‚ÑπÔ∏è  Baseline from Notebook 04 is competitive\")\n",
    "    print(f\"     Consider: single linkage, K=2, Silhouette=0.5952\")\n",
    "\n",
    "print(f'\\nüí° MULTI-LEVEL USE CASE:')\n",
    "print(f\"  Level 1 ({best_k_coarse} clusters): High-level categorization\")\n",
    "print(f\"  Level 2 ({cluster_offset} clusters): Fine-grained routing\")\n",
    "print(f\"  Benefit: Hierarchical interpretability for model selection\")\n",
    "\n",
    "print(f'\\n{'='*80}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}