{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ CodeBERT Hybrid Optimization\n",
    "\n",
    "**Goal:** Deep dive into CodeBERT_hybrid configuration optimization\n",
    "\n",
    "## ðŸ“Š What is CodeBERT_hybrid?\n",
    "\n",
    "**Hybrid = 70% CodeBERT embeddings + 30% metadata features**\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **CodeBERT embeddings (70%)**:\n",
    "   - Semantic understanding of code from microsoft/codebert-base\n",
    "   - 768-dimensional dense vectors\n",
    "   - Captures: syntax patterns, variable names, code structure, API usage\n",
    "\n",
    "2. **Metadata features (30%)**:\n",
    "   - One-hot encoded categorical attributes:\n",
    "     - `language`: python, cpp, java, etc.\n",
    "     - `domain`: web_framework, data_science, algorithms, etc.\n",
    "     - `task_type`: bug_fix, feature, testing, debugging, etc.\n",
    "     - `complexity`: simple, medium, complex\n",
    "   - Normalized to L2 norm\n",
    "\n",
    "3. **Why it works**:\n",
    "   - CodeBERT alone = semantic similarity (code that \"looks\" similar)\n",
    "   - Metadata alone = categorical grouping (exact language/domain matches)\n",
    "   - **Hybrid = best of both**: semantic understanding + explicit structure\n",
    "\n",
    "### Previous Result:\n",
    "- **Configuration**: CodeBERT_hybrid + Agglomerative Average + K=40\n",
    "- **Silhouette**: **0.689** (excellent!)\n",
    "\n",
    "### This Experiment:\n",
    "Systematically optimize:\n",
    "1. K values (25-60)\n",
    "2. Hybrid ratios (60/40, 70/30, 80/20, 90/10)\n",
    "3. Agglomerative linkages (ward, complete, average, single)\n",
    "4. Alternative algorithms (KMeans, Spectral, GMM, HDBSCAN)\n",
    "\n",
    "---\n",
    "**âš¡ GPU recommended for CodeBERT extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 0. ðŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets umap-learn scikit-learn matplotlib seaborn pandas numpy hdbscan torch\n",
    "print('âœ… Packages installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"ðŸ’» CPU mode\")\n",
    "    print(\"   ðŸ’¡ Enable GPU: Runtime â†’ Change runtime type â†’ GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. ðŸ“¦ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.preprocessing import normalize\n",
    "import umap\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Viz\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('âœ… Imports complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. ðŸ“¥ Load Coding Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coding_datasets(max_total=4000):\n",
    "    \"\"\"\n",
    "    Load diverse coding datasets and extract metadata.\n",
    "    Same as notebook 03.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING CODING DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. SWE-bench\n",
    "    print(\"\\n1. Loading SWE-bench (GitHub issues)...\")\n",
    "    try:\n",
    "        swe_dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\", split=\"test\")\n",
    "        count = 0\n",
    "        target = min(2000, len(swe_dataset))\n",
    "\n",
    "        for idx, item in enumerate(swe_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            problem = item.get(\"problem_statement\", \"\")\n",
    "            repo = item.get(\"repo\", \"\")\n",
    "\n",
    "            if \"django\" in repo.lower() or \"flask\" in repo.lower():\n",
    "                domain = \"web_framework\"\n",
    "            elif \"sklearn\" in repo.lower() or \"pandas\" in repo.lower() or \"numpy\" in repo.lower():\n",
    "                domain = \"data_science\"\n",
    "            elif \"matplotlib\" in repo.lower() or \"seaborn\" in repo.lower():\n",
    "                domain = \"visualization\"\n",
    "            elif \"pytest\" in repo.lower() or \"test\" in repo.lower():\n",
    "                domain = \"testing\"\n",
    "            elif \"requests\" in repo.lower() or \"http\" in repo.lower():\n",
    "                domain = \"networking\"\n",
    "            else:\n",
    "                domain = \"general\"\n",
    "\n",
    "            problem_lower = problem.lower()\n",
    "            if \"bug\" in problem_lower or \"fix\" in problem_lower or \"error\" in problem_lower:\n",
    "                task_type = \"bug_fix\"\n",
    "            elif \"test\" in problem_lower:\n",
    "                task_type = \"testing\"\n",
    "            elif \"refactor\" in problem_lower or \"clean\" in problem_lower:\n",
    "                task_type = \"refactor\"\n",
    "            elif \"add\" in problem_lower or \"implement\" in problem_lower or \"feature\" in problem_lower:\n",
    "                task_type = \"feature\"\n",
    "            else:\n",
    "                task_type = \"general\"\n",
    "\n",
    "            if len(problem) < 200:\n",
    "                complexity = \"simple\"\n",
    "            elif len(problem) < 500:\n",
    "                complexity = \"medium\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "\n",
    "            if problem:\n",
    "                questions.append({\n",
    "                    \"question\": problem,\n",
    "                    \"language\": \"python\",\n",
    "                    \"domain\": domain,\n",
    "                    \"task_type\": task_type,\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": f\"swe_bench_{repo}\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   âœ“ Loaded {count} GitHub issues\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Error: {e}\")\n",
    "\n",
    "    # 2. DS-1000\n",
    "    print(\"\\n2. Loading DS-1000 (Data science tasks)...\")\n",
    "    try:\n",
    "        ds_dataset = load_dataset(\"xlangai/DS-1000\", split=\"test\")\n",
    "        count = 0\n",
    "\n",
    "        for item in ds_dataset:\n",
    "            prompt = item.get(\"prompt\", \"\")\n",
    "            metadata = item.get(\"metadata\", {})\n",
    "            library = metadata.get(\"library\", \"unknown\") if isinstance(metadata, dict) else \"unknown\"\n",
    "\n",
    "            if library in [\"Numpy\", \"Pandas\", \"Scipy\"]:\n",
    "                domain = \"data_manipulation\"\n",
    "            elif library in [\"Matplotlib\"]:\n",
    "                domain = \"visualization\"\n",
    "            elif library in [\"Pytorch\", \"Tensorflow\", \"Sklearn\"]:\n",
    "                domain = \"machine_learning\"\n",
    "            else:\n",
    "                domain = \"data_science\"\n",
    "\n",
    "            if len(prompt) < 150:\n",
    "                complexity = \"simple\"\n",
    "            elif len(prompt) < 300:\n",
    "                complexity = \"medium\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "\n",
    "            if prompt:\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"language\": \"python\",\n",
    "                    \"domain\": domain,\n",
    "                    \"task_type\": \"code_generation\",\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": f\"ds1000_{library.lower()}\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   âœ“ Loaded {count} data science tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Error: {e}\")\n",
    "\n",
    "    # 3. BigCodeBench\n",
    "    print(\"\\n3. Loading BigCodeBench (API tasks)...\")\n",
    "    try:\n",
    "        bigcode_dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.2\")\n",
    "        count = 0\n",
    "        target = min(500, len(bigcode_dataset))\n",
    "\n",
    "        for idx, item in enumerate(bigcode_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            complete_prompt = item.get(\"complete_prompt\", \"\")\n",
    "            instruct_prompt = item.get(\"instruct_prompt\", \"\")\n",
    "            prompt = instruct_prompt if instruct_prompt else complete_prompt\n",
    "\n",
    "            if len(prompt) < 200:\n",
    "                complexity = \"simple\"\n",
    "            elif len(prompt) < 400:\n",
    "                complexity = \"medium\"\n",
    "            else:\n",
    "                complexity = \"complex\"\n",
    "\n",
    "            if prompt:\n",
    "                questions.append({\n",
    "                    \"question\": prompt,\n",
    "                    \"language\": \"python\",\n",
    "                    \"domain\": \"api_usage\",\n",
    "                    \"task_type\": \"code_generation\",\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": \"bigcodebench\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   âœ“ Loaded {count} API tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Error: {e}\")\n",
    "\n",
    "    # 4. DebugBench\n",
    "    print(\"\\n4. Loading DebugBench (Debugging tasks)...\")\n",
    "    try:\n",
    "        debug_dataset = load_dataset(\"Rtian/DebugBench\", split=\"test\")\n",
    "        count = 0\n",
    "        target = min(500, len(debug_dataset))\n",
    "\n",
    "        for idx, item in enumerate(debug_dataset):\n",
    "            if count >= target:\n",
    "                break\n",
    "\n",
    "            buggy_code = item.get(\"buggy_code\", \"\")\n",
    "            language = item.get(\"language\", \"python\").lower()\n",
    "            difficulty = item.get(\"difficulty\", \"medium\").lower()\n",
    "\n",
    "            complexity_map = {\"easy\": \"simple\", \"medium\": \"medium\", \"hard\": \"complex\"}\n",
    "            complexity = complexity_map.get(difficulty, \"medium\")\n",
    "\n",
    "            if buggy_code:\n",
    "                questions.append({\n",
    "                    \"question\": f\"Debug this code:\\n{buggy_code}\",\n",
    "                    \"language\": language,\n",
    "                    \"domain\": \"algorithms\",\n",
    "                    \"task_type\": \"debugging\",\n",
    "                    \"complexity\": complexity,\n",
    "                    \"source\": \"debugbench\"\n",
    "                })\n",
    "                count += 1\n",
    "\n",
    "        print(f\"   âœ“ Loaded {count} debugging tasks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Error: {e}\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… Total: {len(questions)} coding tasks\")\n",
    "    print(f\"\\nBreakdown:\")\n",
    "    print(f\"  Languages: {Counter(q['language'] for q in questions)}\")\n",
    "    print(f\"  Domains: {Counter(q['domain'] for q in questions)}\")\n",
    "    print(f\"  Task Types: {Counter(q['task_type'] for q in questions)}\")\n",
    "    print(f\"  Complexity: {Counter(q['complexity'] for q in questions)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Load data\n",
    "questions = load_coding_datasets(max_total=4000)\n",
    "texts = [q['question'] for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-header",
   "metadata": {},
   "source": [
    "## 3. ðŸ§  Extract CodeBERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"Mean pooling - take average of all tokens\"\"\"\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def encode_with_codebert(model, tokenizer, texts, device, batch_size=32):\n",
    "    \"\"\"Encode texts using CodeBERT\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    print(f'ðŸš€ Encoding {len(texts)} texts with CodeBERT...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            outputs = model(**encoded)\n",
    "            \n",
    "            # Mean pooling\n",
    "            embeddings = mean_pooling(outputs.last_hidden_state, encoded['attention_mask'])\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "            \n",
    "            # Progress\n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                print(f'  Processed {min(i+batch_size, len(texts))}/{len(texts)}', end='\\r')\n",
    "    \n",
    "    print(f'  Processed {len(texts)}/{len(texts)} âœ“')\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING CODEBERT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "model = AutoModel.from_pretrained('microsoft/codebert-base').to(device)\n",
    "\n",
    "codebert_embeddings = encode_with_codebert(model, tokenizer, texts, device, batch_size=32)\n",
    "codebert_norm = normalize(codebert_embeddings, norm='l2')\n",
    "\n",
    "print(f\"\\nâœ… CodeBERT embeddings: {codebert_norm.shape}\")\n",
    "print(f\"   Mean: {codebert_norm.mean():.4f}, Std: {codebert_norm.std():.4f}\")\n",
    "\n",
    "# Free memory\n",
    "del model\n",
    "del tokenizer\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata-header",
   "metadata": {},
   "source": [
    "## 4. ðŸ“Š Create Metadata Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating metadata features...')\n",
    "\n",
    "metadata_features = pd.concat([\n",
    "    pd.get_dummies([q['language'] for q in questions], prefix='lang'),\n",
    "    pd.get_dummies([q['domain'] for q in questions], prefix='domain'),\n",
    "    pd.get_dummies([q['task_type'] for q in questions], prefix='task'),\n",
    "    pd.get_dummies([q['complexity'] for q in questions], prefix='complexity')\n",
    "], axis=1).values\n",
    "\n",
    "metadata_norm = normalize(metadata_features, norm='l2')\n",
    "\n",
    "print(f'âœ… Metadata features: {metadata_norm.shape}')\n",
    "print(f\"   Attributes: language, domain, task_type, complexity\")\n",
    "print(f\"   Encoding: One-hot, L2-normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-header",
   "metadata": {},
   "source": [
    "## 5. ðŸ”¬ Comprehensive Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_features(codebert_emb, metadata_emb, ratio=0.7):\n",
    "    \"\"\"Create hybrid features with given ratio.\n",
    "    \n",
    "    Args:\n",
    "        codebert_emb: CodeBERT embeddings (normalized)\n",
    "        metadata_emb: Metadata features (normalized)\n",
    "        ratio: Weight for CodeBERT (0.0-1.0), metadata gets (1-ratio)\n",
    "    \n",
    "    Returns:\n",
    "        Concatenated and normalized hybrid features\n",
    "    \"\"\"\n",
    "    hybrid = np.concatenate([\n",
    "        codebert_emb * ratio,\n",
    "        metadata_emb * (1 - ratio)\n",
    "    ], axis=1)\n",
    "    return normalize(hybrid, norm='l2')\n",
    "\n",
    "def run_clustering(features, algorithm_name, k=None, **kwargs):\n",
    "    \"\"\"Run clustering and return labels + silhouette score.\"\"\"\n",
    "    if algorithm_name == 'KMeans':\n",
    "        alg = KMeans(n_clusters=k, random_state=42, n_init=10, **kwargs)\n",
    "    elif algorithm_name == 'Agglomerative':\n",
    "        alg = AgglomerativeClustering(n_clusters=k, **kwargs)\n",
    "    elif algorithm_name == 'Spectral':\n",
    "        alg = SpectralClustering(n_clusters=k, random_state=42, **kwargs)\n",
    "    elif algorithm_name == 'GMM':\n",
    "        alg = GaussianMixture(n_components=k, random_state=42, **kwargs)\n",
    "    elif algorithm_name == 'HDBSCAN':\n",
    "        alg = hdbscan.HDBSCAN(metric='cosine', **kwargs)\n",
    "    elif algorithm_name == 'DBSCAN':\n",
    "        alg = DBSCAN(metric='cosine', **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm_name}\")\n",
    "    \n",
    "    labels = alg.fit_predict(features)\n",
    "    \n",
    "    # Filter out noise points (-1)\n",
    "    mask = labels >= 0\n",
    "    n_clusters = len(set(labels[mask]))\n",
    "    \n",
    "    if n_clusters > 1 and mask.sum() > 10:\n",
    "        sil = silhouette_score(features[mask], labels[mask], metric='cosine')\n",
    "    else:\n",
    "        sil = -1.0\n",
    "    \n",
    "    return labels, sil, n_clusters\n",
    "\n",
    "print('âœ… Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp1-header",
   "metadata": {},
   "source": [
    "### Experiment 1: K-value Sweep (Agglomerative Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1-k-sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: K-VALUE SWEEP\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration: CodeBERT_hybrid (70/30) + Agglomerative Average\")\n",
    "print(\"K values: 25, 30, 35, 40, 45, 50, 55, 60\\n\")\n",
    "\n",
    "# Create hybrid features (70/30)\n",
    "hybrid_70_30 = create_hybrid_features(codebert_norm, metadata_norm, ratio=0.7)\n",
    "\n",
    "k_values = [25, 30, 35, 40, 45, 50, 55, 60]\n",
    "k_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    labels, sil, n_clusters = run_clustering(\n",
    "        hybrid_70_30,\n",
    "        'Agglomerative',\n",
    "        k=k,\n",
    "        linkage='average'\n",
    "    )\n",
    "    \n",
    "    k_results.append({\n",
    "        'k': k,\n",
    "        'silhouette': sil,\n",
    "        'n_clusters': n_clusters,\n",
    "        'labels': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"K={k:2d}: Silhouette = {sil:.6f}\")\n",
    "\n",
    "best_k = max(k_results, key=lambda x: x['silhouette'])\n",
    "print(f\"\\nâœ… Best K: {best_k['k']} (Silhouette = {best_k['silhouette']:.6f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp2-header",
   "metadata": {},
   "source": [
    "### Experiment 2: Hybrid Ratio Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2-ratio-sweep",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: HYBRID RATIO SWEEP\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration: CodeBERT_hybrid + Agglomerative Average + K=40\")\n",
    "print(\"Ratios: 60/40, 70/30, 80/20, 90/10 (CodeBERT/Metadata)\\n\")\n",
    "\n",
    "ratios = [0.6, 0.7, 0.8, 0.9]\n",
    "ratio_results = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    hybrid = create_hybrid_features(codebert_norm, metadata_norm, ratio=ratio)\n",
    "    \n",
    "    labels, sil, n_clusters = run_clustering(\n",
    "        hybrid,\n",
    "        'Agglomerative',\n",
    "        k=40,\n",
    "        linkage='average'\n",
    "    )\n",
    "    \n",
    "    ratio_results.append({\n",
    "        'ratio': ratio,\n",
    "        'ratio_str': f\"{int(ratio*100)}/{int((1-ratio)*100)}\",\n",
    "        'silhouette': sil,\n",
    "        'labels': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"Ratio {int(ratio*100)}/{int((1-ratio)*100)}: Silhouette = {sil:.6f}\")\n",
    "\n",
    "best_ratio = max(ratio_results, key=lambda x: x['silhouette'])\n",
    "print(f\"\\nâœ… Best Ratio: {best_ratio['ratio_str']} (Silhouette = {best_ratio['silhouette']:.6f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp3-header",
   "metadata": {},
   "source": [
    "### Experiment 3: Linkage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp3-linkage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: LINKAGE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration: CodeBERT_hybrid (70/30) + K=40\")\n",
    "print(\"Linkages: ward, complete, average, single\\n\")\n",
    "\n",
    "linkages = ['ward', 'complete', 'average', 'single']\n",
    "linkage_results = []\n",
    "\n",
    "for linkage in linkages:\n",
    "    labels, sil, n_clusters = run_clustering(\n",
    "        hybrid_70_30,\n",
    "        'Agglomerative',\n",
    "        k=40,\n",
    "        linkage=linkage\n",
    "    )\n",
    "    \n",
    "    linkage_results.append({\n",
    "        'linkage': linkage,\n",
    "        'silhouette': sil,\n",
    "        'labels': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"{linkage:10s}: Silhouette = {sil:.6f}\")\n",
    "\n",
    "best_linkage = max(linkage_results, key=lambda x: x['silhouette'])\n",
    "print(f\"\\nâœ… Best Linkage: {best_linkage['linkage']} (Silhouette = {best_linkage['silhouette']:.6f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exp4-header",
   "metadata": {},
   "source": [
    "### Experiment 4: Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp4-algorithms",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 4: ALGORITHM COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration: CodeBERT_hybrid (70/30) + K=40\\n\")\n",
    "\n",
    "algorithm_results = []\n",
    "\n",
    "# K-based algorithms\n",
    "for alg_name in ['KMeans', 'Spectral', 'GMM']:\n",
    "    labels, sil, n_clusters = run_clustering(hybrid_70_30, alg_name, k=40)\n",
    "    algorithm_results.append({\n",
    "        'algorithm': alg_name,\n",
    "        'silhouette': sil,\n",
    "        'k': 40,\n",
    "        'labels': labels\n",
    "    })\n",
    "    print(f\"{alg_name:15s}: Silhouette = {sil:.6f}\")\n",
    "\n",
    "# Density-based algorithms\n",
    "print(\"\\nDensity-based algorithms (auto K):\")\n",
    "\n",
    "# HDBSCAN\n",
    "labels, sil, n_clusters = run_clustering(\n",
    "    hybrid_70_30,\n",
    "    'HDBSCAN',\n",
    "    min_cluster_size=50\n",
    ")\n",
    "algorithm_results.append({\n",
    "    'algorithm': 'HDBSCAN',\n",
    "    'silhouette': sil,\n",
    "    'k': n_clusters,\n",
    "    'labels': labels\n",
    "})\n",
    "print(f\"HDBSCAN (K={n_clusters:2d})    : Silhouette = {sil:.6f}\")\n",
    "\n",
    "# DBSCAN\n",
    "labels, sil, n_clusters = run_clustering(\n",
    "    hybrid_70_30,\n",
    "    'DBSCAN',\n",
    "    eps=0.3,\n",
    "    min_samples=10\n",
    ")\n",
    "algorithm_results.append({\n",
    "    'algorithm': 'DBSCAN',\n",
    "    'silhouette': sil,\n",
    "    'k': n_clusters,\n",
    "    'labels': labels\n",
    "})\n",
    "print(f\"DBSCAN (K={n_clusters:2d})     : Silhouette = {sil:.6f}\")\n",
    "\n",
    "# Add Agglomerative average from previous experiments\n",
    "algorithm_results.append({\n",
    "    'algorithm': 'Agglomerative-avg',\n",
    "    'silhouette': best_linkage['silhouette'],\n",
    "    'k': 40,\n",
    "    'labels': best_linkage['labels']\n",
    "})\n",
    "\n",
    "best_algorithm = max(algorithm_results, key=lambda x: x['silhouette'])\n",
    "print(f\"\\nâœ… Best Algorithm: {best_algorithm['algorithm']} (Silhouette = {best_algorithm['silhouette']:.6f})\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 6. ðŸ“Š Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. K-Value Optimization:\")\n",
    "print(f\"   Best K: {best_k['k']}\")\n",
    "print(f\"   Silhouette: {best_k['silhouette']:.6f}\")\n",
    "\n",
    "print(\"\\n2. Hybrid Ratio Optimization:\")\n",
    "print(f\"   Best Ratio: {best_ratio['ratio_str']} (CodeBERT/Metadata)\")\n",
    "print(f\"   Silhouette: {best_ratio['silhouette']:.6f}\")\n",
    "\n",
    "print(\"\\n3. Linkage Optimization:\")\n",
    "print(f\"   Best Linkage: {best_linkage['linkage']}\")\n",
    "print(f\"   Silhouette: {best_linkage['silhouette']:.6f}\")\n",
    "\n",
    "print(\"\\n4. Algorithm Comparison:\")\n",
    "print(f\"   Best Algorithm: {best_algorithm['algorithm']}\")\n",
    "print(f\"   Silhouette: {best_algorithm['silhouette']:.6f}\")\n",
    "\n",
    "# Find overall best\n",
    "all_results = [\n",
    "    ('K-sweep', best_k['silhouette'], f\"K={best_k['k']}\"),\n",
    "    ('Ratio-sweep', best_ratio['silhouette'], f\"Ratio={best_ratio['ratio_str']}\"),\n",
    "    ('Linkage', best_linkage['silhouette'], f\"Linkage={best_linkage['linkage']}\"),\n",
    "    ('Algorithm', best_algorithm['silhouette'], f\"Alg={best_algorithm['algorithm']}\")\n",
    "]\n",
    "\n",
    "overall_best = max(all_results, key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ† OVERALL BEST CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Source: {overall_best[0]}\")\n",
    "print(f\"Config: {overall_best[2]}\")\n",
    "print(f\"Silhouette: {overall_best[1]:.6f}\")\n",
    "print(\"\\nComparison to baseline:\")\n",
    "print(f\"  Notebook 04 best: 0.689515\")\n",
    "print(f\"  This experiment:  {overall_best[1]:.6f}\")\n",
    "if overall_best[1] > 0.689515:\n",
    "    print(f\"  ðŸŽ‰ Improvement: +{(overall_best[1] - 0.689515):.6f}\")\n",
    "else:\n",
    "    print(f\"  âš ï¸  Difference: {(overall_best[1] - 0.689515):.6f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 7. ðŸ” Detailed Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silhouette-header",
   "metadata": {},
   "source": [
    "### Per-Cluster Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-cluster-silhouette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best configuration from linkage experiment (Agglomerative average K=40)\n",
    "best_labels = best_linkage['labels']\n",
    "best_features = hybrid_70_30\n",
    "\n",
    "# Calculate per-sample silhouette scores\n",
    "mask = best_labels >= 0  # Filter noise\n",
    "sample_silhouettes = silhouette_samples(best_features[mask], best_labels[mask], metric='cosine')\n",
    "\n",
    "# Calculate per-cluster metrics\n",
    "cluster_metrics = []\n",
    "\n",
    "valid_labels = best_labels[mask]\n",
    "for cluster_id in range(best_k['k']):\n",
    "    cluster_mask = valid_labels == cluster_id\n",
    "    cluster_samples = sample_silhouettes[cluster_mask]\n",
    "    \n",
    "    if len(cluster_samples) > 0:\n",
    "        cluster_metrics.append({\n",
    "            'cluster': cluster_id,\n",
    "            'size': len(cluster_samples),\n",
    "            'silhouette_mean': cluster_samples.mean(),\n",
    "            'silhouette_std': cluster_samples.std(),\n",
    "            'silhouette_min': cluster_samples.min(),\n",
    "            'silhouette_max': cluster_samples.max()\n",
    "        })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_metrics)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PER-CLUSTER SILHOUETTE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTop 10 best clusters (highest mean silhouette):\")\n",
    "print(cluster_df.nlargest(10, 'silhouette_mean')[['cluster', 'size', 'silhouette_mean', 'silhouette_std']])\n",
    "\n",
    "print(f\"\\nTop 10 worst clusters (lowest mean silhouette):\")\n",
    "print(cluster_df.nsmallest(10, 'silhouette_mean')[['cluster', 'size', 'silhouette_mean', 'silhouette_std']])\n",
    "\n",
    "print(f\"\\nCluster size statistics:\")\n",
    "print(f\"  Mean size: {cluster_df['size'].mean():.1f}\")\n",
    "print(f\"  Median size: {cluster_df['size'].median():.1f}\")\n",
    "print(f\"  Std size: {cluster_df['size'].std():.1f}\")\n",
    "print(f\"  Min size: {cluster_df['size'].min()}\")\n",
    "print(f\"  Max size: {cluster_df['size'].max()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "### UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umap-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running UMAP dimensionality reduction...\")\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    random_state=42,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "embeddings_2d = reducer.fit_transform(best_features)\n",
    "\n",
    "print(\"âœ… UMAP complete\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Left: Color by cluster\n",
    "scatter1 = axes[0].scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=best_labels,\n",
    "    cmap='tab20',\n",
    "    alpha=0.6,\n",
    "    s=20\n",
    ")\n",
    "axes[0].set_title(\n",
    "    f'UMAP: Best Configuration (K={best_k[\"k\"]}, Sil={best_linkage[\"silhouette\"]:.4f})',\n",
    "    fontweight='bold',\n",
    "    fontsize=14\n",
    ")\n",
    "axes[0].set_xlabel('UMAP 1', fontsize=12)\n",
    "axes[0].set_ylabel('UMAP 2', fontsize=12)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster ID')\n",
    "\n",
    "# Right: Color by silhouette score\n",
    "full_silhouettes = np.full(len(best_labels), -1.0)\n",
    "full_silhouettes[mask] = sample_silhouettes\n",
    "\n",
    "scatter2 = axes[1].scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=full_silhouettes,\n",
    "    cmap='RdYlGn',\n",
    "    alpha=0.6,\n",
    "    s=20,\n",
    "    vmin=-0.5,\n",
    "    vmax=1.0\n",
    ")\n",
    "axes[1].set_title(\n",
    "    'Per-Sample Silhouette Scores',\n",
    "    fontweight='bold',\n",
    "    fontsize=14\n",
    ")\n",
    "axes[1].set_xlabel('UMAP 1', fontsize=12)\n",
    "axes[1].set_ylabel('UMAP 2', fontsize=12)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('codebert_hybrid_umap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ… Saved: codebert_hybrid_umap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 8. ðŸ’¾ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration\n",
    "best_config = {\n",
    "    'approach': 'codebert_hybrid_optimized',\n",
    "    'embedding_model': 'microsoft/codebert-base',\n",
    "    'hybrid_ratio': {\n",
    "        'codebert': 0.7,\n",
    "        'metadata': 0.3,\n",
    "        'best_from_sweep': best_ratio['ratio']\n",
    "    },\n",
    "    'algorithm': 'AgglomerativeClustering',\n",
    "    'linkage': best_linkage['linkage'],\n",
    "    'k': best_k['k'],\n",
    "    'silhouette': float(best_linkage['silhouette']),\n",
    "    'experiment_results': {\n",
    "        'k_sweep': {\n",
    "            'best_k': int(best_k['k']),\n",
    "            'best_silhouette': float(best_k['silhouette'])\n",
    "        },\n",
    "        'ratio_sweep': {\n",
    "            'best_ratio': best_ratio['ratio_str'],\n",
    "            'best_silhouette': float(best_ratio['silhouette'])\n",
    "        },\n",
    "        'linkage_comparison': {\n",
    "            'best_linkage': best_linkage['linkage'],\n",
    "            'best_silhouette': float(best_linkage['silhouette'])\n",
    "        },\n",
    "        'algorithm_comparison': {\n",
    "            'best_algorithm': best_algorithm['algorithm'],\n",
    "            'best_silhouette': float(best_algorithm['silhouette'])\n",
    "        }\n",
    "    },\n",
    "    'comparison_to_baseline': {\n",
    "        'notebook_04_best': 0.689515,\n",
    "        'this_experiment': float(best_linkage['silhouette']),\n",
    "        'improvement': float(best_linkage['silhouette'] - 0.689515)\n",
    "    },\n",
    "    'total_questions': len(questions),\n",
    "    'feature_dimensions': {\n",
    "        'codebert': codebert_norm.shape[1],\n",
    "        'metadata': metadata_norm.shape[1],\n",
    "        'hybrid': best_features.shape[1]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('codebert_hybrid_best_config.json', 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "# Per-cluster metrics\n",
    "cluster_df.to_csv('codebert_hybrid_cluster_metrics.csv', index=False)\n",
    "\n",
    "# All experiment results\n",
    "all_experiments = pd.DataFrame([\n",
    "    {'experiment': 'k_sweep', 'config': f'K={r[\"k\"]}', 'silhouette': r['silhouette']}\n",
    "    for r in k_results\n",
    "] + [\n",
    "    {'experiment': 'ratio_sweep', 'config': f'Ratio={r[\"ratio_str\"]}', 'silhouette': r['silhouette']}\n",
    "    for r in ratio_results\n",
    "] + [\n",
    "    {'experiment': 'linkage', 'config': f'Linkage={r[\"linkage\"]}', 'silhouette': r['silhouette']}\n",
    "    for r in linkage_results\n",
    "] + [\n",
    "    {'experiment': 'algorithm', 'config': f'Alg={r[\"algorithm\"]}', 'silhouette': r['silhouette']}\n",
    "    for r in algorithm_results\n",
    "])\n",
    "\n",
    "all_experiments.to_csv('codebert_hybrid_all_experiments.csv', index=False)\n",
    "\n",
    "print(\"âœ… Exported:\")\n",
    "print(\"  - codebert_hybrid_best_config.json\")\n",
    "print(\"  - codebert_hybrid_cluster_metrics.csv\")\n",
    "print(\"  - codebert_hybrid_all_experiments.csv\")\n",
    "print(\"  - codebert_hybrid_umap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary-header",
   "metadata": {},
   "source": [
    "## 9. ðŸ“ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY: CODEBERT_HYBRID OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸŽ¯ HYBRID APPROACH:\")\n",
    "print(\"  CodeBERT embeddings: 70% (semantic code understanding)\")\n",
    "print(\"  Metadata features:   30% (categorical attributes)\")\n",
    "print(\"  Combined dimensions:\", best_features.shape[1])\n",
    "\n",
    "print(\"\\nðŸ† BEST CONFIGURATION:\")\n",
    "print(f\"  Algorithm: Agglomerative Clustering\")\n",
    "print(f\"  Linkage: {best_linkage['linkage']}\")\n",
    "print(f\"  K: {best_k['k']}\")\n",
    "print(f\"  Silhouette: {best_linkage['silhouette']:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š KEY FINDINGS:\")\n",
    "print(f\"  1. Optimal K: {best_k['k']} (tested 25-60)\")\n",
    "print(f\"  2. Optimal hybrid ratio: {best_ratio['ratio_str']} (tested 60/40 to 90/10)\")\n",
    "print(f\"  3. Best linkage: {best_linkage['linkage']} (tested all 4 types)\")\n",
    "print(f\"  4. Agglomerative outperforms: KMeans, Spectral, GMM, HDBSCAN, DBSCAN\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ COMPARISON:\")\n",
    "print(f\"  Notebook 04 best: 0.689515\")\n",
    "print(f\"  This optimization: {best_linkage['silhouette']:.6f}\")\n",
    "if best_linkage['silhouette'] > 0.689515:\n",
    "    print(f\"  âœ… Improved by: +{(best_linkage['silhouette'] - 0.689515):.6f}\")\n",
    "else:\n",
    "    print(f\"  â‰ˆ Similar performance: {(best_linkage['silhouette'] - 0.689515):.6f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ CLUSTER QUALITY:\")\n",
    "print(f\"  Mean cluster silhouette: {cluster_df['silhouette_mean'].mean():.4f}\")\n",
    "print(f\"  Std cluster silhouette: {cluster_df['silhouette_mean'].std():.4f}\")\n",
    "print(f\"  Best cluster: {cluster_df['silhouette_mean'].max():.4f}\")\n",
    "print(f\"  Worst cluster: {cluster_df['silhouette_mean'].min():.4f}\")\n",
    "\n",
    "print(\"\\nðŸš€ NEXT STEPS:\")\n",
    "print(\"  1. Implement in adaptive_router/core/cluster_engine.py\")\n",
    "print(\"  2. Train cluster profiles with real model error rates\")\n",
    "print(\"  3. Deploy to production routing system\")\n",
    "print(\"  4. Monitor real-world performance vs. predicted silhouette\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
