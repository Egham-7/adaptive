{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration and Endpoint Benchmarking\n",
    "\n",
    "This notebook provides comprehensive exploration and benchmarking tools for instruction-following datasets against AI model endpoints.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook enables you to:\n",
    "- **Load and analyze datasets**: Load instruction datasets from HuggingFace Hub with sampling options\n",
    "- **Endpoint testing**: Test AI model selection endpoints with real dataset samples\n",
    "- **Cost analysis**: Calculate and track API usage costs across different models\n",
    "- **Export results**: Save detailed results and summaries to CSV files\n",
    "\n",
    "### Supported Datasets:\n",
    "- **Databricks Dolly 15k**: High-quality instruction dataset (default)\n",
    "- **Open-Orca datasets**: Large-scale instruction datasets\n",
    "- **Any HuggingFace instruction dataset**: Customizable dataset loading\n",
    "\n",
    "### Features:\n",
    "- ✅ **Model cost tracking** with accurate pricing data\n",
    "- ✅ **Progress monitoring** with real-time updates  \n",
    "- ✅ **Error handling** for robust API testing\n",
    "- ✅ **Flexible JSON payload** supporting multiple AI providers\n",
    "- ✅ **Detailed analytics** with model selection distributions\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Install Dependencies\n",
    "```bash\n",
    "# Using pip\n",
    "pip install datasets pandas requests numpy matplotlib seaborn huggingface_hub\n",
    "\n",
    "# Using uv (recommended)\n",
    "uv add datasets pandas requests numpy matplotlib seaborn huggingface_hub\n",
    "```\n",
    "\n",
    "### 2. Configure HuggingFace Authentication\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "### 3. Update Endpoint Configuration\n",
    "Before running, update the `ENDPOINT_URL` variable in the \"Configuration\" section with your actual endpoint URL.\n",
    "\n",
    "### 4. Customize Model Costs (Optional)\n",
    "Update the `model_costs` dictionary in the `process_dataset_with_endpoint` function to match your actual model pricing.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run all cells sequentially\n",
    "2. The notebook will load 1000 samples from Databricks Dolly 15k dataset\n",
    "3. Test your endpoint connectivity\n",
    "4. Process samples and generate cost analysis\n",
    "5. Results are saved as CSV files with timestamps\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- `dolly_endpoint_results_YYYYMMDD_HHMMSS.csv`: Detailed results for each sample\n",
    "- `dolly_summary_YYYYMMDD_HHMMSS.csv`: Summary statistics and costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**✏️ Update these settings before running the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n",
      "📊 Dataset: databricks/databricks-dolly-15k\n",
      "📊 Sample size: 1000\n",
      "🌐 Endpoint: https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict\n",
      "⚠️  Remember to update ENDPOINT_URL before running!\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION SECTION =====\n",
    "# Update these variables before running the notebook\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_NAME = \"databricks/databricks-dolly-15k\"  # HuggingFace dataset identifier\n",
    "SAMPLE_SIZE = 1000  # Number of samples to process (None for all)\n",
    "DATASET_SPLIT = \"train\"  # Dataset split to use\n",
    "\n",
    "# Endpoint Configuration\n",
    "ENDPOINT_URL = \"https://prompt-classifer-dev.mangoplant-a7a21605.swedencentral.azurecontainerapps.io/predict\"  # ⚠️ UPDATE THIS\n",
    "\n",
    "# Processing Configuration\n",
    "MAX_SAMPLES_TO_PROCESS = 1000  # Maximum samples to send to endpoint\n",
    "REQUEST_TIMEOUT = 30  # Timeout for each API request (seconds)\n",
    "DELAY_BETWEEN_REQUESTS = 0.05  # Delay between requests (seconds)\n",
    "\n",
    "# Output Configuration\n",
    "SAVE_CSV_RESULTS = True  # Whether to save results to CSV\n",
    "SAVE_SUMMARY = True  # Whether to save summary statistics\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"📊 Dataset: {DATASET_NAME}\")\n",
    "print(f\"📊 Sample size: {SAMPLE_SIZE}\")\n",
    "print(f\"🌐 Endpoint: {ENDPOINT_URL}\")\n",
    "print(\"⚠️  Remember to update ENDPOINT_URL before running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "Pandas version: 2.2.2\n",
      "NumPy version: 1.26.4\n",
      "Requests available for API testing\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import time\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Requests available for API testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Authenticated as: AImen44\n",
      "User type: user\n"
     ]
    }
   ],
   "source": [
    "# Check HuggingFace authentication\n",
    "from huggingface_hub import whoami\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"✓ Authenticated as: {user_info['name']}\")\n",
    "    print(f\"User type: {user_info.get('type', 'Unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Authentication failed: {e}\")\n",
    "    print(\"Please run: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_structure(dataset_info: dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and display dataset structure and statistics.\n",
    "    \"\"\"\n",
    "    if 'error' in dataset_info:\n",
    "        print(f\"Cannot analyze {dataset_info['dataset_name']} due to error: {dataset_info['error']}\")\n",
    "        return\n",
    "\n",
    "    samples = dataset_info['samples']\n",
    "    if not samples:\n",
    "        print(\"No samples to analyze\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATASET ANALYSIS: {dataset_info['dataset_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Basic info\n",
    "    print(f\"Total samples: {dataset_info['num_samples']:,}\")\n",
    "    print(f\"Load time: {dataset_info['load_time']:.2f}s\")\n",
    "    print(f\"Streaming mode: {dataset_info['streaming']}\")\n",
    "\n",
    "    # Schema analysis\n",
    "    first_sample = samples[0]\n",
    "    print(f\"\\nSchema ({len(first_sample)} fields):\")\n",
    "    for field, value in first_sample.items():\n",
    "        value_type = type(value).__name__\n",
    "        if isinstance(value, str):\n",
    "            print(f\"  {field}: {value_type} (avg length: {len(value)} chars)\")\n",
    "        elif isinstance(value, (list, dict)):\n",
    "            print(f\"  {field}: {value_type} (length: {len(value)})\")\n",
    "        else:\n",
    "            print(f\"  {field}: {value_type} = {value}\")\n",
    "\n",
    "    # Text statistics for string fields\n",
    "    print(f\"\\nText Statistics (based on {len(samples)} samples):\")\n",
    "    for field in first_sample.keys():\n",
    "        if isinstance(first_sample[field], str):\n",
    "            lengths = [len(str(sample[field])) for sample in samples]\n",
    "            print(f\"  {field}:\")\n",
    "            print(f\"    Min: {min(lengths)} chars\")\n",
    "            print(f\"    Max: {max(lengths)} chars\")\n",
    "            print(f\"    Avg: {np.mean(lengths):.1f} chars\")\n",
    "            print(f\"    Median: {np.median(lengths):.1f} chars\")\n",
    "\n",
    "def display_sample_data(dataset_info: dict[str, Any], num_samples: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Display sample data from the dataset.\n",
    "    \"\"\"\n",
    "    if 'error' in dataset_info:\n",
    "        return\n",
    "\n",
    "    samples = dataset_info['samples']\n",
    "    if not samples:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAMPLE DATA: {dataset_info['dataset_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, sample in enumerate(samples[:num_samples]):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        for field, value in sample.items():\n",
    "            if isinstance(value, str):\n",
    "                if len(value) > 200:\n",
    "                    print(f\"{field}: {value[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"{field}: {value}\")\n",
    "            else:\n",
    "                print(f\"{field}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"✓ Analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Testing and Benchmarking\n",
    "\n",
    "Now let's test and benchmark different instruction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset processing function with cost calculation defined\n"
     ]
    }
   ],
   "source": [
    "def process_dataset_with_endpoint(dataset_info: dict, endpoint_url: str,\n",
    "                                max_samples: int = None, save_csv: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process dataset instructions through the endpoint and collect results with cost calculation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_info: Dataset information from load_and_sample_dataset\n",
    "        endpoint_url: Endpoint URL\n",
    "        max_samples: Maximum number of samples to process (None for all)\n",
    "        save_csv: Whether to save results to CSV file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all requests and responses\n",
    "    \"\"\"\n",
    "    if 'error' in dataset_info or not dataset_info['samples']:\n",
    "        print(\"❌ No valid dataset to process\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Model costs (cost per 1M tokens) - update these based on your model catalog\n",
    "    model_costs = {\n",
    "        \"gemini-2.5-flash-lite-preview-06-17\": {\"input\": 0.075, \"output\": 0.30},\n",
    "        \"gemini-2.5-flash\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gemini-2.5-pro\": {\"input\": 1.25, \"output\": 10.00},\n",
    "        \"mistral-small-latest\": {\"input\": 0.10, \"output\": 0.30},\n",
    "        \"gpt-4.1-nano\": {\"input\": 0.10, \"output\": 0.40},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4.1-mini\": {\"input\": 0.40, \"output\": 1.60},\n",
    "        \"gpt-4.1\": {\"input\": 2.00, \"output\": 8.00},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"o3-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "        \"o4-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "        \"o3\": {\"input\": 10.00, \"output\": 40.00},\n",
    "        \"gpt-4.5\": {\"input\": 75.00, \"output\": 150.00},\n",
    "        \"o1\": {\"input\": 15.00, \"output\": 60.00},\n",
    "        \"o1-pro\": {\"input\": 150.00, \"output\": 600.00},\n",
    "        \"deepseek-chat\": {\"input\": 0.14, \"output\": 0.28},\n",
    "        \"deepseek-reasoner\": {\"input\": 0.55, \"output\": 2.19},\n",
    "        \"grok-3-mini\": {\"input\": 0.30, \"output\": 0.50},\n",
    "        \"grok-3\": {\"input\": 3.00, \"output\": 15.00},\n",
    "        \"claude-sonnet-4-20250514\": {\"input\": 3.00, \"output\": 15.00},\n",
    "        \"claude-opus-4-20250514\": {\"input\": 15.00, \"output\": 75.00},\n",
    "        \"Qwen/Qwen2.5-14B-Instruct\": {\"input\": 0.12, \"output\": 0.12},\n",
    "        \"meta-llama/Llama-3.1-8B-Instruct\": {\"input\": 0.10, \"output\": 0.10},\n",
    "        \"codellama/CodeLlama-13b-Instruct-hf\": {\"input\": 0.11, \"output\": 0.11},\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.3\": {\"input\": 0.08, \"output\": 0.08},\n",
    "        \"google/flan-t5-xl\": {\"input\": 0.06, \"output\": 0.06},\n",
    "        \"microsoft/deberta-v3-large\": {\"input\": 0.04, \"output\": 0.04},\n",
    "    }\n",
    "\n",
    "    def estimate_tokens(text: str) -> int:\n",
    "        \"\"\"Rough token estimation (1 token ≈ 4 characters)\"\"\"\n",
    "        return len(text) // 4\n",
    "\n",
    "    def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate cost based on model and token counts\"\"\"\n",
    "        if model_name not in model_costs:\n",
    "            return 0.0\n",
    "\n",
    "        costs = model_costs[model_name]\n",
    "        input_cost = (input_tokens / 1_000_000) * costs[\"input\"]\n",
    "        output_cost = (output_tokens / 1_000_000) * costs[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "    samples = dataset_info['samples']\n",
    "    if max_samples:\n",
    "        samples = samples[:max_samples]\n",
    "\n",
    "    print(f\"🚀 Processing {len(samples)} samples through endpoint...\")\n",
    "    print(f\"📊 Endpoint: {endpoint_url}\")\n",
    "\n",
    "    results = []\n",
    "    successful_calls = 0\n",
    "    failed_calls = 0\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Progress: {i}/{len(samples)} ({i/len(samples)*100:.1f}%) - Cost so far: ${total_cost:.6f}\")\n",
    "\n",
    "        # Extract fields from sample\n",
    "        instruction = sample.get('instruction', '')\n",
    "        context = sample.get('context', '')\n",
    "        response = sample.get('response', '')  # Use for output token estimation only\n",
    "\n",
    "        # Create full prompt\n",
    "        full_prompt = f\"{instruction}\\n\\nContext: {context}\" if context.strip() else instruction\n",
    "\n",
    "        # Call endpoint\n",
    "        start_time = time.time()\n",
    "        api_result = call_endpoint(endpoint_url, instruction, context)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Estimate token counts\n",
    "        input_tokens = estimate_tokens(full_prompt)\n",
    "        output_tokens = estimate_tokens(response) if response else 100  # Use actual response for estimation\n",
    "\n",
    "        # Prepare result row\n",
    "        result_row = {\n",
    "            'sample_id': i,\n",
    "            'instruction': instruction,\n",
    "            'context': context,\n",
    "            'full_prompt': full_prompt,\n",
    "            'input_token_estimate': input_tokens,\n",
    "            'output_token_estimate': output_tokens,\n",
    "            'api_success': api_result['success'],\n",
    "            'api_status_code': api_result['status_code'],\n",
    "            'api_error': api_result['error'],\n",
    "            'response_time_seconds': end_time - start_time,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Add API response fields if successful\n",
    "        if api_result['success'] and api_result['response']:\n",
    "            api_response = api_result['response']\n",
    "\n",
    "            # Extract protocol and model information\n",
    "            protocol = api_response.get('protocol', '')\n",
    "            selected_model = ''\n",
    "            selected_provider = ''\n",
    "            estimated_cost = 0.0\n",
    "\n",
    "            # Parse based on protocol type\n",
    "            if protocol == 'standard' and 'standard' in api_response:\n",
    "                standard_info = api_response['standard']\n",
    "                selected_model = standard_info.get('model', '')\n",
    "                selected_provider = standard_info.get('provider', '')\n",
    "\n",
    "                # Calculate cost for selected model\n",
    "                estimated_cost = calculate_cost(selected_model, input_tokens, output_tokens)\n",
    "\n",
    "            elif protocol == 'minion' and 'minion' in api_response:\n",
    "                minion_info = api_response['minion']\n",
    "                selected_model = minion_info.get('model', '')\n",
    "                selected_provider = 'huggingface'  # Minions are HuggingFace models\n",
    "\n",
    "                # Calculate cost for selected model\n",
    "                estimated_cost = calculate_cost(selected_model, input_tokens, output_tokens)\n",
    "\n",
    "            # Update result with API response details\n",
    "            result_row.update({\n",
    "                'api_protocol': protocol,\n",
    "                'api_selected_model': selected_model,\n",
    "                'api_selected_provider': selected_provider,\n",
    "                'api_estimated_cost_usd': estimated_cost,\n",
    "                'api_full_response': json.dumps(api_response, indent=2)\n",
    "            })\n",
    "\n",
    "            total_cost += estimated_cost\n",
    "            successful_calls += 1\n",
    "        else:\n",
    "            # Add empty fields for failed calls\n",
    "            result_row.update({\n",
    "                'api_protocol': '',\n",
    "                'api_selected_model': '',\n",
    "                'api_selected_provider': '',\n",
    "                'api_estimated_cost_usd': 0.0,\n",
    "                'api_full_response': ''\n",
    "            })\n",
    "            failed_calls += 1\n",
    "\n",
    "        results.append(result_row)\n",
    "\n",
    "        # Small delay to avoid overwhelming the endpoint\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n✅ Processing complete!\")\n",
    "    print(f\"📊 Results: {successful_calls} successful, {failed_calls} failed\")\n",
    "    print(f\"📈 Success rate: {successful_calls/(successful_calls+failed_calls)*100:.1f}%\")\n",
    "    print(f\"💰 Total estimated cost: ${total_cost:.6f} USD\")\n",
    "    print(f\"💰 Average cost per request: ${total_cost/len(samples):.6f} USD\")\n",
    "\n",
    "    if save_csv:\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"dolly_endpoint_results_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"💾 Results saved to: {filename}\")\n",
    "\n",
    "        # Also save a summary\n",
    "        summary_filename = f\"dolly_summary_{timestamp}.csv\"\n",
    "\n",
    "        summary_df = pd.DataFrame([{\n",
    "            'total_samples': len(samples),\n",
    "            'successful_calls': successful_calls,\n",
    "            'failed_calls': failed_calls,\n",
    "            'success_rate_percent': successful_calls/(successful_calls+failed_calls)*100,\n",
    "            'total_cost_usd': total_cost,\n",
    "            'avg_cost_per_request_usd': total_cost/len(samples),\n",
    "            'endpoint_url': endpoint_url,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }])\n",
    "        summary_df.to_csv(summary_filename, index=False)\n",
    "        print(f\"📋 Summary saved to: {summary_filename}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"✅ Dataset processing function with cost calculation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint configuration and functions\n",
    "def test_endpoint_connection(url: str) -> bool:\n",
    "    \"\"\"Test if the endpoint is accessible.\"\"\"\n",
    "    try:\n",
    "        # For LitServe endpoints, try the predict endpoint directly\n",
    "        response = requests.get(f\"{url.rstrip('/')}\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ Endpoint {url} is accessible\")\n",
    "            return True\n",
    "        elif response.status_code == 405:  # Method not allowed (GET on POST endpoint)\n",
    "            print(f\"✅ Endpoint {url} is accessible (405 expected for GET on POST endpoint)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ Endpoint returned status code: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Failed to connect to endpoint: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_model_selection_request(prompt: str, context: str = \"\") -> dict:\n",
    "    \"\"\"Create a request payload matching ModelSelectionRequest structure.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n\\nContext: {context}\" if context.strip() else prompt\n",
    "\n",
    "    # Only active providers: OpenAI, GROQ, and DeepSeek\n",
    "    active_providers = [\n",
    "        \"openai\",      # ProviderType.OPENAI\n",
    "        \"groq\",        # ProviderType.GROQ (includes grok-3 models)\n",
    "        \"deepseek\",    # ProviderType.DEEPSEEK\n",
    "    ]\n",
    "\n",
    "    # Correct JSON format matching ModelSelectionRequest from llm_core_models.py\n",
    "    return {\n",
    "        \"prompt\": full_prompt,\n",
    "        \"user_id\": None,\n",
    "        \"provider_constraint\": active_providers,  # Only include active providers\n",
    "        \"cost_bias\": None\n",
    "    }\n",
    "\n",
    "def call_endpoint(url: str, prompt: str, context: str = \"\", timeout: int = None) -> dict:\n",
    "    \"\"\"Call the endpoint with a single prompt.\"\"\"\n",
    "    if timeout is None:\n",
    "        timeout = REQUEST_TIMEOUT\n",
    "\n",
    "    try:\n",
    "        payload = create_model_selection_request(prompt, context)\n",
    "\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response.json(),\n",
    "                \"status_code\": 200,\n",
    "                \"error\": None\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"response\": None,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"error\": f\"HTTP {response.status_code}: {response.text[:200]}\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"response\": None,\n",
    "            \"status_code\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Test endpoint connection\n",
    "print(\"Testing endpoint connection...\")\n",
    "endpoint_accessible = test_endpoint_connection(ENDPOINT_URL)\n",
    "print(f\"Endpoint accessible: {endpoint_accessible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test endpoint with sample data\n",
    "if endpoint_accessible:\n",
    "    print(\"🧪 Testing endpoint with sample data...\")\n",
    "\n",
    "    # Check if dolly_info is available\n",
    "    if 'dolly_info' in globals() and 'error' not in dolly_info:\n",
    "        # Test with first sample from Dolly dataset\n",
    "        test_sample = dolly_info['samples'][0]\n",
    "        test_instruction = test_sample['instruction']\n",
    "        test_context = test_sample['context']\n",
    "\n",
    "        print(f\"Test instruction: {test_instruction}\")\n",
    "        print(f\"Test context: {test_context[:100]}...\" if len(test_context) > 100 else f\"Test context: {test_context}\")\n",
    "\n",
    "        # Create and display the JSON payload\n",
    "        test_payload = create_model_selection_request(test_instruction, test_context)\n",
    "        print(\"\\nJSON payload being sent:\")\n",
    "        print(json.dumps(test_payload, indent=2))\n",
    "\n",
    "        # Test the endpoint\n",
    "        result = call_endpoint(ENDPOINT_URL, test_instruction, test_context)\n",
    "\n",
    "        if result['success']:\n",
    "            print(\"\\n✅ Endpoint test successful!\")\n",
    "            print(f\"Status code: {result['status_code']}\")\n",
    "            print(\"API Response preview:\")\n",
    "            response_preview = json.dumps(result['response'], indent=2)[:500]\n",
    "            print(f\"{response_preview}...\")\n",
    "        else:\n",
    "            print(\"\\n❌ Endpoint test failed:\")\n",
    "            print(f\"Status code: {result['status_code']}\")\n",
    "            print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"⚠️ Dataset not loaded yet. Please run the dataset loading cell first.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Endpoint not accessible - skipping test\")\n",
    "    print(\"💡 Make sure to update ENDPOINT_URL in the configuration section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loading function defined\n",
      "🔄 Loading databricks/databricks-dolly-15k dataset...\n",
      "✅ Dataset loaded successfully!\n",
      "📊 Dataset: databricks/databricks-dolly-15k\n",
      "📊 Samples loaded: 1,000\n",
      "⏱️  Load time: 1.96s\n",
      "\n",
      "============================================================\n",
      "DATASET ANALYSIS: databricks/databricks-dolly-15k\n",
      "============================================================\n",
      "Total samples: 1,000\n",
      "Load time: 1.96s\n",
      "Streaming mode: False\n",
      "\n",
      "Schema (4 fields):\n",
      "  instruction: str (avg length: 45 chars)\n",
      "  context: str (avg length: 0 chars)\n",
      "  response: str (avg length: 340 chars)\n",
      "  category: str (avg length: 13 chars)\n",
      "\n",
      "Text Statistics (based on 1000 samples):\n",
      "  instruction:\n",
      "    Min: 12 chars\n",
      "    Max: 1759 chars\n",
      "    Avg: 68.1 chars\n",
      "    Median: 52.0 chars\n",
      "  context:\n",
      "    Min: 0 chars\n",
      "    Max: 8851 chars\n",
      "    Avg: 319.6 chars\n",
      "    Median: 0.0 chars\n",
      "  response:\n",
      "    Min: 2 chars\n",
      "    Max: 4866 chars\n",
      "    Avg: 364.4 chars\n",
      "    Median: 189.5 chars\n",
      "  category:\n",
      "    Min: 7 chars\n",
      "    Max: 22 chars\n",
      "    Avg: 11.8 chars\n",
      "    Median: 10.0 chars\n",
      "\n",
      "============================================================\n",
      "SAMPLE DATA: databricks/databricks-dolly-15k\n",
      "============================================================\n",
      "\n",
      "--- Sample 1 ---\n",
      "instruction: Give me a list of the best sad songs to play.\n",
      "context: \n",
      "response: ‘Nothing Compares 2 U’ by Sinéad O’Connor\n",
      "‘Hurt’ by Johnny Cash\n",
      "‘Only Love Can Break Your Heart’ by Neil Young\n",
      "‘Teardrop’ by Massive Attack\n",
      "‘I Know It’s Over’ by The Smiths\n",
      "‘No Distance Left to Run’ b...\n",
      "category: brainstorming\n",
      "----------------------------------------\n",
      "\n",
      "--- Sample 2 ---\n",
      "instruction: Give me a bulleted list of who received the last best player awards at the World Cup.\n",
      "context: \n",
      "response: The Best Player award at the World Cup, also known as the Golden Ball for best player, was first awarded in 1982. Here are the last seven recipients of the Golden Ball award:\n",
      "- Brazil Ronaldo (1998 Fr...\n",
      "category: brainstorming\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze the Dolly dataset\n",
    "def load_and_sample_dataset(dataset_name: str, sample_size: int = None,\n",
    "                          dataset_split: str = \"train\", streaming: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Load a dataset from HuggingFace Hub with optional sampling.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: HuggingFace dataset identifier\n",
    "        sample_size: Number of samples to load (None for all)\n",
    "        dataset_split: Dataset split to use\n",
    "        streaming: Whether to use streaming mode\n",
    "    \n",
    "    Returns:\n",
    "        Dict with dataset info and samples\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load dataset\n",
    "        if streaming:\n",
    "            dataset = load_dataset(dataset_name, split=dataset_split, streaming=True)\n",
    "            if sample_size:\n",
    "                dataset = dataset.take(sample_size)\n",
    "            samples = list(dataset)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "            if sample_size:\n",
    "                # Get a sample\n",
    "                if sample_size >= len(dataset):\n",
    "                    samples = list(dataset)\n",
    "                else:\n",
    "                    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "                    samples = [dataset[int(i)] for i in indices]\n",
    "            else:\n",
    "                samples = list(dataset)\n",
    "\n",
    "        load_time = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'num_samples': len(samples),\n",
    "            'samples': samples,\n",
    "            'load_time': load_time,\n",
    "            'streaming': streaming,\n",
    "            'split': dataset_split\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'dataset_name': dataset_name,\n",
    "            'error': str(e),\n",
    "            'samples': []\n",
    "        }\n",
    "\n",
    "print(\"✅ Dataset loading function defined\")\n",
    "\n",
    "# Load the Dolly dataset\n",
    "print(f\"🔄 Loading {DATASET_NAME} dataset...\")\n",
    "dolly_info = load_and_sample_dataset(\n",
    "    DATASET_NAME,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    "    dataset_split=DATASET_SPLIT\n",
    ")\n",
    "\n",
    "if 'error' not in dolly_info:\n",
    "    print(\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"📊 Dataset: {dolly_info['dataset_name']}\")\n",
    "    print(f\"📊 Samples loaded: {dolly_info['num_samples']:,}\")\n",
    "    print(f\"⏱️  Load time: {dolly_info['load_time']:.2f}s\")\n",
    "\n",
    "    # Analyze and display dataset structure\n",
    "    analyze_dataset_structure(dolly_info)\n",
    "\n",
    "    # Display sample data\n",
    "    display_sample_data(dolly_info, num_samples=2)\n",
    "else:\n",
    "    print(f\"❌ Failed to load dataset: {dolly_info['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Endpoint not accessible - cannot process samples\n",
      "💡 Make sure to update ENDPOINT_URL in the configuration section\n"
     ]
    }
   ],
   "source": [
    "# Process samples through endpoint using configuration\n",
    "if endpoint_accessible:\n",
    "    print(f\"🚀 Processing {MAX_SAMPLES_TO_PROCESS} samples through endpoint...\")\n",
    "\n",
    "    # Check if dolly_info is available\n",
    "    if 'dolly_info' in globals() and 'error' not in dolly_info:\n",
    "        # Process samples using configuration values\n",
    "        results_1000 = process_dataset_with_endpoint(\n",
    "            dolly_info,\n",
    "            ENDPOINT_URL,\n",
    "            max_samples=MAX_SAMPLES_TO_PROCESS,\n",
    "            save_csv=SAVE_CSV_RESULTS\n",
    "        )\n",
    "\n",
    "        if not results_1000.empty:\n",
    "            print(\"\\n📊 Results Summary:\")\n",
    "            print(f\"Total samples processed: {len(results_1000)}\")\n",
    "            print(f\"Successful API calls: {results_1000['api_success'].sum()}\")\n",
    "            print(f\"Failed API calls: {(~results_1000['api_success']).sum()}\")\n",
    "            print(f\"Total estimated cost: ${results_1000['api_estimated_cost_usd'].sum():.6f}\")\n",
    "\n",
    "            print(\"\\n📋 Sample Results Preview:\")\n",
    "            preview_cols = ['sample_id', 'api_protocol', 'api_selected_model',\n",
    "                           'api_estimated_cost_usd', 'api_success', 'response_time_seconds']\n",
    "            print(results_1000[preview_cols].head(10))\n",
    "\n",
    "            print(\"\\n📈 Model Selection Distribution:\")\n",
    "            if results_1000['api_selected_model'].notna().any():\n",
    "                model_counts = results_1000['api_selected_model'].value_counts()\n",
    "                print(model_counts.head())\n",
    "\n",
    "            print(\"\\n💰 Cost by Model:\")\n",
    "            if results_1000['api_selected_model'].notna().any():\n",
    "                cost_by_model = results_1000.groupby('api_selected_model')['api_estimated_cost_usd'].agg(['count', 'sum', 'mean'])\n",
    "                print(cost_by_model.head())\n",
    "\n",
    "            print(\"\\n📊 Final CSV Columns:\")\n",
    "            print(f\"Total columns: {len(results_1000.columns)}\")\n",
    "            print(\"Column list:\", list(results_1000.columns))\n",
    "        else:\n",
    "            print(\"❌ No results to process\")\n",
    "    else:\n",
    "        print(\"⚠️ Dataset not loaded yet. Please run the dataset loading cell first.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Endpoint not accessible - cannot process samples\")\n",
    "    print(\"💡 Make sure to update ENDPOINT_URL in the configuration section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive framework for:\n",
    "\n",
    "1. **Dataset Loading**: Efficient loading with sampling and streaming options\n",
    "2. **Structure Analysis**: Understanding dataset schema and statistics\n",
    "3. **Quality Assessment**: Evaluating dataset suitability for instruction tuning\n",
    "4. **Comparative Analysis**: Comparing multiple datasets side by side\n",
    "5. **Visualization**: Creating plots and tables for better understanding\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "- **For Experimentation**: Use `sample_size` parameter to work with smaller subsets\n",
    "- **For Large Datasets**: Use `streaming=True` to avoid memory issues\n",
    "- **For Production**: Consider the load times and implement caching strategies\n",
    "- **For Quality**: Pay attention to empty fields and text length distributions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Extend this notebook to include more datasets\n",
    "2. Add more sophisticated quality metrics\n",
    "3. Implement data preprocessing pipelines\n",
    "4. Create automated benchmarking workflows\n",
    "5. Add model evaluation capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}